{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Versioning data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook we will look at using tools to version data. Specifically, [git-annex](https://git-annex.branchable.com/) and [datalad](http://datalad.org/)\n",
    "\n",
    "- Initializing, searching, downloading, and removing (dropping) a datalad dataset\n",
    "- Creating and modifying a datalad dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's start by using the shell version\n",
    "\n",
    "The `!` character at the beginning of the next line indicates that the subsequent command will be executed in a shell. In the following section we are generating the help command. The `--help-np` option ensures that the underlying help output does not page. Since the notebook does not support paging, we will be using this option."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Usage: datalad [global-opts] command [command-opts]\r\n",
      "\r\n",
      "DataLad provides a unified data distribution with the convenience of git-annex\r\n",
      "repositories as a backend.  DataLad command line tools allow to manipulate\r\n",
      "(obtain, create, update, publish, etc.) datasets and their collections.\r\n",
      "\r\n",
      "*Commands for dataset operations*\r\n",
      "\r\n",
      "  create\r\n",
      "      Create a new dataset from scratch\r\n",
      "  install\r\n",
      "      Install a dataset from a (remote) source\r\n",
      "  get\r\n",
      "      Get any dataset content (files/directories/subdatasets)\r\n",
      "  add\r\n",
      "      Add files/directories to an existing dataset\r\n",
      "  publish\r\n",
      "      Publish a dataset to a known sibling\r\n",
      "  uninstall\r\n",
      "      Uninstall subdatasets\r\n",
      "  drop\r\n",
      "      Drop file content from datasets\r\n",
      "  remove\r\n",
      "      Remove components from datasets\r\n",
      "  update\r\n",
      "      Update a dataset from a sibling\r\n",
      "  create-sibling\r\n",
      "      Create a dataset sibling on a UNIX-like SSH-accessible machine\r\n",
      "  create-sibling-github\r\n",
      "      Create dataset sibling on Github\r\n",
      "  unlock\r\n",
      "      Unlock file(s) of a dataset\r\n",
      "  save\r\n",
      "      Save the current state of a dataset\r\n",
      "  plugin\r\n",
      "      Generic plugin interface\r\n",
      "\r\n",
      "*Commands for meta data handling*\r\n",
      "\r\n",
      "  search\r\n",
      "      Search within available in datasets' meta data\r\n",
      "  metadata\r\n",
      "      Metadata manipulation for files and whole datasets\r\n",
      "  aggregate-metadata\r\n",
      "      Aggregate meta data of a dataset for later query\r\n",
      "\r\n",
      "*Miscellaneous commands*\r\n",
      "\r\n",
      "  test\r\n",
      "      Run internal DataLad (unit)tests\r\n",
      "  crawl\r\n",
      "      Crawl online resource to create or update a dataset\r\n",
      "  crawl-init\r\n",
      "      Initialize crawling configuration\r\n",
      "  ls\r\n",
      "      List summary information about URLs and dataset(s)\r\n",
      "  clean\r\n",
      "      Clean up after DataLad (possible temporary files etc.)\r\n",
      "  add-archive-content\r\n",
      "      Add content of an archive under git annex control\r\n",
      "  download-url\r\n",
      "      Download content\r\n",
      "\r\n",
      "*Plumbing commands*\r\n",
      "\r\n",
      "  annotate-paths\r\n",
      "      Analyze and act upon input paths\r\n",
      "  clone\r\n",
      "      Obtain a dataset copy from a URL or local source (path)\r\n",
      "  create-test-dataset\r\n",
      "      Create test (meta-)dataset\r\n",
      "  diff\r\n",
      "      Report changes of dataset component between revisions\r\n",
      "  siblings\r\n",
      "      Manage sibling configuration\r\n",
      "  sshrun\r\n",
      "      Run command on remote machines via SSH\r\n",
      "  subdatasets\r\n",
      "      Report subdatasets and their properties\r\n",
      "\r\n",
      "*General information*\r\n",
      "\r\n",
      "Detailed usage information for individual commands is available via\r\n",
      "command-specific --help, i.e.: datalad <command> --help\r\n",
      "\r\n",
      "\r\n",
      "*Global options*\r\n",
      "  -h, --help, --help-np\r\n",
      "                        show this help message. --help-np forcefully disables\r\n",
      "                        the use of a pager for displaying the help message\r\n",
      "  -l LEVEL, --log-level LEVEL\r\n",
      "                        set logging verbosity level. Choose among critical,\r\n",
      "                        error, warning, info, debug. Also you can specify an\r\n",
      "                        integer <10 to provide even more debugging information\r\n",
      "  --pbs-runner {condor}\r\n",
      "                        execute command by scheduling it via available PBS.\r\n",
      "                        For settings, config file will be consulted\r\n",
      "  -C PATH               run as if datalad was started in <path> instead of the\r\n",
      "                        current working directory. When multiple -C options\r\n",
      "                        are given, each subsequent non-absolute -C <path> is\r\n",
      "                        interpreted relative to the preceding -C <path>. This\r\n",
      "                        option affects the interpretations of the path names\r\n",
      "                        in that they are made relative to the working\r\n",
      "                        directory caused by the -C option\r\n",
      "  --version             show the program's version and license information\r\n",
      "  --dbg                 enter Python debugger when uncaught exception happens\r\n",
      "  --idbg                enter IPython debugger when uncaught exception happens\r\n",
      "  -c KEY=VALUE          configuration variable setting. Overrides any\r\n",
      "                        configuration read from a file, but is potentially\r\n",
      "                        overridden itself by configuration variables in the\r\n",
      "                        process environment.\r\n",
      "  --output-format {default,json,json_pp,tailored,'<template>'\r\n",
      "                        select format for returned command results. 'default'\r\n",
      "                        give one line per result reporting action, status,\r\n",
      "                        path and an optional message; 'json' renders a JSON\r\n",
      "                        object with all properties for each result (one per\r\n",
      "                        line); 'json_pp' pretty-prints JSON spanning multiple\r\n",
      "                        lines; 'tailored' enables a command-specific rendering\r\n",
      "                        style that is typically tailored to human consumption\r\n",
      "                        (no result output otherwise), '<template>' reports any\r\n",
      "                        value(s) of any result properties in any format\r\n",
      "                        indicated by the template (e.g. '{path}', compare with\r\n",
      "                        JSON output for all key-value choices).\r\n",
      "  --report-status {success,failure,ok,notneeded,impossible,error}\r\n",
      "                        constrain command result report to records matching\r\n",
      "                        the given status. 'success' is a synonym for 'ok' OR\r\n",
      "                        'notneeded', 'failure' stands for 'impossible' OR\r\n",
      "                        'error'.\r\n",
      "  --report-type {dataset,file}\r\n",
      "                        constrain command result report to records matching\r\n",
      "                        the given type. Can be given more than once to match\r\n",
      "                        multiple types.\r\n",
      "  --on-failure {ignore,continue,stop}\r\n",
      "                        when an operation fails: 'ignore' and continue with\r\n",
      "                        remaining operations, the error is logged but does not\r\n",
      "                        lead to a non-zero exit code of the command;\r\n",
      "                        'continue' works like 'ignore', but an error causes a\r\n",
      "                        non-zero exit code; 'stop' halts on first failure and\r\n",
      "                        yields non-zero exit code. A failure is any result\r\n",
      "                        with status 'impossible' or 'error'.\r\n",
      "  --run-before PLUGINSPEC [PLUGINSPEC ...]\r\n",
      "                        DataLad plugin to run after the command. PLUGINSPEC is\r\n",
      "                        a list comprised of a plugin name plus optional\r\n",
      "                        `key=value` pairs with arguments for the plugin call\r\n",
      "                        (see `plugin` command documentation for details). This\r\n",
      "                        option can be given more than once to run multiple\r\n",
      "                        plugins in the order in which they were given. For\r\n",
      "                        running plugins that require a --dataset argument it\r\n",
      "                        is important to provide the respective dataset as the\r\n",
      "                        --dataset argument of the main command, if it is not\r\n",
      "                        in the list of plugin arguments.\r\n",
      "  --run-after PLUGINSPEC [PLUGINSPEC ...]\r\n",
      "                        Like --run-before, but plugins are executed after the\r\n",
      "                        main command has finished.\r\n",
      "  --cmd                 syntactical helper that can be used to end the list of\r\n",
      "                        global command line options before the subcommand\r\n",
      "                        label. Options like --run-before can take an arbitray\r\n",
      "                        number of arguments and may require to be followed by\r\n",
      "                        a single --cmd in order to enable identification of\r\n",
      "                        the subcommand.\r\n",
      "\r\n",
      "\"Control Your Data\"\r\n",
      "\r\n"
     ]
    }
   ],
   "source": [
    "!datalad --help-np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "solution2": "shown",
    "solution2_first": true
   },
   "source": [
    "## Exercise 1: Generate the help for the install command of datalad."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true,
    "solution2": "shown"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Usage: datalad install [--version] [-h] [-l LEVEL] [--pbs-runner {condor}]\r\n",
      "                       [-s SOURCE] [-d DATASET] [-g] [-D DESCRIPTION] [-r]\r\n",
      "                       [--recursion-limit LEVELS] [--nosave] [--reckless]\r\n",
      "                       [-J NJOBS]\r\n",
      "                       [PATH [PATH ...]]\r\n",
      "\r\n",
      "Install a dataset from a (remote) source.\r\n",
      "\r\n",
      "This command creates a local sibling of an existing dataset from a\r\n",
      "(remote) location identified via a URL or path. Optional recursion into\r\n",
      "potential subdatasets, and download of all referenced data is supported.\r\n",
      "The new dataset can be optionally registered in an existing\r\n",
      "superdataset by identifying it via the DATASET argument (the new\r\n",
      "dataset's path needs to be located within the superdataset for that).\r\n",
      "\r\n",
      "It is recommended to provide a brief description to label the dataset's\r\n",
      "nature *and* location, e.g. \"Michael's music on black laptop\". This helps\r\n",
      "humans to identify data locations in distributed scenarios.  By default an\r\n",
      "identifier comprised of user and machine name, plus path will be generated.\r\n",
      "\r\n",
      "When only partial dataset content shall be obtained, it is recommended to\r\n",
      "use this command without the GET-DATA flag, followed by a\r\n",
      "`get` operation to obtain the desired data.\r\n",
      "\r\n",
      "NOTE\r\n",
      "  Power-user info: This command uses git clone, and\r\n",
      "  git annex init to prepare the dataset. Registering to a\r\n",
      "  superdataset is performed via a git submodule add operation\r\n",
      "  in the discovered superdataset.\r\n",
      "\r\n",
      "*Arguments*\r\n",
      "  PATH                  path/name of the installation target. If no PATH is\r\n",
      "                        provided a destination path will be derived from a\r\n",
      "                        source URL similar to git clone. [Default: None]\r\n",
      "\r\n",
      "*Options*\r\n",
      "  --version             show the program's version and license information\r\n",
      "  -h, --help, --help-np\r\n",
      "                        show this help message. --help-np forcefully disables\r\n",
      "                        the use of a pager for displaying the help message\r\n",
      "  -l LEVEL, --log-level LEVEL\r\n",
      "                        set logging verbosity level. Choose among critical,\r\n",
      "                        error, warning, info, debug. Also you can specify an\r\n",
      "                        integer <10 to provide even more debugging information\r\n",
      "  --pbs-runner {condor}\r\n",
      "                        execute command by scheduling it via available PBS.\r\n",
      "                        For settings, config file will be consulted\r\n",
      "  -s SOURCE, --source SOURCE\r\n",
      "                        URL or local path of the installation source.\r\n",
      "                        Constraints: value must be a string [Default: None]\r\n",
      "  -d DATASET, --dataset DATASET\r\n",
      "                        specify the dataset to perform the install operation\r\n",
      "                        on. If no dataset is given, an attempt is made to\r\n",
      "                        identify the dataset in a parent directory of the\r\n",
      "                        current working directory and/or the PATH given.\r\n",
      "                        Constraints: Value must be a Dataset or a valid\r\n",
      "                        identifier of a Dataset (e.g. a path) [Default: None]\r\n",
      "  -g, --get-data        if given, obtain all data content too. [Default:\r\n",
      "                        False]\r\n",
      "  -D DESCRIPTION, --description DESCRIPTION\r\n",
      "                        short description to use for a dataset location. Its\r\n",
      "                        primary purpose is to help humans to identify a\r\n",
      "                        dataset copy (e.g., \"mike's dataset on lab server\").\r\n",
      "                        Note that when a dataset is published, this\r\n",
      "                        information becomes available on the remote side.\r\n",
      "                        Constraints: value must be a string [Default: None]\r\n",
      "  -r, --recursive       if set, recurse into potential subdataset. [Default:\r\n",
      "                        False]\r\n",
      "  --recursion-limit LEVELS\r\n",
      "                        limit recursion into subdataset to the given number of\r\n",
      "                        levels. Constraints: value must be convertible to type\r\n",
      "                        'int' [Default: None]\r\n",
      "  --nosave              by default all modifications to a dataset are\r\n",
      "                        immediately saved. Given this option will disable this\r\n",
      "                        behavior. [Default: True]\r\n",
      "  --reckless            Set up the dataset to be able to obtain content in the\r\n",
      "                        cheapest/fastest possible way, even if this poses a\r\n",
      "                        potential risk the data integrity (e.g. hardlink files\r\n",
      "                        from a local clone of the dataset). Use with care, and\r\n",
      "                        limit to \"read-only\" use cases. With this flag the\r\n",
      "                        installed dataset will be marked as untrusted.\r\n",
      "                        [Default: False]\r\n",
      "  -J NJOBS, --jobs NJOBS\r\n",
      "                        how many parallel jobs (where possible) to use.\r\n",
      "                        Constraints: value must be convertible to type 'int'\r\n",
      "                        [Default: None]\r\n",
      "\r\n"
     ]
    }
   ],
   "source": [
    "!datalad install --help-np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will install the datalad metadataset. Note this only installs the top level directory structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "install(ok): /data/datasets.datalad.org (dataset)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO] Cloning dataset from 'http://datasets.datalad.org/' (trying 2 location candidate(s)) to '/data/datasets.datalad.org' \n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "\n",
    "cd /data\n",
    "datalad install /// "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "solution2": "hidden",
    "solution2_first": true
   },
   "source": [
    "## Exercise 2\n",
    "\n",
    "List the contents of the installed dataset using the tree command up to three levels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "solution2": "hidden"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/data/datasets.datalad.org/\n",
      "├── adhd200\n",
      "├── corr\n",
      "├── crcns\n",
      "├── datapackage.json\n",
      "├── dbic\n",
      "├── devel\n",
      "├── dicoms\n",
      "├── hbnssi\n",
      "├── indi\n",
      "├── kaggle\n",
      "├── labs\n",
      "├── neurovault\n",
      "├── nidm\n",
      "├── openfmri\n",
      "└── workshops\n",
      "\n",
      "14 directories, 1 file\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "\n",
    "tree -L 3 /data/datasets.datalad.org/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "solution2": "hidden"
   },
   "source": [
    "We note that only the top level datasets are created."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another useful command is to `search` for information across datalad datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "solution2": "hidden",
    "solution2_first": true
   },
   "source": [
    "## Exercise 3:\n",
    "\n",
    "Display the help for the search command"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true,
    "solution2": "hidden"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Usage: datalad search [--version] [-h] [-l LEVEL] [--pbs-runner {condor}]\r\n",
      "                      [-d DATASET] [-s PROPERTY] [-r PROPERTY] [-R]\r\n",
      "                      [-f FORMAT] [--regex]\r\n",
      "                      STRING [STRING ...]\r\n",
      "\r\n",
      "Search within available in datasets' meta data\r\n",
      "\r\n",
      "*Arguments*\r\n",
      "  STRING                a string (or a regular expression if --regex) to\r\n",
      "                        search for in all meta data values. If multiple\r\n",
      "                        provided, all must have a match among some fields of a\r\n",
      "                        dataset.\r\n",
      "\r\n",
      "*Options*\r\n",
      "  --version             show the program's version and license information\r\n",
      "  -h, --help, --help-np\r\n",
      "                        show this help message. --help-np forcefully disables\r\n",
      "                        the use of a pager for displaying the help message\r\n",
      "  -l LEVEL, --log-level LEVEL\r\n",
      "                        set logging verbosity level. Choose among critical,\r\n",
      "                        error, warning, info, debug. Also you can specify an\r\n",
      "                        integer <10 to provide even more debugging information\r\n",
      "  --pbs-runner {condor}\r\n",
      "                        execute command by scheduling it via available PBS.\r\n",
      "                        For settings, config file will be consulted\r\n",
      "  -d DATASET, --dataset DATASET\r\n",
      "                        specify the dataset to perform the query operation on.\r\n",
      "                        If no dataset is given, an attempt is made to identify\r\n",
      "                        the dataset based on the current working directory\r\n",
      "                        and/or the PATH given. Constraints: Value must be a\r\n",
      "                        Dataset or a valid identifier of a Dataset (e.g. a\r\n",
      "                        path) [Default: None]\r\n",
      "  -s PROPERTY, --search PROPERTY\r\n",
      "                        name of the property to search for any match. This\r\n",
      "                        option can be given multiple times. By default, all\r\n",
      "                        properties are searched. [Default: None]\r\n",
      "  -r PROPERTY, --report PROPERTY\r\n",
      "                        name of the property to report for any match. This\r\n",
      "                        option can be given multiple times. If '*' is given,\r\n",
      "                        all properties are reported. [Default: None]\r\n",
      "  -R, --report-matched  flag to report those fields which have matches. If\r\n",
      "                        REPORT option values are provided, union of matched\r\n",
      "                        and those in REPORT will be output. [Default: False]\r\n",
      "  -f FORMAT, --format FORMAT\r\n",
      "                        format for output. Constraints: value must be one of\r\n",
      "                        ('custom', 'json', 'yaml') [Default: 'custom']\r\n",
      "  --regex               flag for STRING to be used as a (Python) regular\r\n",
      "                        expression which should match the value. [Default:\r\n",
      "                        False]\r\n",
      "\r\n"
     ]
    }
   ],
   "source": [
    "!datalad search --help-np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's search for datasets containing information about Jim Haxby"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\u001b[1;37mINFO   \u001b[0m] Loading and caching local meta-data... might take a few seconds \n",
      "\u001b[1;4m/data/datasets.datalad.org/labs/haxby/attention\u001b[0m \n",
      "\u001b[1;4m/data/datasets.datalad.org/openfmri/ds000233\u001b[0m \n",
      "\u001b[1;4m/data/datasets.datalad.org/hbnssi\u001b[0m \n",
      "\u001b[1;4m/data/datasets.datalad.org/labs/haxby\u001b[0m \n",
      "\u001b[1;4m/data/datasets.datalad.org/labs/haxby/raiders\u001b[0m \n",
      "\u001b[1;4m/data/datasets.datalad.org/openfmri/ds000105\u001b[0m \n"
     ]
    }
   ],
   "source": [
    "!datalad search -d /data/datasets.datalad.org haxby"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "solution2": "hidden",
    "solution2_first": true
   },
   "source": [
    "## Exercise 4:\n",
    "\n",
    "Search for information about datasets related to chris gorgolewski, using `gorgolewski` as the keyword."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "solution2": "hidden"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;4m/data/datasets.datalad.org/openfmri/ds000114\u001b[0m \r\n",
      "\u001b[1;4m/data/datasets.datalad.org/workshops/nih-2017/ds000114\u001b[0m \r\n",
      "\u001b[1;4m/data/datasets.datalad.org/openfmri/ds000158\u001b[0m \r\n",
      "\u001b[1;4m/data/datasets.datalad.org/openfmri/ds000221\u001b[0m \r\n",
      "\u001b[1;4m/data/datasets.datalad.org/workshops/nipype-2017/ds000114\u001b[0m \r\n"
     ]
    }
   ],
   "source": [
    "!datalad search -d /data/datasets.datalad.org gorgolewski"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now install dataset ds0000114 from the recent NIH workshop. This will help us run some of the other notebooks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "install(ok): /data/datasets.datalad.org/workshops (dataset) [Installed subdataset in order to get /data/datasets.datalad.org/workshops/nih-2017/ds000114]\n",
      "install(ok): /data/datasets.datalad.org/workshops/nih-2017 (dataset) [Installed subdataset in order to get /data/datasets.datalad.org/workshops/nih-2017/ds000114]\n",
      "install(ok): /data/datasets.datalad.org/workshops/nih-2017/ds000114 (dataset) [Installed subdataset in order to get /data/datasets.datalad.org/workshops/nih-2017/ds000114]\n",
      "action summary:\n",
      "  install (ok: 3)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO] Cloning dataset from 'http://datasets.datalad.org/workshops/.git' (trying 1 location candidate(s)) to '/data/datasets.datalad.org/workshops' \n",
      "[INFO] Submodule HEAD got detached. Resetting branch master to point to cf3d00f5. Original location was 5f6b24a0 \n",
      "[INFO] Cloning dataset from 'http://datasets.datalad.org/workshops/nih-2017/.git' (trying 1 location candidate(s)) to '/data/datasets.datalad.org/workshops/nih-2017' \n",
      "[INFO] Cloning dataset from 'http://datasets.datalad.org/workshops/nih-2017/ds000114/.git' (trying 2 location candidate(s)) to '/data/datasets.datalad.org/workshops/nih-2017/ds000114' \n"
     ]
    }
   ],
   "source": [
    "%%bash \n",
    "\n",
    "datalad install /data/datasets.datalad.org/workshops/nih-2017/ds000114"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "solution2": "hidden",
    "solution2_first": true
   },
   "source": [
    "## Exercise 5:\n",
    "\n",
    "Look at the contents of the dataset upto three levels using the `tree` command. Note that there may be files that are not present locally. These will be indicated in red."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "solution2": "hidden"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[01;34m/data/datasets.datalad.org/workshops/nih-2017/ds000114\u001b[00m\n",
      "├── CHANGES\n",
      "├── dataset_description.json\n",
      "├── \u001b[01;34mderivatives\u001b[00m\n",
      "│   ├── \u001b[01;34mfmriprep\u001b[00m\n",
      "│   └── \u001b[01;34mfreesurfer\u001b[00m\n",
      "├── \u001b[40;31;01mdwi.bval\u001b[00m -> \u001b[00m.git/annex/objects/JX/4K/MD5E-s335--5bd6fa32ccd0c79e79f9ac63a2c09c1a.bval/MD5E-s335--5bd6fa32ccd0c79e79f9ac63a2c09c1a.bval\u001b[00m\n",
      "├── \u001b[40;31;01mdwi.bvec\u001b[00m -> \u001b[00m.git/annex/objects/Pg/wk/MD5E-s1248--0641c68ff6ee6164928c984541653430.bvec/MD5E-s1248--0641c68ff6ee6164928c984541653430.bvec\u001b[00m\n",
      "├── \u001b[01;34msub-01\u001b[00m\n",
      "│   ├── \u001b[01;34mses-retest\u001b[00m\n",
      "│   │   ├── \u001b[01;34manat\u001b[00m\n",
      "│   │   ├── \u001b[01;34mdwi\u001b[00m\n",
      "│   │   └── \u001b[01;34mfunc\u001b[00m\n",
      "│   └── \u001b[01;34mses-test\u001b[00m\n",
      "│       ├── \u001b[01;34manat\u001b[00m\n",
      "│       ├── \u001b[01;34mdwi\u001b[00m\n",
      "│       └── \u001b[01;34mfunc\u001b[00m\n",
      "├── \u001b[01;34msub-02\u001b[00m\n",
      "│   ├── \u001b[01;34mses-retest\u001b[00m\n",
      "│   │   ├── \u001b[01;34manat\u001b[00m\n",
      "│   │   ├── \u001b[01;34mdwi\u001b[00m\n",
      "│   │   └── \u001b[01;34mfunc\u001b[00m\n",
      "│   └── \u001b[01;34mses-test\u001b[00m\n",
      "│       ├── \u001b[01;34manat\u001b[00m\n",
      "│       ├── \u001b[01;34mdwi\u001b[00m\n",
      "│       └── \u001b[01;34mfunc\u001b[00m\n",
      "├── \u001b[01;34msub-03\u001b[00m\n",
      "│   ├── \u001b[01;34mses-retest\u001b[00m\n",
      "│   │   ├── \u001b[01;34manat\u001b[00m\n",
      "│   │   ├── \u001b[01;34mdwi\u001b[00m\n",
      "│   │   └── \u001b[01;34mfunc\u001b[00m\n",
      "│   └── \u001b[01;34mses-test\u001b[00m\n",
      "│       ├── \u001b[01;34manat\u001b[00m\n",
      "│       ├── \u001b[01;34mdwi\u001b[00m\n",
      "│       └── \u001b[01;34mfunc\u001b[00m\n",
      "├── \u001b[01;34msub-04\u001b[00m\n",
      "│   ├── \u001b[01;34mses-retest\u001b[00m\n",
      "│   │   ├── \u001b[01;34manat\u001b[00m\n",
      "│   │   ├── \u001b[01;34mdwi\u001b[00m\n",
      "│   │   └── \u001b[01;34mfunc\u001b[00m\n",
      "│   └── \u001b[01;34mses-test\u001b[00m\n",
      "│       ├── \u001b[01;34manat\u001b[00m\n",
      "│       ├── \u001b[01;34mdwi\u001b[00m\n",
      "│       └── \u001b[01;34mfunc\u001b[00m\n",
      "├── \u001b[01;34msub-05\u001b[00m\n",
      "│   ├── \u001b[01;34mses-retest\u001b[00m\n",
      "│   │   ├── \u001b[01;34manat\u001b[00m\n",
      "│   │   ├── \u001b[01;34mdwi\u001b[00m\n",
      "│   │   └── \u001b[01;34mfunc\u001b[00m\n",
      "│   └── \u001b[01;34mses-test\u001b[00m\n",
      "│       ├── \u001b[01;34manat\u001b[00m\n",
      "│       ├── \u001b[01;34mdwi\u001b[00m\n",
      "│       └── \u001b[01;34mfunc\u001b[00m\n",
      "├── \u001b[01;34msub-06\u001b[00m\n",
      "│   ├── \u001b[01;34mses-retest\u001b[00m\n",
      "│   │   ├── \u001b[01;34manat\u001b[00m\n",
      "│   │   ├── \u001b[01;34mdwi\u001b[00m\n",
      "│   │   └── \u001b[01;34mfunc\u001b[00m\n",
      "│   └── \u001b[01;34mses-test\u001b[00m\n",
      "│       ├── \u001b[01;34manat\u001b[00m\n",
      "│       ├── \u001b[01;34mdwi\u001b[00m\n",
      "│       └── \u001b[01;34mfunc\u001b[00m\n",
      "├── \u001b[01;34msub-07\u001b[00m\n",
      "│   ├── \u001b[01;34mses-retest\u001b[00m\n",
      "│   │   ├── \u001b[01;34manat\u001b[00m\n",
      "│   │   ├── \u001b[01;34mdwi\u001b[00m\n",
      "│   │   └── \u001b[01;34mfunc\u001b[00m\n",
      "│   └── \u001b[01;34mses-test\u001b[00m\n",
      "│       ├── \u001b[01;34manat\u001b[00m\n",
      "│       ├── \u001b[01;34mdwi\u001b[00m\n",
      "│       └── \u001b[01;34mfunc\u001b[00m\n",
      "├── \u001b[01;34msub-08\u001b[00m\n",
      "│   ├── \u001b[01;34mses-retest\u001b[00m\n",
      "│   │   ├── \u001b[01;34manat\u001b[00m\n",
      "│   │   ├── \u001b[01;34mdwi\u001b[00m\n",
      "│   │   └── \u001b[01;34mfunc\u001b[00m\n",
      "│   └── \u001b[01;34mses-test\u001b[00m\n",
      "│       ├── \u001b[01;34manat\u001b[00m\n",
      "│       ├── \u001b[01;34mdwi\u001b[00m\n",
      "│       └── \u001b[01;34mfunc\u001b[00m\n",
      "├── \u001b[01;34msub-09\u001b[00m\n",
      "│   ├── \u001b[01;34mses-retest\u001b[00m\n",
      "│   │   ├── \u001b[01;34manat\u001b[00m\n",
      "│   │   ├── \u001b[01;34mdwi\u001b[00m\n",
      "│   │   └── \u001b[01;34mfunc\u001b[00m\n",
      "│   └── \u001b[01;34mses-test\u001b[00m\n",
      "│       ├── \u001b[01;34manat\u001b[00m\n",
      "│       ├── \u001b[01;34mdwi\u001b[00m\n",
      "│       └── \u001b[01;34mfunc\u001b[00m\n",
      "├── \u001b[01;34msub-10\u001b[00m\n",
      "│   ├── \u001b[01;34mses-retest\u001b[00m\n",
      "│   │   ├── \u001b[01;34manat\u001b[00m\n",
      "│   │   ├── \u001b[01;34mdwi\u001b[00m\n",
      "│   │   └── \u001b[01;34mfunc\u001b[00m\n",
      "│   └── \u001b[01;34mses-test\u001b[00m\n",
      "│       ├── \u001b[01;34manat\u001b[00m\n",
      "│       ├── \u001b[01;34mdwi\u001b[00m\n",
      "│       └── \u001b[01;34mfunc\u001b[00m\n",
      "├── task-covertverbgeneration_bold.json\n",
      "├── task-covertverbgeneration_events.tsv\n",
      "├── task-fingerfootlips_bold.json\n",
      "├── task-fingerfootlips_events.tsv\n",
      "├── task-linebisection_bold.json\n",
      "├── task-overtverbgeneration_bold.json\n",
      "├── task-overtverbgeneration_events.tsv\n",
      "├── task-overtwordrepetition_bold.json\n",
      "└── task-overtwordrepetition_events.tsv\n",
      "\n",
      "93 directories, 13 files\n"
     ]
    }
   ],
   "source": [
    "!tree -L 3 /data/datasets.datalad.org/workshops/nih-2017/ds000114"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When we try to list the contents of these files we get `No such file or directory`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cat: /data/datasets.datalad.org/workshops/nih-2017/ds000114/dwi.bval: No such file or directory\r\n"
     ]
    }
   ],
   "source": [
    "!cat /data/datasets.datalad.org/workshops/nih-2017/ds000114/dwi.bval"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's use datalad to fetch this file and then list it again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;1mget\u001b[0m(\u001b[1;32mok\u001b[0m): /data/datasets.datalad.org/workshops/nih-2017/ds000114/dwi.bval (\u001b[1;35mfile\u001b[0m)\n"
     ]
    }
   ],
   "source": [
    "!datalad get /data/datasets.datalad.org/workshops/nih-2017/ds000114/dwi.bval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0 0 0 0 0 0 1000 1000 1000 1000 1000 1000 1000 1000 1000 1000 1000 1000 1000 1000 1000 1000 1000 1000 1000 1000 1000 1000 1000 1000 1000 1000 1000 1000 1000 1000 1000 1000 1000 1000 1000 1000 1000 1000 1000 1000 1000 1000 1000 1000 1000 1000 1000 1000 1000 1000 1000 1000 1000 1000 1000 1000 1000 1000 1000 1000 1000 1000 1000 1000 \r\n"
     ]
    }
   ],
   "source": [
    "!cat /data/datasets.datalad.org/workshops/nih-2017/ds000114/dwi.bval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;4m/data/datasets.datalad.org/workshops/nih-2017/ds000114\u001b[0m   [annex]  master  2.0.1-11-gf763908b6 2017-08-12/15:55:27  \u001b[1;32m✓\u001b[0m\r\n"
     ]
    }
   ],
   "source": [
    "!datalad ls /data/datasets.datalad.org/workshops/nih-2017/ds000114"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since datalad uses git-annex under the hood, let's try to list things with git-annex."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "git-annex: First run: git-annex init\r\n"
     ]
    }
   ],
   "source": [
    "!git-annex list /data/datasets.datalad.org/workshops/nih-2017/ds000114/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Oops! Git and datalad don't recognize root folders without pointing to an annex or dataset location."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "here\n",
      "|origin\n",
      "||web\n",
      "|||bittorrent\n",
      "||||datalad-archives\n",
      "|||||\n",
      "__X_X sub-01/ses-retest/anat/sub-01_ses-retest_T1w.nii.gz\n",
      "__X_X sub-01/ses-retest/dwi/sub-01_ses-retest_dwi.nii.gz\n",
      "__X_X sub-01/ses-retest/func/sub-01_ses-retest_task-covertverbgeneration_bold.nii.gz\n",
      "__X_X sub-01/ses-retest/func/sub-01_ses-retest_task-fingerfootlips_bold.nii.gz\n",
      "__X_X sub-01/ses-retest/func/sub-01_ses-retest_task-linebisection_bold.nii.gz\n",
      "__X_X sub-01/ses-retest/func/sub-01_ses-retest_task-linebisection_events.tsv\n",
      "__X_X sub-01/ses-retest/func/sub-01_ses-retest_task-overtverbgeneration_bold.nii.gz\n",
      "__X_X sub-01/ses-retest/func/sub-01_ses-retest_task-overtwordrepetition_bold.nii.gz\n",
      "__X_X sub-01/ses-test/anat/sub-01_ses-test_T1w.nii.gz\n",
      "__X_X sub-01/ses-test/dwi/sub-01_ses-test_dwi.nii.gz\n",
      "__X_X sub-01/ses-test/func/sub-01_ses-test_task-covertverbgeneration_bold.nii.gz\n",
      "__X_X sub-01/ses-test/func/sub-01_ses-test_task-fingerfootlips_bold.nii.gz\n",
      "__X_X sub-01/ses-test/func/sub-01_ses-test_task-linebisection_bold.nii.gz\n",
      "__X_X sub-01/ses-test/func/sub-01_ses-test_task-linebisection_events.tsv\n",
      "__X_X sub-01/ses-test/func/sub-01_ses-test_task-overtverbgeneration_bold.nii.gz\n",
      "__X_X sub-01/ses-test/func/sub-01_ses-test_task-overtwordrepetition_bold.nii.gz\n"
     ]
    }
   ],
   "source": [
    "%%bash \n",
    "cd /data/datasets.datalad.org/workshops/nih-2017/ds000114/\n",
    "git-annex list sub-01"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "solution2": "hidden",
    "solution2_first": true
   },
   "source": [
    "## Exercise 6:\n",
    "\n",
    "Show the help for `git annex list` and then use it to list the `dwi*` files in `ds000114`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": true,
    "solution2": "hidden"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "git-annex list - show which remotes contain files\n",
      "\n",
      "Usage: git-annex list [PATH ...] [--allrepos]\n",
      "\n",
      "Available options:\n",
      "  --allrepos               show all repositories, not only remotes\n",
      "  --force                  allow actions that may lose annexed data\n",
      "  -F,--fast                avoid slow operations\n",
      "  -q,--quiet               avoid verbose output\n",
      "  -v,--verbose             allow verbose output (default)\n",
      "  -d,--debug               show debug messages\n",
      "  --no-debug               don't show debug messages\n",
      "  -b,--backend NAME        specify key-value backend to use\n",
      "  -N,--numcopies NUMBER    override default number of copies\n",
      "  --trust REMOTE           override trust setting\n",
      "  --semitrust REMOTE       override trust setting back to default\n",
      "  --untrust REMOTE         override trust setting to untrusted\n",
      "  -c,--config NAME=VALUE   override git configuration setting\n",
      "  --user-agent NAME        override default User-Agent\n",
      "  --trust-glacier          Trust Amazon Glacier inventory\n",
      "  --notify-finish          show desktop notification after transfer finishes\n",
      "  --notify-start           show desktop notification after transfer starts\n",
      "  -i,--in REMOTE           match files present in a remote\n",
      "  -C,--copies REMOTE       skip files with fewer copies\n",
      "  --lackingcopies NUMBER   match files that need more copies\n",
      "  --approxlackingcopies NUMBER\n",
      "                           match files that need more copies (faster)\n",
      "  -B,--inbackend NAME      match files using a key-value backend\n",
      "  --securehash             match files using a cryptographically secure hash\n",
      "  --inallgroup GROUP       match files present in all remotes in a group\n",
      "  --metadata FIELD=VALUE   match files with attached metadata\n",
      "  --want-get               match files the repository wants to get\n",
      "  --want-drop              match files the repository wants to drop\n",
      "  -x,--exclude GLOB        skip files matching the glob pattern\n",
      "  -I,--include GLOB        limit to files matching the glob pattern\n",
      "  --largerthan SIZE        match files larger than a size\n",
      "  --smallerthan SIZE       match files smaller than a size\n",
      "  --not                    negate next option\n",
      "  --and                    both previous and next option must match\n",
      "  --or                     either previous or next option must match\n",
      "  -(                       open group of options\n",
      "  -)                       close group of options\n",
      "  -T,--time-limit TIME     stop after the specified amount of time\n",
      "  -h,--help                Show this help text\n",
      "\n",
      "For details, run: git-annex help list\n"
     ]
    }
   ],
   "source": [
    "!git-annex list --help"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "solution2": "hidden"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "here\n",
      "|origin\n",
      "||web\n",
      "|||bittorrent\n",
      "||||datalad-archives\n",
      "|||||\n",
      "X_X_X dwi.bval\n",
      "__X_X dwi.bvec\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "\n",
    "cd /data/datasets.datalad.org/workshops/nih-2017/ds000114/\n",
    "git-annex list dwi.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# help coomand here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list dwi* files here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also remove content from our local storage using the `drop` command."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;1mdrop\u001b[0m(\u001b[1;32mok\u001b[0m): /data/datasets.datalad.org/workshops/nih-2017/ds000114/dwi.bval (\u001b[1;35mfile\u001b[0m) [checking http://openneuro.s3.amazonaws.com/ds000114/ds000114_R2.0.0/uncompressed/dwi.bval?versionId=null...]\r\n"
     ]
    }
   ],
   "source": [
    "!datalad drop /data/datasets.datalad.org/workshops/nih-2017/ds000114/dwi.bval"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 7:\n",
    "\n",
    "Check where the dwi files are with the annex list command and get the missing files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "here\n",
      "|origin\n",
      "||web\n",
      "|||bittorrent\n",
      "||||datalad-archives\n",
      "|||||\n",
      "__X_X dwi.bval\n",
      "__X_X dwi.bvec\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "\n",
    "cd /data/datasets.datalad.org/workshops/nih-2017/ds000114/\n",
    "git-annex list dwi.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "Total:   0%|          | 0.00/1.58K [00:00<?, ?B/s]\r",
      "Total:  21%|██        | 335/1.58K [00:01<00:05, 233B/s]\r",
      "          \r",
      "Total (1 ok out of 2) 21%|██        | 335/1.58K [00:01<00:05, 233B/s]\r",
      "Total (1 ok out of 2)100%|██████████| 1.58K/1.58K [00:02<00:00, 315B/s]\r",
      "          \r",
      "Total (2 ok out of 2)100%|██████████| 1.58K/1.58K [00:02<00:00, 315B/s]\r",
      "                                                                       \r",
      "get(ok): /data/datasets.datalad.org/workshops/nih-2017/ds000114/dwi.bval (file)\n",
      "get(ok): /data/datasets.datalad.org/workshops/nih-2017/ds000114/dwi.bvec (file)\n",
      "action summary:\n",
      "  get (ok: 2)\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "\n",
    "cd /data/datasets.datalad.org/workshops/nih-2017/ds000114/\n",
    "datalad get dwi.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now we will create and version our own toy dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\u001b[1;37mINFO   \u001b[0m] Creating a new annex repo at /data/mydataset \n",
      "\u001b[1;1mcreate\u001b[0m(\u001b[1;32mok\u001b[0m): /data/mydataset (\u001b[1;35mdataset\u001b[0m)           \n"
     ]
    }
   ],
   "source": [
    "!datalad create /data/mydataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will create a dummy file and add it to the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "Total:   0%|          | 0.00/4.00 [00:00<?, ?B/s]\r",
      "Total: 100%|██████████| 4.00/4.00 [00:00<00:00, 27.3B/s]\r",
      "          \r",
      "Total (1 ok out of 1)100%|██████████| 4.00/4.00 [00:00<00:00, 27.3B/s]\r",
      "                                                        \r",
      "add(ok): /data/mydataset/123 (file)\n",
      "save(ok): /data/mydataset (dataset)\n",
      "action summary:\n",
      "  add (ok: 1)\n",
      "  save (ok: 1)\n"
     ]
    }
   ],
   "source": [
    "%%bash \n",
    "\n",
    "echo \"123\" > /data/mydataset/123\n",
    "datalad add -m \"initial file\" /data/mydataset/123"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "list where the copy is available"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "here\n",
      "|web\n",
      "||bittorrent\n",
      "|||\n",
      "X__ 123\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "\n",
    "cd /data/mydataset\n",
    "git-annex list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[01;34m/data/mydataset\u001b[00m\r\n",
      "└── \u001b[01;36m123\u001b[00m -> .git/annex/objects/pF/Zf/MD5E-s4--ba1f2511fc30423bdbb183fe33f3dd0f/MD5E-s4--ba1f2511fc30423bdbb183fe33f3dd0f\r\n",
      "\r\n",
      "0 directories, 1 file\r\n"
     ]
    }
   ],
   "source": [
    "!tree /data/mydataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "123\r\n"
     ]
    }
   ],
   "source": [
    "!cat /data/mydataset/.git/annex/objects/pF/Zf/MD5E-s4--ba1f2511fc30423bdbb183fe33f3dd0f/MD5E-s4--ba1f2511fc30423bdbb183fe33f3dd0f"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us try removing the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\u001b[1;31mERROR  \u001b[0m] Failed to run ['git', '-c', 'receive.autogc=0', '-c', 'gc.auto=0', 'annex', 'drop', '--json', '123'] under '/data/mydataset'. Exit code=1. out={\"command\":\"drop\",\"wanted\":[],\"note\":\"(Use --force to override this check, or adjust numcopies.)\",\"skipped\":[],\"success\":false,\"key\":\"MD5E-s4--ba1f2511fc30423bdbb183fe33f3dd0f\",\"file\":\"123\"}\r\n",
      "|  err=git-annex: drop: 1 failed\r\n",
      "|  \r\n",
      "[\u001b[1;31mERROR  \u001b[0m] configured minimum number of copies not found [drop(/data/mydataset/123)] \r\n",
      "\u001b[1;1mdrop\u001b[0m(\u001b[1;31merror\u001b[0m): /data/mydataset/123 (\u001b[1;35mfile\u001b[0m) [configured minimum number of copies not found]\r\n"
     ]
    }
   ],
   "source": [
    "!datalad drop /data/mydataset/123"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us try modifying the file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/usr/bin/sh: 1: cannot create /data/mydataset/123: Permission denied\r\n"
     ]
    }
   ],
   "source": [
    "!echo \"321\" > /data/mydataset/123"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The proper way to modify this is to unlock the file, change it and then commit it back"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "unlock(ok): 123 (file)\n",
      "\r",
      "Total:   0%|          | 0.00/4.00 [00:00<?, ?B/s]\r",
      "Total: 100%|██████████| 4.00/4.00 [00:00<00:00, 16.2B/s]\r",
      "          \r",
      "Total (1 ok out of 1)100%|██████████| 4.00/4.00 [00:00<00:00, 16.2B/s]\r",
      "                                                        \r",
      "add(ok): /data/mydataset/123 (file)\n",
      "save(ok): /data/mydataset (dataset)\n",
      "action summary:\n",
      "  add (ok: 1)\n",
      "  save (ok: 1)\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "\n",
    "datalad unlock /data/mydataset/123\n",
    "echo \"321\" > /data/mydataset/123\n",
    "datalad add -m \"add modified file\" /data/mydataset/123"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we try modifying it, we now again get permission denied, because the file is locked."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/usr/bin/sh: 1: cannot create /data/mydataset/123: Permission denied\r\n"
     ]
    }
   ],
   "source": [
    "!echo \"123\" > /data/mydataset/123"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[01;34m/data/mydataset\u001b[00m\r\n",
      "└── \u001b[01;36m123\u001b[00m -> .git/annex/objects/6v/gZ/MD5E-s4--9492fe88f263d58e0b686885e8c98c0e/MD5E-s4--9492fe88f263d58e0b686885e8c98c0e\r\n",
      "\r\n",
      "0 directories, 1 file\r\n"
     ]
    }
   ],
   "source": [
    "!tree /data/mydataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "321\r\n"
     ]
    }
   ],
   "source": [
    "!cat /data/mydataset/.git/annex/objects/6v/gZ/MD5E-s4--9492fe88f263d58e0b686885e8c98c0e/MD5E-s4--9492fe88f263d58e0b686885e8c98c0e"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But the old object is still there. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "123\r\n"
     ]
    }
   ],
   "source": [
    "!cat /data/mydataset/.git/annex/objects/pF/Zf/MD5E-s4--ba1f2511fc30423bdbb183fe33f3dd0f/MD5E-s4--ba1f2511fc30423bdbb183fe33f3dd0f"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The entire history of the repo is available."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "commit c1fef9ff6e8494847a9a36d27fad5877a181c391\n",
      "Author: neuro <neuro>\n",
      "Date:   Wed Aug 16 16:19:23 2017 +0000\n",
      "\n",
      "    add modified file\n",
      "\n",
      "commit 8cdcf1758e34c8b559e8cef1b428803e24ee58a1\n",
      "Author: neuro <neuro>\n",
      "Date:   Wed Aug 16 16:19:09 2017 +0000\n",
      "\n",
      "    initial file\n",
      "\n",
      "commit 7e916743ded0cb337a9064f2c355ca5bc9fc3652\n",
      "Author: neuro <neuro>\n",
      "Date:   Wed Aug 16 16:19:07 2017 +0000\n",
      "\n",
      "    [DATALAD] new dataset\n",
      "\n",
      "commit be710698193b13aea758791b4b9d8bab0cd89183\n",
      "Author: neuro <neuro>\n",
      "Date:   Wed Aug 16 16:19:06 2017 +0000\n",
      "\n",
      "    [DATALAD] Set default backend for all files to be MD5E\n"
     ]
    }
   ],
   "source": [
    "%%bash \n",
    "\n",
    "cd /data/mydataset/\n",
    "git log"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us create a simple script that counts the number of characters in a file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#!/bin/bash\n",
      "cat $1 | wc -c\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "\n",
    "cd /data/mydataset\n",
    "mkdir -p scripts\n",
    "cmd=$(cat << EOM\n",
    "#!/bin/bash\\ncat \\$1 | wc -c\n",
    "EOM\n",
    ")\n",
    "echo -e $cmd > scripts/run.sh\n",
    "chmod +x scripts/run.sh\n",
    "cat scripts/run.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will run the script and add the script and the output to annex."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "Total:   0%|          | 0.00/29.0 [00:00<?, ?B/s]\r",
      "Total:   7%|▋         | 2.00/29.0 [00:00<00:02, 11.7B/s]\r",
      "          \r",
      "Total (1 ok out of 2)  7%|▋         | 2.00/29.0 [00:00<00:02, 11.7B/s]\r",
      "Total (1 ok out of 2)100%|██████████| 29.0/29.0 [00:00<00:00, 16.6B/s]\r",
      "          \r",
      "Total (2 ok out of 2)100%|██████████| 29.0/29.0 [00:00<00:00, 16.6B/s]\r",
      "                                                                      \r",
      "add(ok): /data/mydataset/out (file)\n",
      "add(ok): /data/mydataset/scripts/run.sh (file)\n",
      "add(ok): /data/mydataset/scripts (directory)\n",
      "save(ok): /data/mydataset (dataset)\n",
      "action summary:\n",
      "  add (ok: 3)\n",
      "  save (ok: 1)\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "cd /data/mydataset\n",
    "scripts/run.sh 123 > out\n",
    "datalad add -m \"Added scripts and output\" out scripts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can look at the log again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "commit 8d5e9248cf3347457b9a24891cc1989889391dd8\n",
      "Author: neuro <neuro>\n",
      "Date:   Wed Aug 16 16:19:44 2017 +0000\n",
      "\n",
      "    Added scripts and output\n",
      "\n",
      "commit c1fef9ff6e8494847a9a36d27fad5877a181c391\n",
      "Author: neuro <neuro>\n",
      "Date:   Wed Aug 16 16:19:23 2017 +0000\n",
      "\n",
      "    add modified file\n",
      "\n",
      "commit 8cdcf1758e34c8b559e8cef1b428803e24ee58a1\n",
      "Author: neuro <neuro>\n",
      "Date:   Wed Aug 16 16:19:09 2017 +0000\n",
      "\n",
      "    initial file\n",
      "\n",
      "commit 7e916743ded0cb337a9064f2c355ca5bc9fc3652\n",
      "Author: neuro <neuro>\n",
      "Date:   Wed Aug 16 16:19:07 2017 +0000\n",
      "\n",
      "    [DATALAD] new dataset\n",
      "\n",
      "commit be710698193b13aea758791b4b9d8bab0cd89183\n",
      "Author: neuro <neuro>\n",
      "Date:   Wed Aug 16 16:19:06 2017 +0000\n",
      "\n",
      "    [DATALAD] Set default backend for all files to be MD5E\n"
     ]
    }
   ],
   "source": [
    "%%bash \n",
    "\n",
    "cd /data/mydataset/\n",
    "git log"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can go back to a previous state and check the contents of the `123` file. Note that we return back to current state after this excursion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/data/mydataset/\n",
      "├── 123 -> .git/annex/objects/6v/gZ/MD5E-s4--9492fe88f263d58e0b686885e8c98c0e/MD5E-s4--9492fe88f263d58e0b686885e8c98c0e\n",
      "├── out -> .git/annex/objects/6w/1x/MD5E-s2--48a24b70a0b376535542b996af517398/MD5E-s2--48a24b70a0b376535542b996af517398\n",
      "└── scripts\n",
      "    └── run.sh -> ../.git/annex/objects/G7/VV/MD5E-s27--ddaa7c667769596874750a6eff28a467.sh/MD5E-s27--ddaa7c667769596874750a6eff28a467.sh\n",
      "\n",
      "1 directory, 3 files\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "\n",
    "tree /data/mydataset/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/data/mydataset/\n",
      "└── 123 -> .git/annex/objects/pF/Zf/MD5E-s4--ba1f2511fc30423bdbb183fe33f3dd0f/MD5E-s4--ba1f2511fc30423bdbb183fe33f3dd0f\n",
      "\n",
      "0 directories, 1 file\n",
      "/data/mydataset/\n",
      "├── 123 -> .git/annex/objects/6v/gZ/MD5E-s4--9492fe88f263d58e0b686885e8c98c0e/MD5E-s4--9492fe88f263d58e0b686885e8c98c0e\n",
      "├── out -> .git/annex/objects/6w/1x/MD5E-s2--48a24b70a0b376535542b996af517398/MD5E-s2--48a24b70a0b376535542b996af517398\n",
      "└── scripts\n",
      "    └── run.sh -> ../.git/annex/objects/G7/VV/MD5E-s27--ddaa7c667769596874750a6eff28a467.sh/MD5E-s27--ddaa7c667769596874750a6eff28a467.sh\n",
      "\n",
      "1 directory, 3 files\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Note: checking out '8cdcf1758'.\n",
      "\n",
      "You are in 'detached HEAD' state. You can look around, make experimental\n",
      "changes and commit them, and you can discard any commits you make in this\n",
      "state without impacting any branches by performing another checkout.\n",
      "\n",
      "If you want to create a new branch to retain commits you create, you may\n",
      "do so (now or later) by using -b with the checkout command again. Example:\n",
      "\n",
      "  git checkout -b <new-branch-name>\n",
      "\n",
      "HEAD is now at 8cdcf17... initial file\n",
      "Previous HEAD position was 8cdcf17... initial file\n",
      "Switched to branch 'master'\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "\n",
    "cd /data/mydataset/\n",
    "git checkout 8cdcf1758\n",
    "tree /data/mydataset/\n",
    "git checkout master\n",
    "tree /data/mydataset/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "solution2": "hidden",
    "solution2_first": true
   },
   "source": [
    "## Exercise 8:\n",
    "\n",
    "- Copy one binary brainmask image file from ds000114/derivatives/fmriprep into mydataset\n",
    "  - To do so first you should install the dataset recursively\n",
    "  - And then get the file\n",
    "- Add to version control\n",
    "- use git-annex to list where that file can be found\n",
    "- Add a simple python script to count and print the number of non-zero voxels\n",
    "- Store the output into a new out file\n",
    "- Use datalad to add everything to the repository"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "solution2": "hidden"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "get(notneeded): /data/datasets.datalad.org/workshops/nih-2017/ds000114/derivatives/fmriprep (dataset) [already installed]\n",
      "/data/datasets.datalad.org/workshops/nih-2017/ds000114/derivatives/fmriprep/\n",
      "├── sub-01\n",
      "├── sub-01.html -> .git/annex/objects/MF/jw/MD5E-s20077561--03ecea8730492d537e050941bdf654bf.html/MD5E-s20077561--03ecea8730492d537e050941bdf654bf.html\n",
      "├── sub-02\n",
      "├── sub-02.html -> .git/annex/objects/99/j3/MD5E-s19975906--5ede67fcdad59b65a02f572360db2863.html/MD5E-s19975906--5ede67fcdad59b65a02f572360db2863.html\n",
      "├── sub-03\n",
      "├── sub-03.html -> .git/annex/objects/z4/8w/MD5E-s20227534--64e1a981338e8fb9c87f026a79a34785.html/MD5E-s20227534--64e1a981338e8fb9c87f026a79a34785.html\n",
      "├── sub-04\n",
      "├── sub-04.html -> .git/annex/objects/qF/J1/MD5E-s22389786--2954e6ece2a825c0008e9b1dcfcaf0a6.html/MD5E-s22389786--2954e6ece2a825c0008e9b1dcfcaf0a6.html\n",
      "├── sub-05\n",
      "├── sub-05.html -> .git/annex/objects/6G/Z6/MD5E-s22109848--70a1908c811102744f39b87ae03216a2.html/MD5E-s22109848--70a1908c811102744f39b87ae03216a2.html\n",
      "├── sub-06\n",
      "├── sub-06.html -> .git/annex/objects/k9/gx/MD5E-s21892649--c22445c2264626ea8537b440a280d240.html/MD5E-s21892649--c22445c2264626ea8537b440a280d240.html\n",
      "├── sub-07\n",
      "├── sub-07.html -> .git/annex/objects/4v/vV/MD5E-s19939423--f9c96cb528fb62ebde2d33bb6a69cb8b.html/MD5E-s19939423--f9c96cb528fb62ebde2d33bb6a69cb8b.html\n",
      "├── sub-08\n",
      "├── sub-08.html -> .git/annex/objects/Kj/P0/MD5E-s21484045--93abf611fe734778dedbfb65ce983e42.html/MD5E-s21484045--93abf611fe734778dedbfb65ce983e42.html\n",
      "├── sub-09\n",
      "├── sub-09.html -> .git/annex/objects/9M/J0/MD5E-s21261086--9eaa82886171bc130c560ad95e9399ce.html/MD5E-s21261086--9eaa82886171bc130c560ad95e9399ce.html\n",
      "├── sub-10\n",
      "└── sub-10.html -> .git/annex/objects/54/fp/MD5E-s19211083--9cd49ee07578dfdf58246243af5faf16.html/MD5E-s19211083--9cd49ee07578dfdf58246243af5faf16.html\n",
      "\n",
      "10 directories, 10 files\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO] Installing <Dataset path=/data/datasets.datalad.org/workshops/nih-2017/ds000114/derivatives/fmriprep> recursively \n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "\n",
    "datalad install -r /data/datasets.datalad.org/workshops/nih-2017/ds000114/derivatives/fmriprep/\n",
    "tree -L 1 /data/datasets.datalad.org/workshops/nih-2017/ds000114/derivatives/fmriprep/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "solution2": "hidden"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/data/datasets.datalad.org/workshops/nih-2017/ds000114/derivatives/fmriprep/sub-01/anat/\n",
      "├── sub-01_t1w_brainmask.nii.gz -> ../../.git/annex/objects/jJ/Wz/MD5E-s93130--2572248880aa5978f8d4049feff2282a.nii.gz/MD5E-s93130--2572248880aa5978f8d4049feff2282a.nii.gz\n",
      "├── sub-01_t1w_class-csf_probtissue.nii.gz -> ../../.git/annex/objects/jg/4g/MD5E-s3066660--67fbe37bd5440773a9951eb116d9192c.nii.gz/MD5E-s3066660--67fbe37bd5440773a9951eb116d9192c.nii.gz\n",
      "├── sub-01_t1w_class-gm_probtissue.nii.gz -> ../../.git/annex/objects/80/65/MD5E-s3395247--d4e1b01832f3514a796788ff9d814134.nii.gz/MD5E-s3395247--d4e1b01832f3514a796788ff9d814134.nii.gz\n",
      "├── sub-01_t1w_class-wm_probtissue.nii.gz -> ../../.git/annex/objects/W9/v4/MD5E-s3107921--50711fd1ad6729e9d8f8b0c0611578f8.nii.gz/MD5E-s3107921--50711fd1ad6729e9d8f8b0c0611578f8.nii.gz\n",
      "├── sub-01_t1w_dtissue.nii.gz -> ../../.git/annex/objects/gZ/qJ/MD5E-s241087--cedef0e31b37f728f09c92c4bce16f61.nii.gz/MD5E-s241087--cedef0e31b37f728f09c92c4bce16f61.nii.gz\n",
      "├── sub-01_t1w_inflated.l.surf.gii -> ../../.git/annex/objects/Gk/qz/MD5E-s2552048--2f46d819ea04a6b58651d0371f0f4884.surf.gii/MD5E-s2552048--2f46d819ea04a6b58651d0371f0f4884.surf.gii\n",
      "├── sub-01_t1w_inflated.r.surf.gii -> ../../.git/annex/objects/kv/38/MD5E-s2577053--0544f6494c3900b5b0c2ebe3e64ff75f.surf.gii/MD5E-s2577053--0544f6494c3900b5b0c2ebe3e64ff75f.surf.gii\n",
      "├── sub-01_t1w_midthickness.l.surf.gii -> ../../.git/annex/objects/Xg/jf/MD5E-s2541962--12837037044e1d065c271ab7a46f2ec0.surf.gii/MD5E-s2541962--12837037044e1d065c271ab7a46f2ec0.surf.gii\n",
      "├── sub-01_t1w_midthickness.r.surf.gii -> ../../.git/annex/objects/F5/Zw/MD5E-s2566814--2c8a72c3dbd38626185a886c10dba6e8.surf.gii/MD5E-s2566814--2c8a72c3dbd38626185a886c10dba6e8.surf.gii\n",
      "├── sub-01_t1w_pial.l.surf.gii -> ../../.git/annex/objects/kM/25/MD5E-s2546954--50afda07aaf3e09c971b0b2de3ecefa5.surf.gii/MD5E-s2546954--50afda07aaf3e09c971b0b2de3ecefa5.surf.gii\n",
      "├── sub-01_t1w_pial.r.surf.gii -> ../../.git/annex/objects/19/14/MD5E-s2570790--6ee98dfaccd234031e68ae977dd67905.surf.gii/MD5E-s2570790--6ee98dfaccd234031e68ae977dd67905.surf.gii\n",
      "├── sub-01_t1w_preproc.nii.gz -> ../../.git/annex/objects/FF/Xw/MD5E-s34678574--041927b06af0f79b15f19c0cb9d4c42e.nii.gz/MD5E-s34678574--041927b06af0f79b15f19c0cb9d4c42e.nii.gz\n",
      "├── sub-01_t1w_smoothwm.l.surf.gii -> ../../.git/annex/objects/14/kq/MD5E-s2536643--0276ca06137e6022633a9c94743e2baa.surf.gii/MD5E-s2536643--0276ca06137e6022633a9c94743e2baa.surf.gii\n",
      "├── sub-01_t1w_smoothwm.r.surf.gii -> ../../.git/annex/objects/z8/46/MD5E-s2561463--ae614e10c989d893cade45851a1e66d9.surf.gii/MD5E-s2561463--ae614e10c989d893cade45851a1e66d9.surf.gii\n",
      "├── sub-01_t1w_space-mni152nlin2009casym_brainmask.nii.gz -> ../../.git/annex/objects/g7/M1/MD5E-s134764--c66e4c5075182457ba1560c8c839669b.nii.gz/MD5E-s134764--c66e4c5075182457ba1560c8c839669b.nii.gz\n",
      "├── sub-01_t1w_space-mni152nlin2009casym_class-csf_probtissue.nii.gz -> ../../.git/annex/objects/9m/Z1/MD5E-s7230742--0b2f3950c21a9ae691e550c8427b64d6.nii.gz/MD5E-s7230742--0b2f3950c21a9ae691e550c8427b64d6.nii.gz\n",
      "├── sub-01_t1w_space-mni152nlin2009casym_class-gm_probtissue.nii.gz -> ../../.git/annex/objects/WQ/qw/MD5E-s7370648--c4cdb6c3f94b83efaae1b8eb6d47f5a9.nii.gz/MD5E-s7370648--c4cdb6c3f94b83efaae1b8eb6d47f5a9.nii.gz\n",
      "├── sub-01_t1w_space-mni152nlin2009casym_class-wm_probtissue.nii.gz -> ../../.git/annex/objects/0J/kj/MD5E-s7243055--a722f9f7ef533ceeb480b343c5a59a56.nii.gz/MD5E-s7243055--a722f9f7ef533ceeb480b343c5a59a56.nii.gz\n",
      "├── sub-01_t1w_space-mni152nlin2009casym_dtissue.nii.gz -> ../../.git/annex/objects/k4/vJ/MD5E-s434565--9390accd81f24bbd9c32b03e66369955.nii.gz/MD5E-s434565--9390accd81f24bbd9c32b03e66369955.nii.gz\n",
      "├── sub-01_t1w_space-mni152nlin2009casym_preproc.nii.gz -> ../../.git/annex/objects/6M/Gp/MD5E-s8490746--5bb6a33c5d41bc27eefc3ec83a8a6686.nii.gz/MD5E-s8490746--5bb6a33c5d41bc27eefc3ec83a8a6686.nii.gz\n",
      "└── sub-01_t1w_space-mni152nlin2009casym_warp.h5 -> ../../.git/annex/objects/Zm/WZ/MD5E-s102374852--88f500a4cf26d98e913df8c25d41a87d.h5/MD5E-s102374852--88f500a4cf26d98e913df8c25d41a87d.h5\n",
      "\n",
      "0 directories, 21 files\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "\n",
    "tree -L 1 /data/datasets.datalad.org/workshops/nih-2017/ds000114/derivatives/fmriprep/sub-01/anat/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "solution2": "hidden"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "get(notneeded): /data/datasets.datalad.org/workshops/nih-2017/ds000114/derivatives/fmriprep/sub-01/anat/sub-01_t1w_brainmask.nii.gz (file) [already present]\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "\n",
    "datalad get /data/datasets.datalad.org/workshops/nih-2017/ds000114/derivatives/fmriprep/sub-01/anat/sub-01_t1w_brainmask.nii.gz\n",
    "cp /data/datasets.datalad.org/workshops/nih-2017/ds000114/derivatives/fmriprep/sub-01/anat/sub-01_t1w_brainmask.nii.gz \\\n",
    "   /data/mydataset/brainmask.nii.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "solution2": "hidden"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting /data/mydataset/scripts/count_voxels.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile /data/mydataset/scripts/count_voxels.py\n",
    "\n",
    "import nibabel as nb\n",
    "import sys\n",
    "print(nb.load(sys.argv[1]).get_data().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "solution2": "hidden"
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "cd /data/mydataset/\n",
    "python scripts/count_voxels.py brainmask.nii.gz > mask_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "solution2": "hidden"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/data/mydataset/\n",
      "├── 123 -> .git/annex/objects/6v/gZ/MD5E-s4--9492fe88f263d58e0b686885e8c98c0e/MD5E-s4--9492fe88f263d58e0b686885e8c98c0e\n",
      "├── brainmask.nii.gz\n",
      "├── mask_count\n",
      "├── out -> .git/annex/objects/6w/1x/MD5E-s2--48a24b70a0b376535542b996af517398/MD5E-s2--48a24b70a0b376535542b996af517398\n",
      "└── scripts\n",
      "    ├── count_voxels.py\n",
      "    └── run.sh -> ../.git/annex/objects/G7/VV/MD5E-s27--ddaa7c667769596874750a6eff28a467.sh/MD5E-s27--ddaa7c667769596874750a6eff28a467.sh\n",
      "\n",
      "1 directory, 6 files\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "\n",
    "tree /data/mydataset/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "solution2": "hidden"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "Total:   0%|          | 0.00/93.2K [00:00<?, ?B/s]\r",
      "Total: 100%|█████████▉| 93.1K/93.2K [00:00<00:00, 601KB/s]\r",
      "          \r",
      "Total (1 ok out of 3)100%|█████████▉| 93.1K/93.2K [00:00<00:00, 601KB/s]\r",
      "          \r",
      "Total (2 ok out of 3)100%|█████████▉| 93.2K/93.2K [00:00<00:00, 601KB/s]\r",
      "          \r",
      "Total (3 ok out of 3)100%|██████████| 93.2K/93.2K [00:00<00:00, 601KB/s]\r",
      "                                                          \r",
      "add(ok): /data/mydataset/brainmask.nii.gz (file)\n",
      "add(ok): /data/mydataset/scripts/count_voxels.py (file)\n",
      "add(ok): /data/mydataset/mask_count (file)\n",
      "save(ok): /data/mydataset (dataset)\n",
      "action summary:\n",
      "  add (ok: 3)\n",
      "  save (ok: 1)\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "\n",
    "cd /data/mydataset/\n",
    "datalad add -m \"added brainmask, script, and output\" brainmask.nii.gz scripts/count_voxels.py mask_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "solution2": "hidden"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "whereis brainmask.nii.gz (1 copy) \n",
      "  \tb0f579af-62f8-4f5d-9187-690bd6e71d34 -- neuro@9600bfbe8fce:/data/mydataset [here]\n",
      "ok\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "cd /data/mydataset/\n",
    "git-annex whereis brainmask.nii.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
