{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Versioning data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook we will look at using tools to version data. Specifically, [git-annex](https://git-annex.branchable.com/) and [datalad](http://datalad.org/)\n",
    "\n",
    "- Initializing, searching, downloading, and removing (dropping) a datalad dataset\n",
    "- Creating and modifying a datalad dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's start by using the shell version\n",
    "\n",
    "The `!` character at the beginning of the next line indicates that the subsequent command will be executed in a shell. In the following section we are generating the help command. The `--help-np` option ensures that the underlying help output does not page. Since the notebook does not support paging, we will be using this option."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Usage: datalad [global-opts] command [command-opts]\n",
      "\n",
      "Comprehensive data management solution\n",
      "\n",
      "DataLad provides a unified data distribution system built on the Git\n",
      "and Git-annex. DataLad command line tools allow to manipulate (obtain,\n",
      "create, update, publish, etc.) datasets and provide a comprehensive\n",
      "toolbox for joint management of data and code. Compared to Git/annex\n",
      "it primarly extends their functionality to transparently and\n",
      "simultaneously work with multiple inter-related repositories.\n",
      "\n",
      "*Commands for dataset operations*\n",
      "\n",
      "  create\n",
      "      Create a new dataset from scratch\n",
      "  install\n",
      "      Install a dataset from a (remote) source\n",
      "  get\n",
      "      Get any dataset content (files/directories/subdatasets)\n",
      "  add\n",
      "      Add files/directories to an existing dataset\n",
      "  publish\n",
      "      Publish a dataset to a known sibling\n",
      "  uninstall\n",
      "      Uninstall subdatasets\n",
      "  drop\n",
      "      Drop file content from datasets\n",
      "  remove\n",
      "      Remove components from datasets\n",
      "  update\n",
      "      Update a dataset from a sibling\n",
      "  create-sibling\n",
      "      Create a dataset sibling on a UNIX-like SSH-accessible machine\n",
      "  create-sibling-github\n",
      "      Create dataset sibling on Github\n",
      "  unlock\n",
      "      Unlock file(s) of a dataset\n",
      "  save\n",
      "      Save the current state of a dataset\n",
      "\n",
      "*Commands for metadata handling*\n",
      "\n",
      "  search\n",
      "      Search dataset metadata\n",
      "  metadata\n",
      "      Metadata reporting for files and entire datasets\n",
      "  aggregate-metadata\n",
      "      Aggregate metadata of one or more datasets for later query\n",
      "  extract-metadata\n",
      "      Run one or more of DataLad's metadata extractors on a dataset or file\n",
      "\n",
      "*Miscellaneous commands*\n",
      "\n",
      "  test\n",
      "      Run internal DataLad (unit)tests\n",
      "  ls\n",
      "      List summary information about URLs and dataset(s)\n",
      "  clean\n",
      "      Clean up after DataLad (possible temporary files etc.)\n",
      "  add-archive-content\n",
      "      Add content of an archive under git annex control\n",
      "  download-url\n",
      "      Download content\n",
      "  run\n",
      "      Run an arbitrary shell command and record its impact on a dataset\n",
      "  rerun\n",
      "      Re-execute previous `datalad run` commands\n",
      "  run-procedure\n",
      "      Run prepared procedures (DataLad scripts) on a dataset\n",
      "\n",
      "*Plugins*\n",
      "\n",
      "  wtf\n",
      "      Generate a report about the DataLad installation and configuration\n",
      "  check-dates\n",
      "      Find repository dates that are more recent than a reference date\n",
      "  no-annex\n",
      "      Configure a dataset to never put some content into the dataset's\n",
      "      annex\n",
      "  addurls\n",
      "      Create and update a dataset from a list of URLs\n",
      "  add-readme\n",
      "      Add basic information about DataLad datasets to a README file\n",
      "  export-archive\n",
      "      Export the content of a dataset as a TAR/ZIP archive\n",
      "  export-to-figshare\n",
      "      Export the content of a dataset as a ZIP archive to figshare\n",
      "\n",
      "*Plumbing commands*\n",
      "\n",
      "  annotate-paths\n",
      "      Analyze and act upon input paths\n",
      "  clone\n",
      "      Obtain a dataset copy from a URL or local source (path)\n",
      "  create-test-dataset\n",
      "      Create test (meta-)dataset\n",
      "  diff\n",
      "      Report changes of dataset components\n",
      "  siblings\n",
      "      Manage sibling configuration\n",
      "  sshrun\n",
      "      Run command on remote machines via SSH\n",
      "  subdatasets\n",
      "      Report subdatasets and their properties\n",
      "\n",
      "*General information*\n",
      "\n",
      "Detailed usage information for individual commands is available via\n",
      "command-specific --help, i.e.: datalad <command> --help\n",
      "\n",
      "\n",
      "*Global options*\n",
      "  -l LEVEL, --log-level LEVEL\n",
      "                        set logging verbosity level. Choose among critical,\n",
      "                        error, warning, info, debug. Also you can specify an\n",
      "                        integer <10 to provide even more debugging information\n",
      "  --pbs-runner {condor}\n",
      "                        execute command by scheduling it via available PBS.\n",
      "                        For settings, config file will be consulted\n",
      "  -C PATH               run as if datalad was started in <path> instead of the\n",
      "                        current working directory. When multiple -C options\n",
      "                        are given, each subsequent non-absolute -C <path> is\n",
      "                        interpreted relative to the preceding -C <path>. This\n",
      "                        option affects the interpretations of the path names\n",
      "                        in that they are made relative to the working\n",
      "                        directory caused by the -C option\n",
      "  --version             show the program's version and license information\n",
      "  --dbg                 enter Python debugger when uncaught exception happens\n",
      "  --idbg                enter IPython debugger when uncaught exception happens\n",
      "  -c KEY=VALUE          configuration variable setting. Overrides any\n",
      "                        configuration read from a file, but is potentially\n",
      "                        overridden itself by configuration variables in the\n",
      "                        process environment.\n",
      "  -f {default,json,json_pp,tailored,'<template>'}, --output-format {default,json,json_pp,tailored,'<template>'}\n",
      "                        select format for returned command results. 'default'\n",
      "                        give one line per result reporting action, status,\n",
      "                        path and an optional message; 'json' renders a JSON\n",
      "                        object with all properties for each result (one per\n",
      "                        line); 'json_pp' pretty-prints JSON spanning multiple\n",
      "                        lines; 'tailored' enables a command-specific rendering\n",
      "                        style that is typically tailored to human consumption\n",
      "                        (no result output otherwise), '<template>' reports any\n",
      "                        value(s) of any result properties in any format\n",
      "                        indicated by the template (e.g. '{path}'; compare with\n",
      "                        JSON output for all key-value choices). The template\n",
      "                        syntax follows the Python \"format() language\". It is\n",
      "                        possible to report individual dictionary values, e.g.\n",
      "                        '{metadata[name]}'. If a 2nd-level key contains a\n",
      "                        colon, e.g. 'music:Genre', ':' must be substituted by\n",
      "                        '#' in the template, like so:\n",
      "                        '{metadata[music#Genre]}'.\n",
      "  --report-status {success,failure,ok,notneeded,impossible,error}\n",
      "                        constrain command result report to records matching\n",
      "                        the given status. 'success' is a synonym for 'ok' OR\n",
      "                        'notneeded', 'failure' stands for 'impossible' OR\n",
      "                        'error'.\n",
      "  --report-type {dataset,file}\n",
      "                        constrain command result report to records matching\n",
      "                        the given type. Can be given more than once to match\n",
      "                        multiple types.\n",
      "  --on-failure {ignore,continue,stop}\n",
      "                        when an operation fails: 'ignore' and continue with\n",
      "                        remaining operations, the error is logged but does not\n",
      "                        lead to a non-zero exit code of the command;\n",
      "                        'continue' works like 'ignore', but an error causes a\n",
      "                        non-zero exit code; 'stop' halts on first failure and\n",
      "                        yields non-zero exit code. A failure is any result\n",
      "                        with status 'impossible' or 'error'.\n",
      "  --proc-pre <PROCEDURE NAME> [ARGS ...]\n",
      "                        Dataset procedure to run before the main command (see\n",
      "                        run-procedure command for details). This option can be\n",
      "                        given more than once to run multiple procedures in the\n",
      "                        order in which they were given. It is important to\n",
      "                        specify the target dataset via the --dataset argument\n",
      "                        of the main command.\n",
      "  --proc-post <PROCEDURE NAME> [ARGS ...]\n",
      "                        Like --proc-pre, but procedures are executed after the\n",
      "                        main command has finished.\n",
      "  --cmd                 syntactical helper that can be used to end the list of\n",
      "                        global command line options before the subcommand\n",
      "                        label. Options like --proc-pre can take an arbitrary\n",
      "                        number of arguments and may require to be followed by\n",
      "                        a single --cmd in order to enable identification of\n",
      "                        the subcommand.\n",
      "  -h, --help, --help-np\n",
      "                        show this help message. --help-np forcefully disables\n",
      "                        the use of a pager for displaying the help message\n",
      "\n",
      "\"Be happy!\"\n",
      "\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!datalad --help-np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "solution2": "shown",
    "solution2_first": true
   },
   "source": [
    "## Exercise 1: Generate the help for the install command of datalad."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true,
    "solution2": "shown"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Usage: datalad install [-h] [-s SOURCE] [-d DATASET] [-g] [-D DESCRIPTION]\n",
      "                       [-r] [--recursion-limit LEVELS] [--nosave] [--reckless]\n",
      "                       [-J NJOBS]\n",
      "                       [PATH [PATH ...]]\n",
      "\n",
      "Install a dataset from a (remote) source.\n",
      "\n",
      "This command creates a local sibling of an existing dataset from a\n",
      "(remote) location identified via a URL or path. Optional recursion into\n",
      "potential subdatasets, and download of all referenced data is supported.\n",
      "The new dataset can be optionally registered in an existing\n",
      "superdataset by identifying it via the DATASET argument (the new\n",
      "dataset's path needs to be located within the superdataset for that).\n",
      "\n",
      "It is recommended to provide a brief description to label the dataset's\n",
      "nature *and* location, e.g. \"Michael's music on black laptop\". This helps\n",
      "humans to identify data locations in distributed scenarios.  By default an\n",
      "identifier comprised of user and machine name, plus path will be generated.\n",
      "\n",
      "When only partial dataset content shall be obtained, it is recommended to\n",
      "use this command without the GET-DATA flag, followed by a\n",
      "`get` operation to obtain the desired data.\n",
      "\n",
      "NOTE\n",
      "  Power-user info: This command uses git clone, and\n",
      "  git annex init to prepare the dataset. Registering to a\n",
      "  superdataset is performed via a git submodule add operation\n",
      "  in the discovered superdataset.\n",
      "\n",
      "*Arguments*\n",
      "  PATH                  path/name of the installation target. If no PATH is\n",
      "                        provided a destination path will be derived from a\n",
      "                        source URL similar to git clone. [Default: None]\n",
      "\n",
      "*Options*\n",
      "  -h, --help, --help-np\n",
      "                        show this help message. --help-np forcefully disables\n",
      "                        the use of a pager for displaying the help message\n",
      "  -s SOURCE, --source SOURCE\n",
      "                        URL or local path of the installation source.\n",
      "                        Constraints: value must be a string [Default: None]\n",
      "  -d DATASET, --dataset DATASET\n",
      "                        specify the dataset to perform the install operation\n",
      "                        on. If no dataset is given, an attempt is made to\n",
      "                        identify the dataset in a parent directory of the\n",
      "                        current working directory and/or the PATH given.\n",
      "                        Constraints: Value must be a Dataset or a valid\n",
      "                        identifier of a Dataset (e.g. a path) [Default: None]\n",
      "  -g, --get-data        if given, obtain all data content too. [Default:\n",
      "                        False]\n",
      "  -D DESCRIPTION, --description DESCRIPTION\n",
      "                        short description to use for a dataset location. Its\n",
      "                        primary purpose is to help humans to identify a\n",
      "                        dataset copy (e.g., \"mike's dataset on lab server\").\n",
      "                        Note that when a dataset is published, this\n",
      "                        information becomes available on the remote side.\n",
      "                        Constraints: value must be a string [Default: None]\n",
      "  -r, --recursive       if set, recurse into potential subdataset. [Default:\n",
      "                        False]\n",
      "  --recursion-limit LEVELS\n",
      "                        limit recursion into subdataset to the given number of\n",
      "                        levels. Constraints: value must be convertible to type\n",
      "                        'int' [Default: None]\n",
      "  --nosave              by default all modifications to a dataset are\n",
      "                        immediately saved. Giving this option will disable\n",
      "                        this behavior. [Default: True]\n",
      "  --reckless            Set up the dataset to be able to obtain content in the\n",
      "                        cheapest/fastest possible way, even if this poses a\n",
      "                        potential risk the data integrity (e.g. hardlink files\n",
      "                        from a local clone of the dataset). Use with care, and\n",
      "                        limit to \"read-only\" use cases. With this flag the\n",
      "                        installed dataset will be marked as untrusted.\n",
      "                        [Default: False]\n",
      "  -J NJOBS, --jobs NJOBS\n",
      "                        how many parallel jobs (where possible) to use.\n",
      "                        Constraints: value must be convertible to type 'int',\n",
      "                        or value must be one of ('auto',) [Default: None]\n",
      "\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!datalad install --help-np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will install the datalad metadataset. Note this only installs the top level directory structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "install(ok): /data/datasets.datalad.org (dataset)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO] Cloning http://datasets.datalad.org/ to '/data/datasets.datalad.org' \n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "\n",
    "cd /data\n",
    "datalad install /// "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "solution2": "hidden",
    "solution2_first": true
   },
   "source": [
    "## Exercise 2\n",
    "\n",
    "List the contents of the installed dataset using the tree command up to three levels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "solution2": "hidden"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/data/datasets.datalad.org/\n",
      "├── abide\n",
      "├── abide2\n",
      "├── adhd200\n",
      "├── corr\n",
      "├── crcns\n",
      "├── datapackage.json\n",
      "├── dbic\n",
      "├── devel\n",
      "├── dicoms\n",
      "├── hbnssi\n",
      "├── indi\n",
      "├── kaggle\n",
      "├── labs\n",
      "├── neurovault\n",
      "├── nidm\n",
      "├── openfmri\n",
      "└── workshops\n",
      "\n",
      "16 directories, 1 file\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "\n",
    "tree -L 3 /data/datasets.datalad.org/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "solution2": "hidden"
   },
   "source": [
    "We note that only the top level datasets are created."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another useful command is to `search` for information across datalad datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "solution2": "hidden",
    "solution2_first": true
   },
   "source": [
    "## Exercise 3:\n",
    "\n",
    "Display the help for the search command"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true,
    "solution2": "hidden"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Usage: datalad search [-h] [-d DATASET] [--reindex]\n",
      "                      [--max-nresults MAX_NRESULTS]\n",
      "                      [--mode {egrep,textblob,autofield}] [--full-record]\n",
      "                      [--show-keys {name,short,full}] [--show-query]\n",
      "                      [QUERY [QUERY ...]]\n",
      "\n",
      "Search dataset metadata\n",
      "\n",
      "DataLad can search metadata extracted from a dataset and/or aggregated into\n",
      "a superdataset (see the AGGREGATE-METADATA command). This makes it\n",
      "possible to discover datasets, or individual files in a dataset even when\n",
      "they are not available locally.\n",
      "\n",
      "Ultimately DataLad metadata are a graph of linked data structures. However,\n",
      "this command does not (yet) support queries that can exploit all information\n",
      "stored in the metadata. At the moment three search modes are implemented that\n",
      "represent different trade-offs between the expressiveness of a query and\n",
      "the computational and storage resources required to execute a query.\n",
      "\n",
      "- egrep (default)\n",
      "\n",
      "- textblob\n",
      "\n",
      "- autofield\n",
      "\n",
      "An alternative default mode can be configured by tuning the\n",
      "Configuration variable 'datalad.search.default-mode'::\n",
      "\n",
      "  [datalad \"search\"]\n",
      "    default-mode = egrep\n",
      "\n",
      "Each search mode has its own default configuration for what kind of\n",
      "documents to query. The respective default can be changed via configuration\n",
      "Variables::\n",
      "\n",
      "  [datalad \"search\"]\n",
      "    index-<mode_name>-documenttype = (all|datasets|files)\n",
      "\n",
      "*Mode: egrep*\n",
      "\n",
      "This search mode is completely ignorant of the metadata structure, and\n",
      "simply performs matching of a search pattern against a flat\n",
      "string-representation of metadata. This mode is advantageous when the\n",
      "query is simple and the metadata structure is irrelevant. Moreover,\n",
      "this mode does not require a search index, hence results can be reported\n",
      "without an initial latency for building a search index when the underlying\n",
      "metadata has changed (e.g. due to a dataset update). By default, this\n",
      "search mode only considers datasets and does not investigate records\n",
      "for individual files for speed reasons.\n",
      "\n",
      "Search results are reported in the order in which they were discovered.\n",
      "\n",
      "Examples:\n",
      "\n",
      "  Query for (what happens to be) an author::\n",
      "\n",
      "    % datalad search haxby\n",
      "\n",
      "  Queries are case-insensitive and can be regular expressions. The\n",
      "  following queries for datasets with (what happens to be) two\n",
      "  particular authors::\n",
      "\n",
      "    % datalad search halchenko.*haxby\n",
      "\n",
      "  However, this search mode performs NO analysis of the metadata content.\n",
      "  Therefore queries can easily fail to match. For example, the above\n",
      "  query implicitly assumes that authors are listed in alphabetical order.\n",
      "  If that is the case (which may or may not be true), the following query\n",
      "  would yield NO hits::\n",
      "\n",
      "    % datalad search haxby.*halchenko\n",
      "\n",
      "  The TEXTBLOB search mode represents an alternative that is more\n",
      "  robust in such cases.\n",
      "\n",
      "*Mode: textblob*\n",
      "\n",
      "This search mode is very similar to the EGREP mode, but with a few key\n",
      "differences. A search index is built from the string-representation of\n",
      "metadata records. By default, only datasets are included in this index, hence\n",
      "the indexing is usually completed within a few seconds, even for hundreds\n",
      "of datasets. This mode uses its own query language (not regular expressions)\n",
      "that is similar to other search engines. It supports logical conjunctions\n",
      "and fuzzy search terms. More information on this is available from the Whoosh\n",
      "Project (search engine implementation):\n",
      "\n",
      "  - Description of the Whoosh query language:\n",
      "    http://whoosh.readthedocs.io/en/latest/querylang.html)\n",
      "\n",
      "  - Description of a number of query language customizations that are\n",
      "    enabled in DataLad, such as, fuzzy term matching:\n",
      "    http://whoosh.readthedocs.io/en/latest/parsing.html#common-customizations\n",
      "\n",
      "Importantly, search hits are scored and reported in order of descending\n",
      "relevance, hence limiting the number of search results is more meaningful\n",
      "than in the 'egrep' mode and can also reduce the query duration.\n",
      "\n",
      "Examples:\n",
      "\n",
      "  Search for (what happens to be) two authors, regardless of the order in\n",
      "  which those names appear in the metadata::\n",
      "\n",
      "    % datalad search --mode textblob halchenko haxby\n",
      "\n",
      "  Fuzzy search when you only have an approximate idea what you are looking\n",
      "  for or how it is spelled::\n",
      "\n",
      "    % datalad search --mode textblob haxbi~\n",
      "\n",
      "  Very fuzzy search, when you are basically only confident about the first\n",
      "  two characters and how it sounds approximately (or more precisely: allow\n",
      "  for three edits and require matching of the first two characters)::\n",
      "\n",
      "    % datalad search --mode textblob haksbi~3/2\n",
      "\n",
      "  Combine fuzzy search with logical constructs::\n",
      "\n",
      "    % datalad search --mode textblob 'haxbi~ AND (hanke OR halchenko)'\n",
      "\n",
      "*Mode: autofield*\n",
      "\n",
      "This mode is similar to the 'textblob' mode, but builds a vastly more\n",
      "detailed search index that represents individual metadata variables as\n",
      "individual fields. By default, this search index includes records for\n",
      "datasets and individual fields, hence it can grow very quickly into\n",
      "a huge structure that can easily take an hour or more to build and require\n",
      "more than a GB of storage. However, limiting it to documents on datasets\n",
      "(see above) retains the enhanced expressiveness of queries while\n",
      "dramatically reducing the resource demands.\n",
      "\n",
      "Examples:\n",
      "\n",
      "  List names of search index fields (auto-discovered from the set of\n",
      "  indexed datasets)::\n",
      "\n",
      "    % datalad search --mode autofield --show-keys name\n",
      "\n",
      "  Fuzzy search for datasets with an author that is specified in a particular\n",
      "  metadata field::\n",
      "\n",
      "    % datalad search --mode autofield bids.author:haxbi~ type:dataset\n",
      "\n",
      "  Search for individual files that carry a particular description\n",
      "  prefix in their 'nifti1' metadata::\n",
      "\n",
      "    % datalad search --mode autofield nifti1.description:FSL* type:file\n",
      "\n",
      "*Reporting*\n",
      "\n",
      "Search hits are returned as standard DataLad results. On the command line\n",
      "the '--output-format' (or '-f') option can be used to tweak results for\n",
      "further processing.\n",
      "\n",
      "Examples:\n",
      "\n",
      "  Format search hits as a JSON stream (one hit per line)::\n",
      "\n",
      "    % datalad -f json search haxby\n",
      "\n",
      "  Custom formatting: which terms matched the query of particular\n",
      "  results. Useful for investigating fuzzy search results::\n",
      "\n",
      "    $ datalad -f '{path}: {query_matched}' search --mode autofield bids.author:haxbi~\n",
      "\n",
      "*Arguments*\n",
      "  QUERY                 query string, supported syntax and features depends on\n",
      "                        the selected search mode (see documentation).\n",
      "                        [Default: None]\n",
      "\n",
      "*Options*\n",
      "  -h, --help, --help-np\n",
      "                        show this help message. --help-np forcefully disables\n",
      "                        the use of a pager for displaying the help message\n",
      "  -d DATASET, --dataset DATASET\n",
      "                        specify the dataset to perform the query operation on.\n",
      "                        If no dataset is given, an attempt is made to identify\n",
      "                        the dataset based on the current working directory\n",
      "                        and/or the PATH given. Constraints: Value must be a\n",
      "                        Dataset or a valid identifier of a Dataset (e.g. a\n",
      "                        path) [Default: None]\n",
      "  --reindex             force rebuilding the search index, even if no change\n",
      "                        in the dataset's state has been detected, for example,\n",
      "                        when the index documenttype configuration has changed.\n",
      "                        [Default: False]\n",
      "  --max-nresults MAX_NRESULTS\n",
      "                        maxmimum number of search results to report. Setting\n",
      "                        this to 0 will report all search matches, and make\n",
      "                        searching substantially slower on large metadata sets.\n",
      "                        Constraints: value must be convertible to type 'int'\n",
      "                        [Default: 20]\n",
      "  --mode {egrep,textblob,autofield}\n",
      "                        Mode of search index structure and content. See\n",
      "                        section SEARCH MODES for details. . [Default: None]\n",
      "  --full-record, -f     If set, return the full metadata record for each\n",
      "                        search hit. Depending on the search mode this might\n",
      "                        require additional queries. By default, only data that\n",
      "                        is available to the respective search modes is\n",
      "                        returned. This always includes essential information,\n",
      "                        such as the path and the type. [Default: False]\n",
      "  --show-keys {name,short,full}\n",
      "                        if given, a list of known search keys is shown. If\n",
      "                        'name' - only the name is printed one per line. If\n",
      "                        'short' or 'full', statistics (in how many datasets,\n",
      "                        and how many unique values) are printed. 'short'\n",
      "                        truncates the listing of unique values. No other\n",
      "                        action is performed (except for reindexing), even if\n",
      "                        other arguments are given. Each key is accompanied by\n",
      "                        a term definition in parenthesis (TODO). In most cases\n",
      "                        a definition is given in the form of a URL. If an\n",
      "                        ontology definition for a term is known, this URL can\n",
      "                        resolve to a webpage that provides a comprehensive\n",
      "                        definition of the term. However, for speed reasons\n",
      "                        term resolution is solely done on information\n",
      "                        contained in a local dataset's metadata, and\n",
      "                        definition URLs might be outdated or point to no\n",
      "                        longer existing resources. [Default: None]\n",
      "  --show-query          if given, the formal query that was generated from the\n",
      "                        given query string is shown, but not actually\n",
      "                        executed. This is mostly useful for debugging\n",
      "                        purposes. [Default: False]\n",
      "\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!datalad search --help-np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's search for datasets containing information about Jim Haxby"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;1msearch\u001b[0m(\u001b[1;32mok\u001b[0m): /data/datasets.datalad.org/hbnssi (\u001b[1;35mdataset\u001b[0m)\n",
      "\u001b[1;1msearch\u001b[0m(\u001b[1;32mok\u001b[0m): /data/datasets.datalad.org/labs (\u001b[1;35mdataset\u001b[0m)\n",
      "\u001b[1;1msearch\u001b[0m(\u001b[1;32mok\u001b[0m): /data/datasets.datalad.org/labs/gobbini/famface/data (\u001b[1;35mdataset\u001b[0m)\n",
      "\u001b[1;1msearch\u001b[0m(\u001b[1;32mok\u001b[0m): /data/datasets.datalad.org/labs/haxby (\u001b[1;35mdataset\u001b[0m)\n",
      "\u001b[1;1msearch\u001b[0m(\u001b[1;32mok\u001b[0m): /data/datasets.datalad.org/labs/haxby/attention (\u001b[1;35mdataset\u001b[0m)\n",
      "\u001b[1;1msearch\u001b[0m(\u001b[1;32mok\u001b[0m): /data/datasets.datalad.org/labs/haxby/life (\u001b[1;35mdataset\u001b[0m)\n",
      "\u001b[1;1msearch\u001b[0m(\u001b[1;32mok\u001b[0m): /data/datasets.datalad.org/labs/haxby/raiders (\u001b[1;35mdataset\u001b[0m)\n",
      "\u001b[1;1msearch\u001b[0m(\u001b[1;32mok\u001b[0m): /data/datasets.datalad.org/openfmri/ds000105 (\u001b[1;35mdataset\u001b[0m)\n",
      "\u001b[1;1msearch\u001b[0m(\u001b[1;32mok\u001b[0m): /data/datasets.datalad.org/openfmri/ds000233 (\u001b[1;35mdataset\u001b[0m)\n",
      "\u001b[1;1msearch\u001b[0m(\u001b[1;32mok\u001b[0m): /data/datasets.datalad.org/openfmri/ds000241 (\u001b[1;35mdataset\u001b[0m)\n",
      "action summary:\n",
      "  search (ok: 10)\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!datalad search -d /data/datasets.datalad.org haxby"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "solution2": "hidden",
    "solution2_first": true
   },
   "source": [
    "## Exercise 4:\n",
    "\n",
    "Search for information about datasets related to chris gorgolewski, using `gorgolewski` as the keyword."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "solution2": "hidden"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;1msearch\u001b[0m(\u001b[1;32mok\u001b[0m): /data/datasets.datalad.org/openfmri/ds000114 (\u001b[1;35mdataset\u001b[0m)\n",
      "\u001b[1;1msearch\u001b[0m(\u001b[1;32mok\u001b[0m): /data/datasets.datalad.org/openfmri/ds000158 (\u001b[1;35mdataset\u001b[0m)\n",
      "\u001b[1;1msearch\u001b[0m(\u001b[1;32mok\u001b[0m): /data/datasets.datalad.org/openfmri/ds000221 (\u001b[1;35mdataset\u001b[0m)\n",
      "\u001b[1;1msearch\u001b[0m(\u001b[1;32mok\u001b[0m): /data/datasets.datalad.org/workshops/nipype-2017/ds000114 (\u001b[1;35mdataset\u001b[0m)\n",
      "action summary:\n",
      "  search (ok: 4)\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!datalad search -d /data/datasets.datalad.org gorgolewski"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now install dataset ds0000114 from the recent NIH workshop. This will help us run some of the other notebooks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "install(ok): /data/datasets.datalad.org/workshops/nih-2017/ds000114 (dataset) [Installed subdataset in order to get /data/datasets.datalad.org/workshops/nih-2017/ds000114]\n",
      "action summary:\n",
      "  install (ok: 3)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO] Cloning http://datasets.datalad.org/workshops/.git to '/data/datasets.datalad.org/workshops' \n",
      "[INFO] Cloning http://datasets.datalad.org/workshops/nih-2017/.git to '/data/datasets.datalad.org/workshops/nih-2017' \n",
      "[INFO] Cloning http://datasets.datalad.org/workshops/nih-2017/ds000114/.git to '/data/datasets.datalad.org/workshops/nih-2017/ds000114' \n",
      "[INFO] access to dataset sibling \"datalad\" not auto-enabled, enable with:\n",
      "| \t\tdatalad siblings -d \"/data/datasets.datalad.org/workshops/nih-2017/ds000114\" enable -s datalad \n"
     ]
    }
   ],
   "source": [
    "%%bash \n",
    "\n",
    "datalad install /data/datasets.datalad.org/workshops/nih-2017/ds000114"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "solution2": "hidden",
    "solution2_first": true
   },
   "source": [
    "## Exercise 5:\n",
    "\n",
    "Look at the contents of the dataset upto three levels using the `tree` command. Note that there may be files that are not present locally. These will be indicated in red."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "solution2": "hidden"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/data/datasets.datalad.org/workshops/nih-2017/ds000114\n",
      "├── CHANGES\n",
      "├── dataset_description.json\n",
      "├── derivatives\n",
      "│   ├── fmriprep\n",
      "│   └── freesurfer\n",
      "├── dwi.bval -> .git/annex/objects/JX/4K/MD5E-s335--5bd6fa32ccd0c79e79f9ac63a2c09c1a.bval/MD5E-s335--5bd6fa32ccd0c79e79f9ac63a2c09c1a.bval\n",
      "├── dwi.bvec -> .git/annex/objects/Pg/wk/MD5E-s1248--0641c68ff6ee6164928c984541653430.bvec/MD5E-s1248--0641c68ff6ee6164928c984541653430.bvec\n",
      "├── sub-01\n",
      "│   ├── ses-retest\n",
      "│   │   ├── anat\n",
      "│   │   ├── dwi\n",
      "│   │   └── func\n",
      "│   └── ses-test\n",
      "│       ├── anat\n",
      "│       ├── dwi\n",
      "│       └── func\n",
      "├── sub-02\n",
      "│   ├── ses-retest\n",
      "│   │   ├── anat\n",
      "│   │   ├── dwi\n",
      "│   │   └── func\n",
      "│   └── ses-test\n",
      "│       ├── anat\n",
      "│       ├── dwi\n",
      "│       └── func\n",
      "├── sub-03\n",
      "│   ├── ses-retest\n",
      "│   │   ├── anat\n",
      "│   │   ├── dwi\n",
      "│   │   └── func\n",
      "│   └── ses-test\n",
      "│       ├── anat\n",
      "│       ├── dwi\n",
      "│       └── func\n",
      "├── sub-04\n",
      "│   ├── ses-retest\n",
      "│   │   ├── anat\n",
      "│   │   ├── dwi\n",
      "│   │   └── func\n",
      "│   └── ses-test\n",
      "│       ├── anat\n",
      "│       ├── dwi\n",
      "│       └── func\n",
      "├── sub-05\n",
      "│   ├── ses-retest\n",
      "│   │   ├── anat\n",
      "│   │   ├── dwi\n",
      "│   │   └── func\n",
      "│   └── ses-test\n",
      "│       ├── anat\n",
      "│       ├── dwi\n",
      "│       └── func\n",
      "├── sub-06\n",
      "│   ├── ses-retest\n",
      "│   │   ├── anat\n",
      "│   │   ├── dwi\n",
      "│   │   └── func\n",
      "│   └── ses-test\n",
      "│       ├── anat\n",
      "│       ├── dwi\n",
      "│       └── func\n",
      "├── sub-07\n",
      "│   ├── ses-retest\n",
      "│   │   ├── anat\n",
      "│   │   ├── dwi\n",
      "│   │   └── func\n",
      "│   └── ses-test\n",
      "│       ├── anat\n",
      "│       ├── dwi\n",
      "│       └── func\n",
      "├── sub-08\n",
      "│   ├── ses-retest\n",
      "│   │   ├── anat\n",
      "│   │   ├── dwi\n",
      "│   │   └── func\n",
      "│   └── ses-test\n",
      "│       ├── anat\n",
      "│       ├── dwi\n",
      "│       └── func\n",
      "├── sub-09\n",
      "│   ├── ses-retest\n",
      "│   │   ├── anat\n",
      "│   │   ├── dwi\n",
      "│   │   └── func\n",
      "│   └── ses-test\n",
      "│       ├── anat\n",
      "│       ├── dwi\n",
      "│       └── func\n",
      "├── sub-10\n",
      "│   ├── ses-retest\n",
      "│   │   ├── anat\n",
      "│   │   ├── dwi\n",
      "│   │   └── func\n",
      "│   └── ses-test\n",
      "│       ├── anat\n",
      "│       ├── dwi\n",
      "│       └── func\n",
      "├── task-covertverbgeneration_bold.json\n",
      "├── task-covertverbgeneration_events.tsv\n",
      "├── task-fingerfootlips_bold.json\n",
      "├── task-fingerfootlips_events.tsv\n",
      "├── task-linebisection_bold.json\n",
      "├── task-overtverbgeneration_bold.json\n",
      "├── task-overtverbgeneration_events.tsv\n",
      "├── task-overtwordrepetition_bold.json\n",
      "└── task-overtwordrepetition_events.tsv\n",
      "\n",
      "93 directories, 13 files\n"
     ]
    }
   ],
   "source": [
    "!tree -L 3 /data/datasets.datalad.org/workshops/nih-2017/ds000114"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When we try to list the contents of these files we get `No such file or directory`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cat: /data/datasets.datalad.org/workshops/nih-2017/ds000114/dwi.bval: No such file or directory\n"
     ]
    }
   ],
   "source": [
    "!cat /data/datasets.datalad.org/workshops/nih-2017/ds000114/dwi.bval"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's use datalad to fetch this file and then list it again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;1mget\u001b[0m(\u001b[1;32mok\u001b[0m): /data/datasets.datalad.org/workshops/nih-2017/ds000114/dwi.bval (\u001b[1;35mfile\u001b[0m) [from origin...]\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!datalad get /data/datasets.datalad.org/workshops/nih-2017/ds000114/dwi.bval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0 0 0 0 0 0 1000 1000 1000 1000 1000 1000 1000 1000 1000 1000 1000 1000 1000 1000 1000 1000 1000 1000 1000 1000 1000 1000 1000 1000 1000 1000 1000 1000 1000 1000 1000 1000 1000 1000 1000 1000 1000 1000 1000 1000 1000 1000 1000 1000 1000 1000 1000 1000 1000 1000 1000 1000 1000 1000 1000 1000 1000 1000 1000 1000 1000 1000 1000 1000 \n"
     ]
    }
   ],
   "source": [
    "!cat /data/datasets.datalad.org/workshops/nih-2017/ds000114/dwi.bval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;4m/data/datasets.datalad.org/workshops/nih-2017/ds000114\u001b[0m   [annex]  master  2.0.1-13-g26d1fc2e0 2018-06-08/11:33:43  \u001b[1;32m✓\u001b[0m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!datalad ls /data/datasets.datalad.org/workshops/nih-2017/ds000114"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since datalad uses git-annex under the hood, let's try to list things with git-annex."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "git-annex: First run: git-annex init\n"
     ]
    }
   ],
   "source": [
    "!git-annex list /data/datasets.datalad.org/workshops/nih-2017/ds000114/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Oops! Git and datalad don't recognize root folders without pointing to an annex or dataset location."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "here\n",
      "|origin\n",
      "||web\n",
      "|||bittorrent\n",
      "||||datalad-archives\n",
      "|||||\n",
      "_XX_X sub-01/ses-retest/anat/sub-01_ses-retest_T1w.nii.gz\n",
      "_XX_X sub-01/ses-retest/dwi/sub-01_ses-retest_dwi.nii.gz\n",
      "_XX_X sub-01/ses-retest/func/sub-01_ses-retest_task-covertverbgeneration_bold.nii.gz\n",
      "_XX_X sub-01/ses-retest/func/sub-01_ses-retest_task-fingerfootlips_bold.nii.gz\n",
      "_XX_X sub-01/ses-retest/func/sub-01_ses-retest_task-linebisection_bold.nii.gz\n",
      "_XX_X sub-01/ses-retest/func/sub-01_ses-retest_task-linebisection_events.tsv\n",
      "_XX_X sub-01/ses-retest/func/sub-01_ses-retest_task-overtverbgeneration_bold.nii.gz\n",
      "_XX_X sub-01/ses-retest/func/sub-01_ses-retest_task-overtwordrepetition_bold.nii.gz\n",
      "_XX_X sub-01/ses-test/anat/sub-01_ses-test_T1w.nii.gz\n",
      "_XX_X sub-01/ses-test/dwi/sub-01_ses-test_dwi.nii.gz\n",
      "_XX_X sub-01/ses-test/func/sub-01_ses-test_task-covertverbgeneration_bold.nii.gz\n",
      "_XX_X sub-01/ses-test/func/sub-01_ses-test_task-fingerfootlips_bold.nii.gz\n",
      "_XX_X sub-01/ses-test/func/sub-01_ses-test_task-linebisection_bold.nii.gz\n",
      "_XX_X sub-01/ses-test/func/sub-01_ses-test_task-linebisection_events.tsv\n",
      "_XX_X sub-01/ses-test/func/sub-01_ses-test_task-overtverbgeneration_bold.nii.gz\n",
      "_XX_X sub-01/ses-test/func/sub-01_ses-test_task-overtwordrepetition_bold.nii.gz\n"
     ]
    }
   ],
   "source": [
    "%%bash \n",
    "cd /data/datasets.datalad.org/workshops/nih-2017/ds000114/\n",
    "git-annex list sub-01"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "solution2": "hidden",
    "solution2_first": true
   },
   "source": [
    "## Exercise 6:\n",
    "\n",
    "Show the help for `git annex list` and then use it to list the `dwi*` files in `ds000114`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true,
    "solution2": "hidden"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "git-annex list - show which remotes contain files\n",
      "\n",
      "Usage: git-annex list [PATH ...] [--allrepos]\n",
      "\n",
      "Available options:\n",
      "  --allrepos               show all repositories, not only remotes\n",
      "  --force                  allow actions that may lose annexed data\n",
      "  -F,--fast                avoid slow operations\n",
      "  -q,--quiet               avoid verbose output\n",
      "  -v,--verbose             allow verbose output (default)\n",
      "  -d,--debug               show debug messages\n",
      "  --no-debug               don't show debug messages\n",
      "  -b,--backend NAME        specify key-value backend to use\n",
      "  -N,--numcopies NUMBER    override default number of copies\n",
      "  --trust REMOTE           override trust setting\n",
      "  --semitrust REMOTE       override trust setting back to default\n",
      "  --untrust REMOTE         override trust setting to untrusted\n",
      "  -c,--config NAME=VALUE   override git configuration setting\n",
      "  --user-agent NAME        override default User-Agent\n",
      "  --trust-glacier          Trust Amazon Glacier inventory\n",
      "  --notify-finish          show desktop notification after transfer finishes\n",
      "  --notify-start           show desktop notification after transfer starts\n",
      "  -i,--in REMOTE           match files present in a remote\n",
      "  -C,--copies REMOTE       skip files with fewer copies\n",
      "  --lackingcopies NUMBER   match files that need more copies\n",
      "  --approxlackingcopies NUMBER\n",
      "                           match files that need more copies (faster)\n",
      "  -B,--inbackend NAME      match files using a key-value backend\n",
      "  --securehash             match files using a cryptographically secure hash\n",
      "  --inallgroup GROUP       match files present in all remotes in a group\n",
      "  --metadata FIELD=VALUE   match files with attached metadata\n",
      "  --want-get               match files the repository wants to get\n",
      "  --want-drop              match files the repository wants to drop\n",
      "  -x,--exclude GLOB        skip files matching the glob pattern\n",
      "  -I,--include GLOB        limit to files matching the glob pattern\n",
      "  --largerthan SIZE        match files larger than a size\n",
      "  --smallerthan SIZE       match files smaller than a size\n",
      "  --not                    negate next option\n",
      "  --and                    both previous and next option must match\n",
      "  --or                     either previous or next option must match\n",
      "  -(                       open group of options\n",
      "  -)                       close group of options\n",
      "  -T,--time-limit TIME     stop after the specified amount of time\n",
      "  -h,--help                Show this help text\n",
      "\n",
      "For details, run: git-annex help list\n"
     ]
    }
   ],
   "source": [
    "!git-annex list --help"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "solution2": "hidden"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "here\n",
      "|origin\n",
      "||web\n",
      "|||bittorrent\n",
      "||||datalad-archives\n",
      "|||||\n",
      "XXX_X dwi.bval\n",
      "_XX_X dwi.bvec\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "\n",
    "cd /data/datasets.datalad.org/workshops/nih-2017/ds000114/\n",
    "git-annex list dwi.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# help coomand here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list dwi* files here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also remove content from our local storage using the `drop` command."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;1mdrop\u001b[0m(\u001b[1;32mok\u001b[0m): /data/datasets.datalad.org/workshops/nih-2017/ds000114/dwi.bval (\u001b[1;35mfile\u001b[0m) [checking origin...]\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!datalad drop /data/datasets.datalad.org/workshops/nih-2017/ds000114/dwi.bval"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 7:\n",
    "\n",
    "Check where the dwi files are with the annex list command and get the missing files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "here\n",
      "|origin\n",
      "||web\n",
      "|||bittorrent\n",
      "||||datalad-archives\n",
      "|||||\n",
      "_XX_X dwi.bval\n",
      "_XX_X dwi.bvec\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "\n",
    "cd /data/datasets.datalad.org/workshops/nih-2017/ds000114/\n",
    "git-annex list dwi.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total (1 ok out of 2):  79%|███████▉  | 1.25k/1.58k [00:00<00:00, 4.18kB/s]\n",
      "Total (2 ok out of 2): 100%|██████████| 1.58k/1.58k [00:00<00:00, 3.72kB/s]\n",
      "get(ok): /data/datasets.datalad.org/workshops/nih-2017/ds000114/dwi.bvec (file) [from origin...]\n",
      "get(ok): /data/datasets.datalad.org/workshops/nih-2017/ds000114/dwi.bval (file) [from web...]\n",
      "action summary:\n",
      "  get (ok: 2)\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "\n",
    "cd /data/datasets.datalad.org/workshops/nih-2017/ds000114/\n",
    "datalad get dwi.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now we will create and version our own toy dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\u001b[1;37mINFO   \u001b[0m] Creating a new annex repo at /data/mydataset \n",
      "\u001b[1;1mcreate\u001b[0m(\u001b[1;32mok\u001b[0m): /data/mydataset (\u001b[1;35mdataset\u001b[0m)\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!datalad create /data/mydataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will create a dummy file and add it to the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "add(ok): /data/mydataset/123 (file)\n",
      "save(ok): /data/mydataset (dataset)\n",
      "action summary:\n",
      "  add (ok: 1)\n",
      "  save (ok: 1)\n"
     ]
    }
   ],
   "source": [
    "%%bash \n",
    "\n",
    "echo \"123\" > /data/mydataset/123\n",
    "datalad add -m \"initial file\" /data/mydataset/123"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "list where the copy is available"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "here\n",
      "|web\n",
      "||bittorrent\n",
      "|||\n",
      "X__ 123\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "\n",
    "cd /data/mydataset\n",
    "git-annex list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/data/mydataset\n",
      "└── 123 -> .git/annex/objects/pF/Zf/MD5E-s4--ba1f2511fc30423bdbb183fe33f3dd0f/MD5E-s4--ba1f2511fc30423bdbb183fe33f3dd0f\n",
      "\n",
      "0 directories, 1 file\n"
     ]
    }
   ],
   "source": [
    "!tree /data/mydataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "123\n"
     ]
    }
   ],
   "source": [
    "!cat /data/mydataset/.git/annex/objects/pF/Zf/MD5E-s4--ba1f2511fc30423bdbb183fe33f3dd0f/MD5E-s4--ba1f2511fc30423bdbb183fe33f3dd0f"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us try removing the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\u001b[1;33mWARNING\u001b[0m] Running drop resulted in stderr output: git-annex: drop: 1 failed\n",
      " \n",
      "[\u001b[1;31mERROR  \u001b[0m] unsafe; Could only verify the existence of 0 out of 1 necessary copies; Rather than dropping this file, try using: git annex move; (Use --force to override this check, or adjust numcopies.) [drop(/data/mydataset/123)] \n",
      "\u001b[1;1mdrop\u001b[0m(\u001b[1;31merror\u001b[0m): /data/mydataset/123 (\u001b[1;35mfile\u001b[0m) [unsafe; Could only verify the existence of 0 out of 1 necessary copies; Rather than dropping this file, try using: git annex move; (Use --force to override this check, or adjust numcopies.)]\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!datalad drop /data/mydataset/123"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us try modifying the file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/bin/sh: 1: cannot create /data/mydataset/123: Permission denied\n"
     ]
    }
   ],
   "source": [
    "!echo \"321\" > /data/mydataset/123"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The proper way to modify this is to unlock the file, change it and then commit it back"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "unlock(ok): /data/mydataset/123 (file)\n",
      "add(ok): /data/mydataset/123 (file)\n",
      "save(ok): /data/mydataset (dataset)\n",
      "action summary:\n",
      "  add (ok: 1)\n",
      "  save (ok: 1)\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "\n",
    "datalad unlock /data/mydataset/123\n",
    "echo \"321\" > /data/mydataset/123\n",
    "datalad add -m \"add modified file\" /data/mydataset/123"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we try modifying it, we now again get permission denied, because the file is locked."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/bin/sh: 1: cannot create /data/mydataset/123: Permission denied\n"
     ]
    }
   ],
   "source": [
    "!echo \"123\" > /data/mydataset/123"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/data/mydataset\n",
      "└── 123 -> .git/annex/objects/6v/gZ/MD5E-s4--9492fe88f263d58e0b686885e8c98c0e/MD5E-s4--9492fe88f263d58e0b686885e8c98c0e\n",
      "\n",
      "0 directories, 1 file\n"
     ]
    }
   ],
   "source": [
    "!tree /data/mydataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "321\n"
     ]
    }
   ],
   "source": [
    "!cat /data/mydataset/.git/annex/objects/6v/gZ/MD5E-s4--9492fe88f263d58e0b686885e8c98c0e/MD5E-s4--9492fe88f263d58e0b686885e8c98c0e"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But the old object is still there. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "123\n"
     ]
    }
   ],
   "source": [
    "!cat /data/mydataset/.git/annex/objects/pF/Zf/MD5E-s4--ba1f2511fc30423bdbb183fe33f3dd0f/MD5E-s4--ba1f2511fc30423bdbb183fe33f3dd0f"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The entire history of the repo is available."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "commit 7161896bf141743a64d64c6cc217d36f04382623\n",
      "Author: yarikoptic <neuro>\n",
      "Date:   Thu Aug 2 02:46:26 2018 +0000\n",
      "\n",
      "    add modified file\n",
      "\n",
      "commit 8b5428983abe4f46c41952d1f72ab29a721c0033\n",
      "Author: yarikoptic <neuro>\n",
      "Date:   Thu Aug 2 02:46:22 2018 +0000\n",
      "\n",
      "    initial file\n",
      "\n",
      "commit ef2e870514e6f59779c7ca1c0cc0c705b0cc0bc4\n",
      "Author: yarikoptic <neuro>\n",
      "Date:   Thu Aug 2 02:46:21 2018 +0000\n",
      "\n",
      "    [DATALAD] new dataset\n",
      "\n",
      "commit ee113840402702a31150f908272dd4f1acad027b\n",
      "Author: yarikoptic <neuro>\n",
      "Date:   Thu Aug 2 02:46:20 2018 +0000\n",
      "\n",
      "    [DATALAD] Set default backend for all files to be MD5E\n"
     ]
    }
   ],
   "source": [
    "%%bash \n",
    "\n",
    "cd /data/mydataset/\n",
    "git log"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us create a simple script that counts the number of characters in a file and commit it directly into git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#!/bin/bash\n",
      "cat $1 | wc -c\n",
      "add(ok): /data/mydataset/scripts/run.sh (file) [non-large file; adding content to git repository]\n",
      "add(ok): /data/mydataset/scripts (directory)\n",
      "save(ok): /data/mydataset (dataset)\n",
      "action summary:\n",
      "  add (ok: 2)\n",
      "  save (ok: 1)\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "\n",
    "cd /data/mydataset\n",
    "mkdir -p scripts\n",
    "cmd=$(cat << EOM\n",
    "#!/bin/bash\\ncat \\$1 | wc -c\n",
    "EOM\n",
    ")\n",
    "echo -e $cmd > scripts/run.sh\n",
    "chmod +x scripts/run.sh\n",
    "cat scripts/run.sh\n",
    "datalad add -m \"Added the mighty script\" --to-git scripts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We could have just ran a script to generate output file, `datalad add out` but that would leave no record of **how** that file was generated.\n",
    "[datalad run](http://docs.datalad.org/en/latest/generated/man/datalad-run.html) command assists with running a command a saving all produced results.\n",
    "So now we will `datalad run` the script and have output added to the annex:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "add(ok): out (file)\n",
      "save(ok): /data/mydataset (dataset)\n",
      "action summary:\n",
      "  add (ok: 1)\n",
      "  save (ok: 1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO] == Command start (output follows) ===== \n",
      "[INFO] == Command exit (modification check follows) ===== \n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "cd /data/mydataset\n",
    "datalad run -m \"Running the mighty script using datalad run\" bash -c 'scripts/run.sh 123 > out'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can look at the log again (with `--stat` to see statistics on changed files) to see a special message body for the latest commit containing special session with information about executed command, and that produced out file was added to the repository."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "commit 742ca8cf3224413665456a6d9eb39c49e85e5b02\n",
      "Author: yarikoptic <neuro>\n",
      "Date:   Thu Aug 2 02:46:30 2018 +0000\n",
      "\n",
      "    [DATALAD RUNCMD] Running the mighty script using datalad run\n",
      "    \n",
      "    === Do not change lines below ===\n",
      "    {\n",
      "     \"chain\": [],\n",
      "     \"cmd\": \"bash -c 'scripts/run.sh 123 > out'\",\n",
      "     \"dsid\": \"3c991c3a-95fe-11e8-8616-eb661f417fb8\",\n",
      "     \"exit\": 0,\n",
      "     \"inputs\": [],\n",
      "     \"outputs\": [],\n",
      "     \"pwd\": \".\"\n",
      "    }\n",
      "    ^^^ Do not change lines above ^^^\n",
      "\n",
      " out | 1 +\n",
      " 1 file changed, 1 insertion(+)\n",
      "\n",
      "commit fd195ced83fd2adf380560d72c13eb382b8f2001\n",
      "Author: yarikoptic <neuro>\n",
      "Date:   Thu Aug 2 02:46:29 2018 +0000\n",
      "\n",
      "    Added the mighty script\n",
      "\n",
      " scripts/run.sh | 2 ++\n",
      " 1 file changed, 2 insertions(+)\n",
      "\n",
      "commit 7161896bf141743a64d64c6cc217d36f04382623\n",
      "Author: yarikoptic <neuro>\n",
      "Date:   Thu Aug 2 02:46:26 2018 +0000\n",
      "\n",
      "    add modified file\n",
      "\n",
      " 123 | 2 +-\n",
      " 1 file changed, 1 insertion(+), 1 deletion(-)\n",
      "\n",
      "commit 8b5428983abe4f46c41952d1f72ab29a721c0033\n",
      "Author: yarikoptic <neuro>\n",
      "Date:   Thu Aug 2 02:46:22 2018 +0000\n",
      "\n",
      "    initial file\n",
      "\n",
      " 123 | 1 +\n",
      " 1 file changed, 1 insertion(+)\n",
      "\n",
      "commit ef2e870514e6f59779c7ca1c0cc0c705b0cc0bc4\n",
      "Author: yarikoptic <neuro>\n",
      "Date:   Thu Aug 2 02:46:21 2018 +0000\n",
      "\n",
      "    [DATALAD] new dataset\n",
      "\n",
      " .datalad/.gitattributes | 3 +++\n",
      " .datalad/config         | 2 ++\n",
      " .gitattributes          | 1 +\n",
      " 3 files changed, 6 insertions(+)\n",
      "\n",
      "commit ee113840402702a31150f908272dd4f1acad027b\n",
      "Author: yarikoptic <neuro>\n",
      "Date:   Thu Aug 2 02:46:20 2018 +0000\n",
      "\n",
      "    [DATALAD] Set default backend for all files to be MD5E\n",
      "\n",
      " .gitattributes | 1 +\n",
      " 1 file changed, 1 insertion(+)\n"
     ]
    }
   ],
   "source": [
    "%%bash \n",
    "\n",
    "cd /data/mydataset/\n",
    "git log --stat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can go back to a previous state and check the contents of the `123` file. Note that we return back to current state after this excursion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/data/mydataset/\n",
      "├── 123 -> .git/annex/objects/6v/gZ/MD5E-s4--9492fe88f263d58e0b686885e8c98c0e/MD5E-s4--9492fe88f263d58e0b686885e8c98c0e\n",
      "├── out -> .git/annex/objects/6w/1x/MD5E-s2--48a24b70a0b376535542b996af517398/MD5E-s2--48a24b70a0b376535542b996af517398\n",
      "└── scripts\n",
      "    └── run.sh\n",
      "\n",
      "1 directory, 3 files\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "\n",
    "tree /data/mydataset/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/data/mydataset/\n",
      "└── 123 -> .git/annex/objects/6v/gZ/MD5E-s4--9492fe88f263d58e0b686885e8c98c0e/MD5E-s4--9492fe88f263d58e0b686885e8c98c0e\n",
      "\n",
      "0 directories, 1 file\n",
      "/data/mydataset/\n",
      "├── 123 -> .git/annex/objects/6v/gZ/MD5E-s4--9492fe88f263d58e0b686885e8c98c0e/MD5E-s4--9492fe88f263d58e0b686885e8c98c0e\n",
      "├── out -> .git/annex/objects/6w/1x/MD5E-s2--48a24b70a0b376535542b996af517398/MD5E-s2--48a24b70a0b376535542b996af517398\n",
      "└── scripts\n",
      "    └── run.sh\n",
      "\n",
      "1 directory, 3 files\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Note: checking out 'HEAD^^'.\n",
      "\n",
      "You are in 'detached HEAD' state. You can look around, make experimental\n",
      "changes and commit them, and you can discard any commits you make in this\n",
      "state without impacting any branches by performing another checkout.\n",
      "\n",
      "If you want to create a new branch to retain commits you create, you may\n",
      "do so (now or later) by using -b with the checkout command again. Example:\n",
      "\n",
      "  git checkout -b <new-branch-name>\n",
      "\n",
      "HEAD is now at 7161896 add modified file\n",
      "Previous HEAD position was 7161896 add modified file\n",
      "Switched to branch 'master'\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "\n",
    "cd /data/mydataset/\n",
    "git checkout HEAD^^\n",
    "tree /data/mydataset/\n",
    "git checkout master\n",
    "tree /data/mydataset/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Those records in the commit messages done by `datalad run` could later be used to rerun the entire history of changes.\n",
    "It might be desired if the environment or input data changes. [datalad rerun](http://docs.datalad.org/en/latest/generated/man/datalad-rerun.html) provides a number of options to fulfil a variety of such cases. For our excercise we will rerun the entire history (well - just one commit for now) recorded using `datalad run`, while reproducing all the results in a separate branch we will call `verify`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "add(ok): out (file)\n",
      "save(ok): /data/mydataset (dataset)\n",
      "action summary:\n",
      "  add (ok: 1)\n",
      "  save (ok: 1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Already on 'master'\n",
      "[INFO] == Command start (output follows) ===== \n",
      "[INFO] == Command exit (modification check follows) ===== \n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "cd /data/mydataset\n",
    "git checkout master\n",
    "datalad rerun --since= --onto= --branch=verify"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can look at the git log as a graph to visualize the branch we have created:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* 30edba4 (HEAD -> verify) [DATALAD RUNCMD] Running the mighty script using datalad run\n",
      "| out\n",
      "| * 742ca8c (master) [DATALAD RUNCMD] Running the mighty script using datalad run\n",
      "|/  \n",
      "|   out\n",
      "* fd195ce Added the mighty script\n",
      "| scripts/run.sh\n",
      "* 7161896 add modified file\n",
      "| 123\n",
      "* 8b54289 initial file\n",
      "| 123\n",
      "* ef2e870 [DATALAD] new dataset\n",
      "| .datalad/.gitattributes\n",
      "| .datalad/config\n",
      "| .gitattributes\n",
      "* ee11384 [DATALAD] Set default backend for all files to be MD5E\n",
      "  .gitattributes\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "cd /data/mydataset\n",
    "git log --oneline --graph --name-only --decorate master verify"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and can `git diff` verify and master to see that they are identical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Switched to branch 'master'\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "cd /data/mydataset\n",
    "\n",
    "git checkout master\n",
    "git diff verify master"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " whoohoo, we have reproduced our results!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "solution2": "hidden",
    "solution2_first": true
   },
   "source": [
    "## Exercise 8:\n",
    "\n",
    "- Copy one binary brainmask image file from `ds000114/derivatives/fmriprep` into `mydataset\n",
    "  - To do so first you should install the dataset recursively\n",
    "  - And then get the file\n",
    "- Add to version control\n",
    "- use git-annex to list where that file can be found\n",
    "- Add (to git) a simple python script to count and print the number of non-zero voxels\n",
    "- Run the script using `datalad run` (learn about `--input` and `--output` options) to store the output into a new out file and make a record of running the script\n",
    "- Rerun the entire history of changes using `datalad rerun`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "solution2": "hidden"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "install(ok): /data/datasets.datalad.org/workshops/nih-2017/ds000114/derivatives/fmriprep (dataset) [Installed subdataset in order to get /data/datasets.datalad.org/workshops/nih-2017/ds000114/derivatives/fmriprep]\n",
      "/data/datasets.datalad.org/workshops/nih-2017/ds000114/derivatives/fmriprep/\n",
      "├── sub-01\n",
      "├── sub-01.html -> .git/annex/objects/MF/jw/MD5E-s20077561--03ecea8730492d537e050941bdf654bf.html/MD5E-s20077561--03ecea8730492d537e050941bdf654bf.html\n",
      "├── sub-02\n",
      "├── sub-02.html -> .git/annex/objects/99/j3/MD5E-s19975906--5ede67fcdad59b65a02f572360db2863.html/MD5E-s19975906--5ede67fcdad59b65a02f572360db2863.html\n",
      "├── sub-03\n",
      "├── sub-03.html -> .git/annex/objects/z4/8w/MD5E-s20227534--64e1a981338e8fb9c87f026a79a34785.html/MD5E-s20227534--64e1a981338e8fb9c87f026a79a34785.html\n",
      "├── sub-04\n",
      "├── sub-04.html -> .git/annex/objects/qF/J1/MD5E-s22389786--2954e6ece2a825c0008e9b1dcfcaf0a6.html/MD5E-s22389786--2954e6ece2a825c0008e9b1dcfcaf0a6.html\n",
      "├── sub-05\n",
      "├── sub-05.html -> .git/annex/objects/6G/Z6/MD5E-s22109848--70a1908c811102744f39b87ae03216a2.html/MD5E-s22109848--70a1908c811102744f39b87ae03216a2.html\n",
      "├── sub-06\n",
      "├── sub-06.html -> .git/annex/objects/k9/gx/MD5E-s21892649--c22445c2264626ea8537b440a280d240.html/MD5E-s21892649--c22445c2264626ea8537b440a280d240.html\n",
      "├── sub-07\n",
      "├── sub-07.html -> .git/annex/objects/4v/vV/MD5E-s19939423--f9c96cb528fb62ebde2d33bb6a69cb8b.html/MD5E-s19939423--f9c96cb528fb62ebde2d33bb6a69cb8b.html\n",
      "├── sub-08\n",
      "├── sub-08.html -> .git/annex/objects/Kj/P0/MD5E-s21484045--93abf611fe734778dedbfb65ce983e42.html/MD5E-s21484045--93abf611fe734778dedbfb65ce983e42.html\n",
      "├── sub-09\n",
      "├── sub-09.html -> .git/annex/objects/9M/J0/MD5E-s21261086--9eaa82886171bc130c560ad95e9399ce.html/MD5E-s21261086--9eaa82886171bc130c560ad95e9399ce.html\n",
      "├── sub-10\n",
      "└── sub-10.html -> .git/annex/objects/54/fp/MD5E-s19211083--9cd49ee07578dfdf58246243af5faf16.html/MD5E-s19211083--9cd49ee07578dfdf58246243af5faf16.html\n",
      "\n",
      "10 directories, 10 files\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO] Cloning http://datasets.datalad.org/workshops/nih-2017/ds000114/derivatives/fmriprep/.git to '/data/datasets.datalad.org/workshops/nih-2017/ds000114/derivatives/fmriprep' \n",
      "[INFO] Installing <Dataset path=/data/datasets.datalad.org/workshops/nih-2017/ds000114/derivatives/fmriprep> recursively \n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "\n",
    "datalad install -r /data/datasets.datalad.org/workshops/nih-2017/ds000114/derivatives/fmriprep/\n",
    "tree -L 1 /data/datasets.datalad.org/workshops/nih-2017/ds000114/derivatives/fmriprep/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "solution2": "hidden"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/data/datasets.datalad.org/workshops/nih-2017/ds000114/derivatives/fmriprep/sub-01/anat/\n",
      "├── sub-01_t1w_brainmask.nii.gz -> ../../.git/annex/objects/jJ/Wz/MD5E-s93130--2572248880aa5978f8d4049feff2282a.nii.gz/MD5E-s93130--2572248880aa5978f8d4049feff2282a.nii.gz\n",
      "├── sub-01_t1w_class-csf_probtissue.nii.gz -> ../../.git/annex/objects/jg/4g/MD5E-s3066660--67fbe37bd5440773a9951eb116d9192c.nii.gz/MD5E-s3066660--67fbe37bd5440773a9951eb116d9192c.nii.gz\n",
      "├── sub-01_t1w_class-gm_probtissue.nii.gz -> ../../.git/annex/objects/80/65/MD5E-s3395247--d4e1b01832f3514a796788ff9d814134.nii.gz/MD5E-s3395247--d4e1b01832f3514a796788ff9d814134.nii.gz\n",
      "├── sub-01_t1w_class-wm_probtissue.nii.gz -> ../../.git/annex/objects/W9/v4/MD5E-s3107921--50711fd1ad6729e9d8f8b0c0611578f8.nii.gz/MD5E-s3107921--50711fd1ad6729e9d8f8b0c0611578f8.nii.gz\n",
      "├── sub-01_t1w_dtissue.nii.gz -> ../../.git/annex/objects/gZ/qJ/MD5E-s241087--cedef0e31b37f728f09c92c4bce16f61.nii.gz/MD5E-s241087--cedef0e31b37f728f09c92c4bce16f61.nii.gz\n",
      "├── sub-01_t1w_inflated.l.surf.gii -> ../../.git/annex/objects/Gk/qz/MD5E-s2552048--2f46d819ea04a6b58651d0371f0f4884.surf.gii/MD5E-s2552048--2f46d819ea04a6b58651d0371f0f4884.surf.gii\n",
      "├── sub-01_t1w_inflated.r.surf.gii -> ../../.git/annex/objects/kv/38/MD5E-s2577053--0544f6494c3900b5b0c2ebe3e64ff75f.surf.gii/MD5E-s2577053--0544f6494c3900b5b0c2ebe3e64ff75f.surf.gii\n",
      "├── sub-01_t1w_midthickness.l.surf.gii -> ../../.git/annex/objects/Xg/jf/MD5E-s2541962--12837037044e1d065c271ab7a46f2ec0.surf.gii/MD5E-s2541962--12837037044e1d065c271ab7a46f2ec0.surf.gii\n",
      "├── sub-01_t1w_midthickness.r.surf.gii -> ../../.git/annex/objects/F5/Zw/MD5E-s2566814--2c8a72c3dbd38626185a886c10dba6e8.surf.gii/MD5E-s2566814--2c8a72c3dbd38626185a886c10dba6e8.surf.gii\n",
      "├── sub-01_t1w_pial.l.surf.gii -> ../../.git/annex/objects/kM/25/MD5E-s2546954--50afda07aaf3e09c971b0b2de3ecefa5.surf.gii/MD5E-s2546954--50afda07aaf3e09c971b0b2de3ecefa5.surf.gii\n",
      "├── sub-01_t1w_pial.r.surf.gii -> ../../.git/annex/objects/19/14/MD5E-s2570790--6ee98dfaccd234031e68ae977dd67905.surf.gii/MD5E-s2570790--6ee98dfaccd234031e68ae977dd67905.surf.gii\n",
      "├── sub-01_t1w_preproc.nii.gz -> ../../.git/annex/objects/FF/Xw/MD5E-s34678574--041927b06af0f79b15f19c0cb9d4c42e.nii.gz/MD5E-s34678574--041927b06af0f79b15f19c0cb9d4c42e.nii.gz\n",
      "├── sub-01_t1w_smoothwm.l.surf.gii -> ../../.git/annex/objects/14/kq/MD5E-s2536643--0276ca06137e6022633a9c94743e2baa.surf.gii/MD5E-s2536643--0276ca06137e6022633a9c94743e2baa.surf.gii\n",
      "├── sub-01_t1w_smoothwm.r.surf.gii -> ../../.git/annex/objects/z8/46/MD5E-s2561463--ae614e10c989d893cade45851a1e66d9.surf.gii/MD5E-s2561463--ae614e10c989d893cade45851a1e66d9.surf.gii\n",
      "├── sub-01_t1w_space-mni152nlin2009casym_brainmask.nii.gz -> ../../.git/annex/objects/g7/M1/MD5E-s134764--c66e4c5075182457ba1560c8c839669b.nii.gz/MD5E-s134764--c66e4c5075182457ba1560c8c839669b.nii.gz\n",
      "├── sub-01_t1w_space-mni152nlin2009casym_class-csf_probtissue.nii.gz -> ../../.git/annex/objects/9m/Z1/MD5E-s7230742--0b2f3950c21a9ae691e550c8427b64d6.nii.gz/MD5E-s7230742--0b2f3950c21a9ae691e550c8427b64d6.nii.gz\n",
      "├── sub-01_t1w_space-mni152nlin2009casym_class-gm_probtissue.nii.gz -> ../../.git/annex/objects/WQ/qw/MD5E-s7370648--c4cdb6c3f94b83efaae1b8eb6d47f5a9.nii.gz/MD5E-s7370648--c4cdb6c3f94b83efaae1b8eb6d47f5a9.nii.gz\n",
      "├── sub-01_t1w_space-mni152nlin2009casym_class-wm_probtissue.nii.gz -> ../../.git/annex/objects/0J/kj/MD5E-s7243055--a722f9f7ef533ceeb480b343c5a59a56.nii.gz/MD5E-s7243055--a722f9f7ef533ceeb480b343c5a59a56.nii.gz\n",
      "├── sub-01_t1w_space-mni152nlin2009casym_dtissue.nii.gz -> ../../.git/annex/objects/k4/vJ/MD5E-s434565--9390accd81f24bbd9c32b03e66369955.nii.gz/MD5E-s434565--9390accd81f24bbd9c32b03e66369955.nii.gz\n",
      "├── sub-01_t1w_space-mni152nlin2009casym_preproc.nii.gz -> ../../.git/annex/objects/6M/Gp/MD5E-s8490746--5bb6a33c5d41bc27eefc3ec83a8a6686.nii.gz/MD5E-s8490746--5bb6a33c5d41bc27eefc3ec83a8a6686.nii.gz\n",
      "└── sub-01_t1w_space-mni152nlin2009casym_warp.h5 -> ../../.git/annex/objects/Zm/WZ/MD5E-s102374852--88f500a4cf26d98e913df8c25d41a87d.h5/MD5E-s102374852--88f500a4cf26d98e913df8c25d41a87d.h5\n",
      "\n",
      "0 directories, 21 files\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "\n",
    "tree -L 1 /data/datasets.datalad.org/workshops/nih-2017/ds000114/derivatives/fmriprep/sub-01/anat/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "solution2": "hidden"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "get(ok): /data/datasets.datalad.org/workshops/nih-2017/ds000114/derivatives/fmriprep/sub-01/anat/sub-01_t1w_brainmask.nii.gz (file) [from origin...]\n",
      "add(ok): brainmask.nii.gz (file)\n",
      "save(ok): /data/mydataset (dataset)\n",
      "action summary:\n",
      "  add (ok: 1)\n",
      "  save (ok: 1)\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "\n",
    "datalad get /data/datasets.datalad.org/workshops/nih-2017/ds000114/derivatives/fmriprep/sub-01/anat/sub-01_t1w_brainmask.nii.gz\n",
    "cp /data/datasets.datalad.org/workshops/nih-2017/ds000114/derivatives/fmriprep/sub-01/anat/sub-01_t1w_brainmask.nii.gz \\\n",
    "   /data/mydataset/brainmask.nii.gz\n",
    "datalad add -d /data/mydataset/ -m \"adding the best brainmask out there\" brainmask.nii.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "solution2": "hidden"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing /data/mydataset/scripts/count_voxels.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile /data/mydataset/scripts/count_voxels.py\n",
    "\n",
    "import nibabel as nb\n",
    "import sys\n",
    "print(nb.load(sys.argv[1]).get_data().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "add(ok): scripts/count_voxels.py (file) [non-large file; adding content to git repository]\n",
      "save(ok): /data/mydataset (dataset)\n",
      "action summary:\n",
      "  add (ok: 1)\n",
      "  save (ok: 1)\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "\n",
    "datalad add -d /data/mydataset --to-git -m \"New feature: magnificent script to count voxels\" scripts/count_voxels.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "solution2": "hidden"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "883352.0\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "\n",
    "cd /data/mydataset/\n",
    "python scripts/count_voxels.py brainmask.nii.gz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now run while recording produced output:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "get(notneeded): brainmask.nii.gz (file) [already present]\n",
      "add(ok): mask_count (file)\n",
      "save(ok): /data/mydataset (dataset)\n",
      "action summary:\n",
      "  add (ok: 1)\n",
      "  get (notneeded: 1)\n",
      "  save (ok: 1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO] == Command start (output follows) ===== \n",
      "[INFO] == Command exit (modification check follows) ===== \n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "\n",
    "cd /data/mydataset/\n",
    "datalad run --input brainmask.nii.gz --output mask_count bash -c 'python scripts/count_voxels.py {inputs} > {outputs}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "solution2": "hidden"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/data/mydataset/\n",
      "├── 123 -> .git/annex/objects/6v/gZ/MD5E-s4--9492fe88f263d58e0b686885e8c98c0e/MD5E-s4--9492fe88f263d58e0b686885e8c98c0e\n",
      "├── brainmask.nii.gz -> .git/annex/objects/jJ/Wz/MD5E-s93130--2572248880aa5978f8d4049feff2282a.nii.gz/MD5E-s93130--2572248880aa5978f8d4049feff2282a.nii.gz\n",
      "├── mask_count -> .git/annex/objects/wk/5f/MD5E-s9--b1f87c22419b3d0723026b361b26866c/MD5E-s9--b1f87c22419b3d0723026b361b26866c\n",
      "├── out -> .git/annex/objects/6w/1x/MD5E-s2--48a24b70a0b376535542b996af517398/MD5E-s2--48a24b70a0b376535542b996af517398\n",
      "└── scripts\n",
      "    ├── count_voxels.py\n",
      "    └── run.sh\n",
      "\n",
      "1 directory, 6 files\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "\n",
    "tree /data/mydataset/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "solution2": "hidden"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "commit dc02409cab83d4093724d664fda31ab4a5ffb0c8\n",
      "Author: yarikoptic <neuro>\n",
      "Date:   Thu Aug 2 02:46:55 2018 +0000\n",
      "\n",
      "    [DATALAD RUNCMD] bash -c 'python scripts/count_voxels.py ...\n",
      "    \n",
      "    === Do not change lines below ===\n",
      "    {\n",
      "     \"chain\": [],\n",
      "     \"cmd\": \"bash -c 'python scripts/count_voxels.py {inputs} > {outputs}'\",\n",
      "     \"dsid\": \"3c991c3a-95fe-11e8-8616-eb661f417fb8\",\n",
      "     \"exit\": 0,\n",
      "     \"inputs\": [\n",
      "      \"brainmask.nii.gz\"\n",
      "     ],\n",
      "     \"outputs\": [\n",
      "      \"mask_count\"\n",
      "     ],\n",
      "     \"pwd\": \".\"\n",
      "    }\n",
      "    ^^^ Do not change lines above ^^^\n",
      "\n",
      "diff --git a/mask_count b/mask_count\n",
      "new file mode 120000\n",
      "index 0000000..4dc1458\n",
      "--- /dev/null\n",
      "+++ b/mask_count\n",
      "@@ -0,0 +1 @@\n",
      "+.git/annex/objects/wk/5f/MD5E-s9--b1f87c22419b3d0723026b361b26866c/MD5E-s9--b1f87c22419b3d0723026b361b26866c\n",
      "\\ No newline at end of file\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "\n",
    "cd /data/mydataset/\n",
    "git show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "solution2": "hidden"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "whereis brainmask.nii.gz (1 copy) \n",
      "  \tede265e2-f3c3-4f61-96ca-3d5d4aa0a8ad -- jovyan@jupyter-yarikoptic:/data/mydataset [here]\n",
      "ok\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "cd /data/mydataset/\n",
    "git-annex whereis brainmask.nii.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "add(ok): out (file)\n",
      "save(ok): /data/mydataset (dataset)\n",
      "run(ok): /data/mydataset (dataset) [517fe3e does not have a command; cherry picking]\n",
      "run(ok): /data/mydataset (dataset) [230f628 does not have a command; cherry picking]\n",
      "get(notneeded): brainmask.nii.gz (file) [already present]\n",
      "add(ok): mask_count (file)\n",
      "save(ok): /data/mydataset (dataset)\n",
      "action summary:\n",
      "  add (ok: 2)\n",
      "  get (notneeded: 1)\n",
      "  run (ok: 2)\n",
      "  save (ok: 2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO] == Command start (output follows) ===== \n",
      "[INFO] == Command exit (modification check follows) ===== \n",
      "[INFO] == Command start (output follows) ===== \n",
      "[INFO] == Command exit (modification check follows) ===== \n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "\n",
    "datalad rerun -d /data/mydataset --since= --onto= --branch=verify2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "cd /data/mydataset\n",
    "git diff master verify2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* bdfe52d (HEAD -> verify2) [DATALAD RUNCMD] bash -c 'python scripts/count_voxels.py ...\n",
      "| mask_count\n",
      "* 4a7276a New feature: magnificent script to count voxels\n",
      "| scripts/count_voxels.py\n",
      "* f4ca086 adding the best brainmask out there\n",
      "| brainmask.nii.gz\n",
      "* 2b27e00 [DATALAD RUNCMD] Running the mighty script using datalad run\n",
      "| out\n",
      "| * dc02409 (master) [DATALAD RUNCMD] bash -c 'python scripts/count_voxels.py ...\n",
      "| | mask_count\n",
      "| * 230f628 New feature: magnificent script to count voxels\n",
      "| | scripts/count_voxels.py\n",
      "| * 517fe3e adding the best brainmask out there\n",
      "| | brainmask.nii.gz\n",
      "| * 742ca8c [DATALAD RUNCMD] Running the mighty script using datalad run\n",
      "|/  \n",
      "|   out\n",
      "| * 30edba4 (verify) [DATALAD RUNCMD] Running the mighty script using datalad run\n",
      "|/  \n",
      "|   out\n",
      "* fd195ce Added the mighty script\n",
      "| scripts/run.sh\n",
      "* 7161896 add modified file\n",
      "| 123\n",
      "* 8b54289 initial file\n",
      "| 123\n",
      "* ef2e870 [DATALAD] new dataset\n",
      "| .datalad/.gitattributes\n",
      "| .datalad/config\n",
      "| .gitattributes\n",
      "* ee11384 [DATALAD] Set default backend for all files to be MD5E\n",
      "  .gitattributes\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "\n",
    "cd /data/mydataset\n",
    "git log --oneline --graph --name-only --decorate master verify2 verify"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
