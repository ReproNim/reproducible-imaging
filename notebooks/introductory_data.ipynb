{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Versioning data and attaching metadata"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "pointer to git-annex + datalad\n",
    "\n",
    "1. Downloading a datalad dataset\n",
    "\n",
    "    a. searching\n",
    "    b. downloading metadata\n",
    "    c. getting data\n",
    "    d. dropping data\n",
    "\n",
    "2. Creating a datalad dataset\n",
    "\n",
    "    a. adding data\n",
    "    b. committing it\n",
    "    c. modifying it\n",
    "    d. committing it\n",
    "    e. adding a script\n",
    "    f. generating some analysis\n",
    "    g. committing the results\n",
    "\n",
    "3. Adding metadata for searching\n",
    "\n",
    "4. Using python api for datalad\n",
    "\n",
    "    a. searching\n",
    "    b. fetching\n",
    "\n",
    "5. BIDS example layout"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's start by using the shell version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Usage: datalad [global-opts] command [command-opts]\r\n",
      "\r\n",
      "DataLad provides a unified data distribution with the convenience of git-annex\r\n",
      "repositories as a backend.  DataLad command line tools allow to manipulate\r\n",
      "(obtain, create, update, publish, etc.) datasets and their collections.\r\n",
      "\r\n",
      "*Commands for dataset operations*\r\n",
      "\r\n",
      "  create\r\n",
      "      Create a new dataset from scratch\r\n",
      "  install\r\n",
      "      Install a dataset from a (remote) source\r\n",
      "  get\r\n",
      "      Get any dataset content (files/directories/subdatasets)\r\n",
      "  add\r\n",
      "      Add files/directories to an existing dataset\r\n",
      "  publish\r\n",
      "      Publish a dataset to a known sibling\r\n",
      "  uninstall\r\n",
      "      Uninstall subdatasets\r\n",
      "  drop\r\n",
      "      Drop file content from datasets\r\n",
      "  remove\r\n",
      "      Remove components from datasets\r\n",
      "  update\r\n",
      "      Update a dataset from a sibling\r\n",
      "  create-sibling\r\n",
      "      Create a dataset sibling on a UNIX-like SSH-accessible machine\r\n",
      "  create-sibling-github\r\n",
      "      Create dataset sibling on Github\r\n",
      "  unlock\r\n",
      "      Unlock file(s) of a dataset\r\n",
      "  save\r\n",
      "      Save the current state of a dataset\r\n",
      "  plugin\r\n",
      "      Generic plugin interface\r\n",
      "\r\n",
      "*Commands for meta data handling*\r\n",
      "\r\n",
      "  search\r\n",
      "      Search within available in datasets' meta data\r\n",
      "  metadata\r\n",
      "      Metadata manipulation for files and whole datasets\r\n",
      "  aggregate-metadata\r\n",
      "      Aggregate meta data of a dataset for later query\r\n",
      "\r\n",
      "*Miscellaneous commands*\r\n",
      "\r\n",
      "  test\r\n",
      "      Run internal DataLad (unit)tests\r\n",
      "  crawl\r\n",
      "      Crawl online resource to create or update a dataset\r\n",
      "  crawl-init\r\n",
      "      Initialize crawling configuration\r\n",
      "  ls\r\n",
      "      List summary information about URLs and dataset(s)\r\n",
      "  clean\r\n",
      "      Clean up after DataLad (possible temporary files etc.)\r\n",
      "  add-archive-content\r\n",
      "      Add content of an archive under git annex control\r\n",
      "  download-url\r\n",
      "      Download content\r\n",
      "\r\n",
      "*Plumbing commands*\r\n",
      "\r\n",
      "  annotate-paths\r\n",
      "      Analyze and act upon input paths\r\n",
      "  clone\r\n",
      "      Obtain a dataset copy from a URL or local source (path)\r\n",
      "  create-test-dataset\r\n",
      "      Create test (meta-)dataset\r\n",
      "  diff\r\n",
      "      Report changes of dataset component between revisions\r\n",
      "  siblings\r\n",
      "      Manage sibling configuration\r\n",
      "  sshrun\r\n",
      "      Run command on remote machines via SSH\r\n",
      "  subdatasets\r\n",
      "      Report subdatasets and their properties\r\n",
      "\r\n",
      "*General information*\r\n",
      "\r\n",
      "Detailed usage information for individual commands is available via\r\n",
      "command-specific --help, i.e.: datalad <command> --help\r\n",
      "\r\n",
      "\r\n",
      "*Global options*\r\n",
      "  -h, --help, --help-np\r\n",
      "                        show this help message. --help-np forcefully disables\r\n",
      "                        the use of a pager for displaying the help message\r\n",
      "  -l LEVEL, --log-level LEVEL\r\n",
      "                        set logging verbosity level. Choose among critical,\r\n",
      "                        error, warning, info, debug. Also you can specify an\r\n",
      "                        integer <10 to provide even more debugging information\r\n",
      "  --pbs-runner {condor}\r\n",
      "                        execute command by scheduling it via available PBS.\r\n",
      "                        For settings, config file will be consulted\r\n",
      "  -C PATH               run as if datalad was started in <path> instead of the\r\n",
      "                        current working directory. When multiple -C options\r\n",
      "                        are given, each subsequent non-absolute -C <path> is\r\n",
      "                        interpreted relative to the preceding -C <path>. This\r\n",
      "                        option affects the interpretations of the path names\r\n",
      "                        in that they are made relative to the working\r\n",
      "                        directory caused by the -C option\r\n",
      "  --version             show the program's version and license information\r\n",
      "  --dbg                 enter Python debugger when uncaught exception happens\r\n",
      "  --idbg                enter IPython debugger when uncaught exception happens\r\n",
      "  -c KEY=VALUE          configuration variable setting. Overrides any\r\n",
      "                        configuration read from a file, but is potentially\r\n",
      "                        overridden itself by configuration variables in the\r\n",
      "                        process environment.\r\n",
      "  --output-format {default,json,json_pp,tailored,'<template>'\r\n",
      "                        select format for returned command results. 'default'\r\n",
      "                        give one line per result reporting action, status,\r\n",
      "                        path and an optional message; 'json' renders a JSON\r\n",
      "                        object with all properties for each result (one per\r\n",
      "                        line); 'json_pp' pretty-prints JSON spanning multiple\r\n",
      "                        lines; 'tailored' enables a command-specific rendering\r\n",
      "                        style that is typically tailored to human consumption\r\n",
      "                        (no result output otherwise), '<template>' reports any\r\n",
      "                        value(s) of any result properties in any format\r\n",
      "                        indicated by the template (e.g. '{path}', compare with\r\n",
      "                        JSON output for all key-value choices).\r\n",
      "  --report-status {success,failure,ok,notneeded,impossible,error}\r\n",
      "                        constrain command result report to records matching\r\n",
      "                        the given status. 'success' is a synonym for 'ok' OR\r\n",
      "                        'notneeded', 'failure' stands for 'impossible' OR\r\n",
      "                        'error'.\r\n",
      "  --report-type {dataset,file}\r\n",
      "                        constrain command result report to records matching\r\n",
      "                        the given type. Can be given more than once to match\r\n",
      "                        multiple types.\r\n",
      "  --on-failure {ignore,continue,stop}\r\n",
      "                        when an operation fails: 'ignore' and continue with\r\n",
      "                        remaining operations, the error is logged but does not\r\n",
      "                        lead to a non-zero exit code of the command;\r\n",
      "                        'continue' works like 'ignore', but an error causes a\r\n",
      "                        non-zero exit code; 'stop' halts on first failure and\r\n",
      "                        yields non-zero exit code. A failure is any result\r\n",
      "                        with status 'impossible' or 'error'.\r\n",
      "  --run-before PLUGINSPEC [PLUGINSPEC ...]\r\n",
      "                        DataLad plugin to run after the command. PLUGINSPEC is\r\n",
      "                        a list comprised of a plugin name plus optional\r\n",
      "                        `key=value` pairs with arguments for the plugin call\r\n",
      "                        (see `plugin` command documentation for details). This\r\n",
      "                        option can be given more than once to run multiple\r\n",
      "                        plugins in the order in which they were given. For\r\n",
      "                        running plugins that require a --dataset argument it\r\n",
      "                        is important to provide the respective dataset as the\r\n",
      "                        --dataset argument of the main command, if it is not\r\n",
      "                        in the list of plugin arguments.\r\n",
      "  --run-after PLUGINSPEC [PLUGINSPEC ...]\r\n",
      "                        Like --run-before, but plugins are executed after the\r\n",
      "                        main command has finished.\r\n",
      "  --cmd                 syntactical helper that can be used to end the list of\r\n",
      "                        global command line options before the subcommand\r\n",
      "                        label. Options like --run-before can take an arbitray\r\n",
      "                        number of arguments and may require to be followed by\r\n",
      "                        a single --cmd in order to enable identification of\r\n",
      "                        the subcommand.\r\n",
      "\r\n",
      "\"Control Your Data\"\r\n",
      "\r\n"
     ]
    }
   ],
   "source": [
    "!datalad --help-np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Usage: datalad install [--version] [-h] [-l LEVEL] [--pbs-runner {condor}]\r\n",
      "                       [-s SOURCE] [-d DATASET] [-g] [-D DESCRIPTION] [-r]\r\n",
      "                       [--recursion-limit LEVELS] [--nosave] [--reckless]\r\n",
      "                       [-J NJOBS]\r\n",
      "                       [PATH [PATH ...]]\r\n",
      "\r\n",
      "Install a dataset from a (remote) source.\r\n",
      "\r\n",
      "This command creates a local sibling of an existing dataset from a\r\n",
      "(remote) location identified via a URL or path. Optional recursion into\r\n",
      "potential subdatasets, and download of all referenced data is supported.\r\n",
      "The new dataset can be optionally registered in an existing\r\n",
      "superdataset by identifying it via the DATASET argument (the new\r\n",
      "dataset's path needs to be located within the superdataset for that).\r\n",
      "\r\n",
      "It is recommended to provide a brief description to label the dataset's\r\n",
      "nature *and* location, e.g. \"Michael's music on black laptop\". This helps\r\n",
      "humans to identify data locations in distributed scenarios.  By default an\r\n",
      "identifier comprised of user and machine name, plus path will be generated.\r\n",
      "\r\n",
      "When only partial dataset content shall be obtained, it is recommended to\r\n",
      "use this command without the GET-DATA flag, followed by a\r\n",
      "`get` operation to obtain the desired data.\r\n",
      "\r\n",
      "NOTE\r\n",
      "  Power-user info: This command uses git clone, and\r\n",
      "  git annex init to prepare the dataset. Registering to a\r\n",
      "  superdataset is performed via a git submodule add operation\r\n",
      "  in the discovered superdataset.\r\n",
      "\r\n",
      "*Arguments*\r\n",
      "  PATH                  path/name of the installation target. If no PATH is\r\n",
      "                        provided a destination path will be derived from a\r\n",
      "                        source URL similar to git clone. [Default: None]\r\n",
      "\r\n",
      "*Options*\r\n",
      "  --version             show the program's version and license information\r\n",
      "  -h, --help, --help-np\r\n",
      "                        show this help message. --help-np forcefully disables\r\n",
      "                        the use of a pager for displaying the help message\r\n",
      "  -l LEVEL, --log-level LEVEL\r\n",
      "                        set logging verbosity level. Choose among critical,\r\n",
      "                        error, warning, info, debug. Also you can specify an\r\n",
      "                        integer <10 to provide even more debugging information\r\n",
      "  --pbs-runner {condor}\r\n",
      "                        execute command by scheduling it via available PBS.\r\n",
      "                        For settings, config file will be consulted\r\n",
      "  -s SOURCE, --source SOURCE\r\n",
      "                        URL or local path of the installation source.\r\n",
      "                        Constraints: value must be a string [Default: None]\r\n",
      "  -d DATASET, --dataset DATASET\r\n",
      "                        specify the dataset to perform the install operation\r\n",
      "                        on. If no dataset is given, an attempt is made to\r\n",
      "                        identify the dataset in a parent directory of the\r\n",
      "                        current working directory and/or the PATH given.\r\n",
      "                        Constraints: Value must be a Dataset or a valid\r\n",
      "                        identifier of a Dataset (e.g. a path) [Default: None]\r\n",
      "  -g, --get-data        if given, obtain all data content too. [Default:\r\n",
      "                        False]\r\n",
      "  -D DESCRIPTION, --description DESCRIPTION\r\n",
      "                        short description to use for a dataset location. Its\r\n",
      "                        primary purpose is to help humans to identify a\r\n",
      "                        dataset copy (e.g., \"mike's dataset on lab server\").\r\n",
      "                        Note that when a dataset is published, this\r\n",
      "                        information becomes available on the remote side.\r\n",
      "                        Constraints: value must be a string [Default: None]\r\n",
      "  -r, --recursive       if set, recurse into potential subdataset. [Default:\r\n",
      "                        False]\r\n",
      "  --recursion-limit LEVELS\r\n",
      "                        limit recursion into subdataset to the given number of\r\n",
      "                        levels. Constraints: value must be convertible to type\r\n",
      "                        'int' [Default: None]\r\n",
      "  --nosave              by default all modifications to a dataset are\r\n",
      "                        immediately saved. Given this option will disable this\r\n",
      "                        behavior. [Default: True]\r\n",
      "  --reckless            Set up the dataset to be able to obtain content in the\r\n",
      "                        cheapest/fastest possible way, even if this poses a\r\n",
      "                        potential risk the data integrity (e.g. hardlink files\r\n",
      "                        from a local clone of the dataset). Use with care, and\r\n",
      "                        limit to \"read-only\" use cases. With this flag the\r\n",
      "                        installed dataset will be marked as untrusted.\r\n",
      "                        [Default: False]\r\n",
      "  -J NJOBS, --jobs NJOBS\r\n",
      "                        how many parallel jobs (where possible) to use.\r\n",
      "                        Constraints: value must be convertible to type 'int'\r\n",
      "                        [Default: None]\r\n",
      "\r\n"
     ]
    }
   ],
   "source": [
    "!datalad install --help-np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/data\n"
     ]
    }
   ],
   "source": [
    "cd /data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0m\u001b[01;34mcifti-data\u001b[0m/  \u001b[01;34mds000114\u001b[0m/\r\n"
     ]
    }
   ],
   "source": [
    "ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "install(ok): /data/datasets.datalad.org (dataset)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO] Cloning dataset from 'http://datasets.datalad.org/' (trying 2 location candidate(s)) to '/data/datasets.datalad.org' \n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "\n",
    "datalad install /// "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Usage: datalad search [--version] [-h] [-l LEVEL] [--pbs-runner {condor}]\r\n",
      "                      [-d DATASET] [-s PROPERTY] [-r PROPERTY] [-R]\r\n",
      "                      [-f FORMAT] [--regex]\r\n",
      "                      STRING [STRING ...]\r\n",
      "\r\n",
      "Search within available in datasets' meta data\r\n",
      "\r\n",
      "*Arguments*\r\n",
      "  STRING                a string (or a regular expression if --regex) to\r\n",
      "                        search for in all meta data values. If multiple\r\n",
      "                        provided, all must have a match among some fields of a\r\n",
      "                        dataset.\r\n",
      "\r\n",
      "*Options*\r\n",
      "  --version             show the program's version and license information\r\n",
      "  -h, --help, --help-np\r\n",
      "                        show this help message. --help-np forcefully disables\r\n",
      "                        the use of a pager for displaying the help message\r\n",
      "  -l LEVEL, --log-level LEVEL\r\n",
      "                        set logging verbosity level. Choose among critical,\r\n",
      "                        error, warning, info, debug. Also you can specify an\r\n",
      "                        integer <10 to provide even more debugging information\r\n",
      "  --pbs-runner {condor}\r\n",
      "                        execute command by scheduling it via available PBS.\r\n",
      "                        For settings, config file will be consulted\r\n",
      "  -d DATASET, --dataset DATASET\r\n",
      "                        specify the dataset to perform the query operation on.\r\n",
      "                        If no dataset is given, an attempt is made to identify\r\n",
      "                        the dataset based on the current working directory\r\n",
      "                        and/or the PATH given. Constraints: Value must be a\r\n",
      "                        Dataset or a valid identifier of a Dataset (e.g. a\r\n",
      "                        path) [Default: None]\r\n",
      "  -s PROPERTY, --search PROPERTY\r\n",
      "                        name of the property to search for any match. This\r\n",
      "                        option can be given multiple times. By default, all\r\n",
      "                        properties are searched. [Default: None]\r\n",
      "  -r PROPERTY, --report PROPERTY\r\n",
      "                        name of the property to report for any match. This\r\n",
      "                        option can be given multiple times. If '*' is given,\r\n",
      "                        all properties are reported. [Default: None]\r\n",
      "  -R, --report-matched  flag to report those fields which have matches. If\r\n",
      "                        REPORT option values are provided, union of matched\r\n",
      "                        and those in REPORT will be output. [Default: False]\r\n",
      "  -f FORMAT, --format FORMAT\r\n",
      "                        format for output. Constraints: value must be one of\r\n",
      "                        ('custom', 'json', 'yaml') [Default: 'custom']\r\n",
      "  --regex               flag for STRING to be used as a (Python) regular\r\n",
      "                        expression which should match the value. [Default:\r\n",
      "                        False]\r\n",
      "\r\n"
     ]
    }
   ],
   "source": [
    "!datalad search --help-np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\u001b[1;37mINFO   \u001b[0m] Loading and caching local meta-data... might take a few seconds \r\n",
      "\u001b[1;4mdatasets.datalad.org/labs/haxby/attention\u001b[0m \r\n",
      "\u001b[1;4mdatasets.datalad.org/openfmri/ds000233\u001b[0m \r\n",
      "\u001b[1;4mdatasets.datalad.org/hbnssi\u001b[0m \r\n",
      "\u001b[1;4mdatasets.datalad.org/labs/haxby\u001b[0m \r\n",
      "\u001b[1;4mdatasets.datalad.org/labs/haxby/raiders\u001b[0m \r\n",
      "\u001b[1;4mdatasets.datalad.org/openfmri/ds000105\u001b[0m \r\n"
     ]
    }
   ],
   "source": [
    "!datalad search -d datasets.datalad.org haxby"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;4mdatasets.datalad.org/openfmri/ds000114\u001b[0m \r\n",
      "\u001b[1;4mdatasets.datalad.org/workshops/nih-2017/ds000114\u001b[0m \r\n",
      "\u001b[1;4mdatasets.datalad.org/openfmri/ds000158\u001b[0m \r\n",
      "\u001b[1;4mdatasets.datalad.org/openfmri/ds000221\u001b[0m \r\n",
      "\u001b[1;4mdatasets.datalad.org/workshops/nipype-2017/ds000114\u001b[0m \r\n"
     ]
    }
   ],
   "source": [
    "!datalad search -d datasets.datalad.org gorgolewski"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\u001b[1;37mINFO   \u001b[0m] Cloning dataset from 'http://datasets.datalad.org/workshops/.git' (trying 1 location candidate(s)) to '/data/datasets.datalad.org/workshops' \r\n",
      "\u001b[1;1minstall\u001b[0m(\u001b[1;32mok\u001b[0m): /data/datasets.datalad.org/workshops (\u001b[1;35mdataset\u001b[0m) [Installed subdataset in order to get /data/datasets.datalad.org/workshops/nih-2017/ds000114]\r\n",
      "[\u001b[1;37mINFO   \u001b[0m] Cloning dataset from 'http://datasets.datalad.org/workshops/nih-2017/.git' (trying 1 location candidate(s)) to '/data/datasets.datalad.org/workshops/nih-2017' \r\n",
      "\u001b[1;1minstall\u001b[0m(\u001b[1;32mok\u001b[0m): /data/datasets.datalad.org/workshops/nih-2017 (\u001b[1;35mdataset\u001b[0m) [Installed subdataset in order to get /data/datasets.datalad.org/workshops/nih-2017/ds000114]\r\n",
      "[\u001b[1;37mINFO   \u001b[0m] Cloning dataset from 'http://datasets.datalad.org/workshops/nih-2017/ds000114/.git' (trying 2 location candidate(s)) to '/data/datasets.datalad.org/workshops/nih-2017/ds000114' \r\n",
      "\u001b[1;1minstall\u001b[0m(\u001b[1;32mok\u001b[0m): /data/datasets.datalad.org/workshops/nih-2017/ds000114 (\u001b[1;35mdataset\u001b[0m) [Installed subdataset in order to get /data/datasets.datalad.org/workshops/nih-2017/ds000114]\r\n",
      "action summary:\r\n",
      "  install (ok: 3)\r\n"
     ]
    }
   ],
   "source": [
    "!datalad install datasets.datalad.org/workshops/nih-2017/ds000114"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[01;34mdatasets.datalad.org/workshops/nih-2017/ds000114\u001b[00m\r\n",
      "|-- CHANGES\r\n",
      "|-- dataset_description.json\r\n",
      "|-- \u001b[01;34mderivatives\u001b[00m\r\n",
      "|   |-- \u001b[01;34mfmriprep\u001b[00m\r\n",
      "|   `-- \u001b[01;34mfreesurfer\u001b[00m\r\n",
      "|-- \u001b[40;31;01mdwi.bval\u001b[00m -> \u001b[00m.git/annex/objects/JX/4K/MD5E-s335--5bd6fa32ccd0c79e79f9ac63a2c09c1a.bval/MD5E-s335--5bd6fa32ccd0c79e79f9ac63a2c09c1a.bval\u001b[00m\r\n",
      "|-- \u001b[40;31;01mdwi.bvec\u001b[00m -> \u001b[00m.git/annex/objects/Pg/wk/MD5E-s1248--0641c68ff6ee6164928c984541653430.bvec/MD5E-s1248--0641c68ff6ee6164928c984541653430.bvec\u001b[00m\r\n",
      "|-- \u001b[01;34msub-01\u001b[00m\r\n",
      "|   |-- \u001b[01;34mses-retest\u001b[00m\r\n",
      "|   |   |-- \u001b[01;34manat\u001b[00m\r\n",
      "|   |   |-- \u001b[01;34mdwi\u001b[00m\r\n",
      "|   |   `-- \u001b[01;34mfunc\u001b[00m\r\n",
      "|   `-- \u001b[01;34mses-test\u001b[00m\r\n",
      "|       |-- \u001b[01;34manat\u001b[00m\r\n",
      "|       |-- \u001b[01;34mdwi\u001b[00m\r\n",
      "|       `-- \u001b[01;34mfunc\u001b[00m\r\n",
      "|-- \u001b[01;34msub-02\u001b[00m\r\n",
      "|   |-- \u001b[01;34mses-retest\u001b[00m\r\n",
      "|   |   |-- \u001b[01;34manat\u001b[00m\r\n",
      "|   |   |-- \u001b[01;34mdwi\u001b[00m\r\n",
      "|   |   `-- \u001b[01;34mfunc\u001b[00m\r\n",
      "|   `-- \u001b[01;34mses-test\u001b[00m\r\n",
      "|       |-- \u001b[01;34manat\u001b[00m\r\n",
      "|       |-- \u001b[01;34mdwi\u001b[00m\r\n",
      "|       `-- \u001b[01;34mfunc\u001b[00m\r\n",
      "|-- \u001b[01;34msub-03\u001b[00m\r\n",
      "|   |-- \u001b[01;34mses-retest\u001b[00m\r\n",
      "|   |   |-- \u001b[01;34manat\u001b[00m\r\n",
      "|   |   |-- \u001b[01;34mdwi\u001b[00m\r\n",
      "|   |   `-- \u001b[01;34mfunc\u001b[00m\r\n",
      "|   `-- \u001b[01;34mses-test\u001b[00m\r\n",
      "|       |-- \u001b[01;34manat\u001b[00m\r\n",
      "|       |-- \u001b[01;34mdwi\u001b[00m\r\n",
      "|       `-- \u001b[01;34mfunc\u001b[00m\r\n",
      "|-- \u001b[01;34msub-04\u001b[00m\r\n",
      "|   |-- \u001b[01;34mses-retest\u001b[00m\r\n",
      "|   |   |-- \u001b[01;34manat\u001b[00m\r\n",
      "|   |   |-- \u001b[01;34mdwi\u001b[00m\r\n",
      "|   |   `-- \u001b[01;34mfunc\u001b[00m\r\n",
      "|   `-- \u001b[01;34mses-test\u001b[00m\r\n",
      "|       |-- \u001b[01;34manat\u001b[00m\r\n",
      "|       |-- \u001b[01;34mdwi\u001b[00m\r\n",
      "|       `-- \u001b[01;34mfunc\u001b[00m\r\n",
      "|-- \u001b[01;34msub-05\u001b[00m\r\n",
      "|   |-- \u001b[01;34mses-retest\u001b[00m\r\n",
      "|   |   |-- \u001b[01;34manat\u001b[00m\r\n",
      "|   |   |-- \u001b[01;34mdwi\u001b[00m\r\n",
      "|   |   `-- \u001b[01;34mfunc\u001b[00m\r\n",
      "|   `-- \u001b[01;34mses-test\u001b[00m\r\n",
      "|       |-- \u001b[01;34manat\u001b[00m\r\n",
      "|       |-- \u001b[01;34mdwi\u001b[00m\r\n",
      "|       `-- \u001b[01;34mfunc\u001b[00m\r\n",
      "|-- \u001b[01;34msub-06\u001b[00m\r\n",
      "|   |-- \u001b[01;34mses-retest\u001b[00m\r\n",
      "|   |   |-- \u001b[01;34manat\u001b[00m\r\n",
      "|   |   |-- \u001b[01;34mdwi\u001b[00m\r\n",
      "|   |   `-- \u001b[01;34mfunc\u001b[00m\r\n",
      "|   `-- \u001b[01;34mses-test\u001b[00m\r\n",
      "|       |-- \u001b[01;34manat\u001b[00m\r\n",
      "|       |-- \u001b[01;34mdwi\u001b[00m\r\n",
      "|       `-- \u001b[01;34mfunc\u001b[00m\r\n",
      "|-- \u001b[01;34msub-07\u001b[00m\r\n",
      "|   |-- \u001b[01;34mses-retest\u001b[00m\r\n",
      "|   |   |-- \u001b[01;34manat\u001b[00m\r\n",
      "|   |   |-- \u001b[01;34mdwi\u001b[00m\r\n",
      "|   |   `-- \u001b[01;34mfunc\u001b[00m\r\n",
      "|   `-- \u001b[01;34mses-test\u001b[00m\r\n",
      "|       |-- \u001b[01;34manat\u001b[00m\r\n",
      "|       |-- \u001b[01;34mdwi\u001b[00m\r\n",
      "|       `-- \u001b[01;34mfunc\u001b[00m\r\n",
      "|-- \u001b[01;34msub-08\u001b[00m\r\n",
      "|   |-- \u001b[01;34mses-retest\u001b[00m\r\n",
      "|   |   |-- \u001b[01;34manat\u001b[00m\r\n",
      "|   |   |-- \u001b[01;34mdwi\u001b[00m\r\n",
      "|   |   `-- \u001b[01;34mfunc\u001b[00m\r\n",
      "|   `-- \u001b[01;34mses-test\u001b[00m\r\n",
      "|       |-- \u001b[01;34manat\u001b[00m\r\n",
      "|       |-- \u001b[01;34mdwi\u001b[00m\r\n",
      "|       `-- \u001b[01;34mfunc\u001b[00m\r\n",
      "|-- \u001b[01;34msub-09\u001b[00m\r\n",
      "|   |-- \u001b[01;34mses-retest\u001b[00m\r\n",
      "|   |   |-- \u001b[01;34manat\u001b[00m\r\n",
      "|   |   |-- \u001b[01;34mdwi\u001b[00m\r\n",
      "|   |   `-- \u001b[01;34mfunc\u001b[00m\r\n",
      "|   `-- \u001b[01;34mses-test\u001b[00m\r\n",
      "|       |-- \u001b[01;34manat\u001b[00m\r\n",
      "|       |-- \u001b[01;34mdwi\u001b[00m\r\n",
      "|       `-- \u001b[01;34mfunc\u001b[00m\r\n",
      "|-- \u001b[01;34msub-10\u001b[00m\r\n",
      "|   |-- \u001b[01;34mses-retest\u001b[00m\r\n",
      "|   |   |-- \u001b[01;34manat\u001b[00m\r\n",
      "|   |   |-- \u001b[01;34mdwi\u001b[00m\r\n",
      "|   |   `-- \u001b[01;34mfunc\u001b[00m\r\n",
      "|   `-- \u001b[01;34mses-test\u001b[00m\r\n",
      "|       |-- \u001b[01;34manat\u001b[00m\r\n",
      "|       |-- \u001b[01;34mdwi\u001b[00m\r\n",
      "|       `-- \u001b[01;34mfunc\u001b[00m\r\n",
      "|-- task-covertverbgeneration_bold.json\r\n",
      "|-- task-covertverbgeneration_events.tsv\r\n",
      "|-- task-fingerfootlips_bold.json\r\n",
      "|-- task-fingerfootlips_events.tsv\r\n",
      "|-- task-linebisection_bold.json\r\n",
      "|-- task-overtverbgeneration_bold.json\r\n",
      "|-- task-overtverbgeneration_events.tsv\r\n",
      "|-- task-overtwordrepetition_bold.json\r\n",
      "`-- task-overtwordrepetition_events.tsv\r\n",
      "\r\n",
      "93 directories, 13 files\r\n"
     ]
    }
   ],
   "source": [
    "!tree -L 3 datasets.datalad.org/workshops/nih-2017/ds000114"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cat: datasets.datalad.org/workshops/nih-2017/ds000114/dwi.bval: No such file or directory\r\n"
     ]
    }
   ],
   "source": [
    "!cat datasets.datalad.org/workshops/nih-2017/ds000114/dwi.bval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "Total:   0%|                                          | 0.00/335 [00:00<?, ?B/s]\r",
      "Total: 100%|#####################################| 335/335 [00:00<00:00, 734B/s]\r",
      "                                                                                \r",
      "Total (1 ok out of 1)100%|#######################| 335/335 [00:00<00:00, 734B/s]\r",
      "                                                                                \r",
      "\u001b[1;1mget\u001b[0m(\u001b[1;32mok\u001b[0m): /data/datasets.datalad.org/workshops/nih-2017/ds000114/dwi.bval (\u001b[1;35mfile\u001b[0m)\r\n"
     ]
    }
   ],
   "source": [
    "!datalad get datasets.datalad.org/workshops/nih-2017/ds000114/dwi.bval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0 0 0 0 0 0 1000 1000 1000 1000 1000 1000 1000 1000 1000 1000 1000 1000 1000 1000 1000 1000 1000 1000 1000 1000 1000 1000 1000 1000 1000 1000 1000 1000 1000 1000 1000 1000 1000 1000 1000 1000 1000 1000 1000 1000 1000 1000 1000 1000 1000 1000 1000 1000 1000 1000 1000 1000 1000 1000 1000 1000 1000 1000 1000 1000 1000 1000 1000 1000 \r\n"
     ]
    }
   ],
   "source": [
    "!cat datasets.datalad.org/workshops/nih-2017/ds000114/dwi.bval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;4mdatasets.datalad.org/workshops/nih-2017/ds000114\u001b[0m   [annex]  master  2.0.1-9-g324ebf438 2017-07-30/02:51:13  \u001b[1;32m���\u001b[0m\r\n"
     ]
    }
   ],
   "source": [
    "!LC_ALL= datalad ls datasets.datalad.org/workshops/nih-2017/ds000114"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "git-annex: Not in a git repository.\r\n"
     ]
    }
   ],
   "source": [
    "!git-annex list datasets.datalad.org/workshops/nih-2017/ds000114/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "here\r\n",
      "|origin\r\n",
      "||web\r\n",
      "|||bittorrent\r\n",
      "||||datalad-archives\r\n",
      "|||||\r\n",
      "git-annex: dwi* not found\r\n",
      "git-annex: list: 1 failed\r\n"
     ]
    }
   ],
   "source": [
    "!git -C datasets.datalad.org/workshops/nih-2017/ds000114/ annex list dwi*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "git-annex list - show which remotes contain files\r\n",
      "\r\n",
      "Usage: git-annex list [PATH ...] [--allrepos]\r\n",
      "\r\n",
      "Available options:\r\n",
      "  --allrepos               show all repositories, not only remotes\r\n",
      "  --force                  allow actions that may lose annexed data\r\n",
      "  -F,--fast                avoid slow operations\r\n",
      "  -q,--quiet               avoid verbose output\r\n",
      "  -v,--verbose             allow verbose output (default)\r\n",
      "  -d,--debug               show debug messages\r\n",
      "  --no-debug               don't show debug messages\r\n",
      "  -b,--backend NAME        specify key-value backend to use\r\n",
      "  -N,--numcopies NUMBER    override default number of copies\r\n",
      "  --trust REMOTE           override trust setting\r\n",
      "  --semitrust REMOTE       override trust setting back to default\r\n",
      "  --untrust REMOTE         override trust setting to untrusted\r\n",
      "  -c,--config NAME=VALUE   override git configuration setting\r\n",
      "  --user-agent NAME        override default User-Agent\r\n",
      "  --trust-glacier          Trust Amazon Glacier inventory\r\n",
      "  --notify-finish          show desktop notification after transfer finishes\r\n",
      "  --notify-start           show desktop notification after transfer starts\r\n",
      "  -i,--in REMOTE           match files present in a remote\r\n",
      "  -C,--copies REMOTE       skip files with fewer copies\r\n",
      "  --lackingcopies NUMBER   match files that need more copies\r\n",
      "  --approxlackingcopies NUMBER\r\n",
      "                           match files that need more copies (faster)\r\n",
      "  -B,--inbackend NAME      match files using a key-value backend\r\n",
      "  --securehash             match files using a cryptographically secure hash\r\n",
      "  --inallgroup GROUP       match files present in all remotes in a group\r\n",
      "  --metadata FIELD=VALUE   match files with attached metadata\r\n",
      "  --want-get               match files the repository wants to get\r\n",
      "  --want-drop              match files the repository wants to drop\r\n",
      "  -x,--exclude GLOB        skip files matching the glob pattern\r\n",
      "  -I,--include GLOB        limit to files matching the glob pattern\r\n",
      "  --largerthan SIZE        match files larger than a size\r\n",
      "  --smallerthan SIZE       match files smaller than a size\r\n",
      "  --not                    negate next option\r\n",
      "  --and                    both previous and next option must match\r\n",
      "  --or                     either previous or next option must match\r\n",
      "  -(                       open group of options\r\n",
      "  -)                       close group of options\r\n",
      "  -T,--time-limit TIME     stop after the specified amount of time\r\n",
      "  -h,--help                Show this help text\r\n",
      "\r\n",
      "For details, run: git-annex help list\r\n"
     ]
    }
   ],
   "source": [
    "!git-annex list --help"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "here\n",
      "|origin\n",
      "||web\n",
      "|||bittorrent\n",
      "||||datalad-archives\n",
      "|||||\n",
      "X_X_X dwi.bval\n",
      "__X_X dwi.bvec\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "\n",
    "cd datasets.datalad.org/workshops/nih-2017/ds000114/\n",
    "git-annex list dwi.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;1mdrop\u001b[0m(\u001b[1;32mok\u001b[0m): /data/datasets.datalad.org/workshops/nih-2017/ds000114/dwi.bval (\u001b[1;35mfile\u001b[0m) [checking http://openneuro.s3.amazonaws.com/ds000114/ds000114_R2.0.0/uncompressed/dwi.bval?versionId=null...]\r\n"
     ]
    }
   ],
   "source": [
    "!datalad drop datasets.datalad.org/workshops/nih-2017/ds000114/dwi.bval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "here\n",
      "|origin\n",
      "||web\n",
      "|||bittorrent\n",
      "||||datalad-archives\n",
      "|||||\n",
      "__X_X dwi.bval\n",
      "__X_X dwi.bvec\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "\n",
    "cd datasets.datalad.org/workshops/nih-2017/ds000114/\n",
    "git-annex list dwi.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0m\u001b[01;34mcifti-data\u001b[0m/  \u001b[01;34mdatasets.datalad.org\u001b[0m/  \u001b[01;34mds000114\u001b[0m/\r\n"
     ]
    }
   ],
   "source": [
    "ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\u001b[1;37mINFO   \u001b[0m] Creating a new annex repo at /data/mydataset \r\n",
      "\r",
      "Total:   0%|                                         | 0.00/21.0 [00:00<?, ?B/s]\r",
      "                                                                                \r",
      "\r",
      "Total:   0%|                                          | 0.00/233 [00:00<?, ?B/s]\r",
      "                                                                                \r",
      "\r",
      "Total:   0%|                                          | 0.00/233 [00:00<?, ?B/s]\r",
      "                                                                                \r",
      "\u001b[1;1mcreate\u001b[0m(\u001b[1;32mok\u001b[0m): /data/mydataset (\u001b[1;35mdataset\u001b[0m)\r\n"
     ]
    }
   ],
   "source": [
    "!datalad create mydataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "Total:   0%|          | 0.00/4.00 [00:00<?, ?B/s]\r",
      "Total: 100%|##########| 4.00/4.00 [00:00<00:00, 31.2B/s]\r",
      "          \r",
      "Total (1 ok out of 1)100%|##########| 4.00/4.00 [00:00<00:00, 31.2B/s]\r",
      "                                                        \r",
      "add(ok): /data/mydataset/123 (file)\n",
      "save(ok): /data/mydataset (dataset)\n",
      "action summary:\n",
      "  add (ok: 1)\n",
      "  save (ok: 1)\n"
     ]
    }
   ],
   "source": [
    "%%bash \n",
    "\n",
    "echo \"123\" > mydataset/123\n",
    "datalad add mydataset/123"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "here\n",
      "|web\n",
      "||bittorrent\n",
      "|||\n",
      "X__ 123\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "\n",
    "cd mydataset\n",
    "git-annex list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[01;34mmydataset\u001b[00m\r\n",
      "`-- \u001b[01;36m123\u001b[00m -> .git/annex/objects/pF/Zf/MD5E-s4--ba1f2511fc30423bdbb183fe33f3dd0f/MD5E-s4--ba1f2511fc30423bdbb183fe33f3dd0f\r\n",
      "\r\n",
      "0 directories, 1 file\r\n"
     ]
    }
   ],
   "source": [
    "!tree mydataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "123\r\n"
     ]
    }
   ],
   "source": [
    "!cat mydataset/.git/annex/objects/pF/Zf/MD5E-s4--ba1f2511fc30423bdbb183fe33f3dd0f/MD5E-s4--ba1f2511fc30423bdbb183fe33f3dd0f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\u001b[1;31mERROR  \u001b[0m] Failed to run ['git', '-c', 'receive.autogc=0', '-c', 'gc.auto=0', 'annex', 'drop', '--json', '123'] under '/data/mydataset'. Exit code=1. out={\"command\":\"drop\",\"wanted\":[],\"note\":\"(Use --force to override this check, or adjust numcopies.)\",\"skipped\":[],\"success\":false,\"key\":\"MD5E-s4--ba1f2511fc30423bdbb183fe33f3dd0f\",\"file\":\"123\"}\r\n",
      "|  err=git-annex: drop: 1 failed\r\n",
      "|  \r\n",
      "[\u001b[1;31mERROR  \u001b[0m] configured minimum number of copies not found [drop(/data/mydataset/123)] \r\n",
      "\u001b[1;1mdrop\u001b[0m(\u001b[1;31merror\u001b[0m): /data/mydataset/123 (\u001b[1;35mfile\u001b[0m) [configured minimum number of copies not found]\r\n"
     ]
    }
   ],
   "source": [
    "!datalad drop mydataset/123"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/usr/bin/sh: 1: cannot create mydataset/123: Permission denied\r\n"
     ]
    }
   ],
   "source": [
    "!echo \"321\" > mydataset/123"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "unlock(ok): 123 (file)\n",
      "\r",
      "Total:   0%|          | 0.00/4.00 [00:00<?, ?B/s]\r",
      "Total: 100%|##########| 4.00/4.00 [00:00<00:00, 18.1B/s]\r",
      "          \r",
      "Total (1 ok out of 1)100%|##########| 4.00/4.00 [00:00<00:00, 18.1B/s]\r",
      "                                                        \r",
      "add(ok): /data/mydataset/123 (file)\n",
      "\r",
      "Total:   0%|          | 0.00/4.00 [00:00<?, ?B/s]\r",
      "                                                 \r",
      "save(ok): /data/mydataset (dataset)\n",
      "action summary:\n",
      "  add (ok: 1)\n",
      "  save (ok: 1)\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "\n",
    "datalad unlock mydataset/123\n",
    "echo \"321\" > mydataset/123\n",
    "datalad add mydataset/123"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/usr/bin/sh: 1: cannot create mydataset/123: Permission denied\r\n"
     ]
    }
   ],
   "source": [
    "!echo \"123\" > mydataset/123"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[01;34mmydataset\u001b[00m\r\n",
      "`-- \u001b[01;36m123\u001b[00m -> .git/annex/objects/6v/gZ/MD5E-s4--9492fe88f263d58e0b686885e8c98c0e/MD5E-s4--9492fe88f263d58e0b686885e8c98c0e\r\n",
      "\r\n",
      "0 directories, 1 file\r\n"
     ]
    }
   ],
   "source": [
    "!tree mydataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "321\r\n"
     ]
    }
   ],
   "source": [
    "!cat mydataset/.git/annex/objects/6v/gZ/MD5E-s4--9492fe88f263d58e0b686885e8c98c0e/MD5E-s4--9492fe88f263d58e0b686885e8c98c0e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "123\r\n"
     ]
    }
   ],
   "source": [
    "!cat mydataset/.git/annex/objects/pF/Zf/MD5E-s4--ba1f2511fc30423bdbb183fe33f3dd0f/MD5E-s4--ba1f2511fc30423bdbb183fe33f3dd0f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "commit a78256fc3061be8e8279450cabf1585ec3d8c84e\n",
      "Author: neuro <neuro>\n",
      "Date:   Fri Aug 11 22:27:14 2017 +0000\n",
      "\n",
      "    [DATALAD] added content\n",
      "\n",
      "commit 22b057eed990e359b4dc3b2660b3a59f9d01abf7\n",
      "Author: neuro <neuro>\n",
      "Date:   Fri Aug 11 22:27:02 2017 +0000\n",
      "\n",
      "    [DATALAD] added content\n",
      "\n",
      "commit 6dfa9690b1dfd2da0c0b0f79788c664858805472\n",
      "Author: neuro <neuro>\n",
      "Date:   Fri Aug 11 22:27:01 2017 +0000\n",
      "\n",
      "    [DATALAD] new dataset\n",
      "\n",
      "commit 4c59b78d86f799490e6091fcaba946c02b6a0035\n",
      "Author: neuro <neuro>\n",
      "Date:   Fri Aug 11 22:27:00 2017 +0000\n",
      "\n",
      "    [DATALAD] Set default backend for all files to be MD5E\n"
     ]
    }
   ],
   "source": [
    "%%bash \n",
    "\n",
    "cd mydataset/\n",
    "git log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#!/bin/bash\n",
      "cat $1 | wc -c\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "\n",
    "cd mydataset\n",
    "mkdir -p scripts\n",
    "cmd=$(cat << EOM\n",
    "#!/bin/bash\\ncat \\$1 | wc -c\n",
    "EOM\n",
    ")\n",
    "echo -e $cmd > scripts/run.sh\n",
    "chmod +x scripts/run.sh\n",
    "cat scripts/run.sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "Total:   0%|          | 0.00/2.00 [00:00<?, ?B/s]\r",
      "Total: 100%|##########| 2.00/2.00 [00:00<00:00, 16.8B/s]\r",
      "          \r",
      "Total (1 ok out of 1)100%|##########| 2.00/2.00 [00:00<00:00, 16.8B/s]\r",
      "                                                        \r",
      "add(ok): /data/mydataset/out (file)\n",
      "\r",
      "Total:   0%|          | 0.00/2.00 [00:00<?, ?B/s]\r",
      "                                                 \r",
      "save(ok): /data/mydataset (dataset)\n",
      "action summary:\n",
      "  add (ok: 1)\n",
      "  save (ok: 1)\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "cd mydataset\n",
    "scripts/run.sh 123 > out\n",
    "datalad add out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
