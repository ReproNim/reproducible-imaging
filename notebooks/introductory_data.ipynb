{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Versioning data and attaching metadata"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "pointer to git-annex + datalad\n",
    "\n",
    "1. Downloading a datalad dataset\n",
    "\n",
    "    a. searching\n",
    "    b. downloading metadata\n",
    "    c. getting data\n",
    "    d. dropping data\n",
    "\n",
    "2. Creating a datalad dataset\n",
    "\n",
    "    a. adding data\n",
    "    b. committing it\n",
    "    c. modifying it\n",
    "    d. committing it\n",
    "    e. adding a script\n",
    "    f. generating some analysis\n",
    "    g. committing the results\n",
    "\n",
    "3. Adding metadata for searching\n",
    "\n",
    "4. Using python api for datalad\n",
    "\n",
    "    a. searching\n",
    "    b. fetching\n",
    "\n",
    "5. BIDS example layout"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's start by using the shell version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Usage: datalad [global-opts] command [command-opts]\r\n",
      "\r\n",
      "DataLad provides a unified data distribution with the convenience of git-annex\r\n",
      "repositories as a backend.  DataLad command line tools allow to manipulate\r\n",
      "(obtain, create, update, publish, etc.) datasets and their collections.\r\n",
      "\r\n",
      "*Commands for dataset operations*\r\n",
      "\r\n",
      "  create\r\n",
      "      Create a new dataset from scratch\r\n",
      "  install\r\n",
      "      Install a dataset from a (remote) source\r\n",
      "  get\r\n",
      "      Get any dataset content (files/directories/subdatasets)\r\n",
      "  add\r\n",
      "      Add files/directories to an existing dataset\r\n",
      "  publish\r\n",
      "      Publish a dataset to a known sibling\r\n",
      "  uninstall\r\n",
      "      Uninstall subdatasets\r\n",
      "  drop\r\n",
      "      Drop file content from datasets\r\n",
      "  remove\r\n",
      "      Remove components from datasets\r\n",
      "  update\r\n",
      "      Update a dataset from a sibling\r\n",
      "  create-sibling\r\n",
      "      Create a dataset sibling on a UNIX-like SSH-accessible machine\r\n",
      "  create-sibling-github\r\n",
      "      Create dataset sibling on Github\r\n",
      "  unlock\r\n",
      "      Unlock file(s) of a dataset\r\n",
      "  save\r\n",
      "      Save the current state of a dataset\r\n",
      "  plugin\r\n",
      "      Generic plugin interface\r\n",
      "\r\n",
      "*Commands for meta data handling*\r\n",
      "\r\n",
      "  search\r\n",
      "      Search within available in datasets' meta data\r\n",
      "  metadata\r\n",
      "      Metadata manipulation for files and whole datasets\r\n",
      "  aggregate-metadata\r\n",
      "      Aggregate meta data of a dataset for later query\r\n",
      "\r\n",
      "*Miscellaneous commands*\r\n",
      "\r\n",
      "  test\r\n",
      "      Run internal DataLad (unit)tests\r\n",
      "  crawl\r\n",
      "      Crawl online resource to create or update a dataset\r\n",
      "  crawl-init\r\n",
      "      Initialize crawling configuration\r\n",
      "  ls\r\n",
      "      List summary information about URLs and dataset(s)\r\n",
      "  clean\r\n",
      "      Clean up after DataLad (possible temporary files etc.)\r\n",
      "  add-archive-content\r\n",
      "      Add content of an archive under git annex control\r\n",
      "  download-url\r\n",
      "      Download content\r\n",
      "\r\n",
      "*Plumbing commands*\r\n",
      "\r\n",
      "  annotate-paths\r\n",
      "      Analyze and act upon input paths\r\n",
      "  clone\r\n",
      "      Obtain a dataset copy from a URL or local source (path)\r\n",
      "  create-test-dataset\r\n",
      "      Create test (meta-)dataset\r\n",
      "  diff\r\n",
      "      Report changes of dataset component between revisions\r\n",
      "  siblings\r\n",
      "      Manage sibling configuration\r\n",
      "  sshrun\r\n",
      "      Run command on remote machines via SSH\r\n",
      "  subdatasets\r\n",
      "      Report subdatasets and their properties\r\n",
      "\r\n",
      "*General information*\r\n",
      "\r\n",
      "Detailed usage information for individual commands is available via\r\n",
      "command-specific --help, i.e.: datalad <command> --help\r\n",
      "\r\n",
      "\r\n",
      "*Global options*\r\n",
      "  -h, --help, --help-np\r\n",
      "                        show this help message. --help-np forcefully disables\r\n",
      "                        the use of a pager for displaying the help message\r\n",
      "  -l LEVEL, --log-level LEVEL\r\n",
      "                        set logging verbosity level. Choose among critical,\r\n",
      "                        error, warning, info, debug. Also you can specify an\r\n",
      "                        integer <10 to provide even more debugging information\r\n",
      "  --pbs-runner {condor}\r\n",
      "                        execute command by scheduling it via available PBS.\r\n",
      "                        For settings, config file will be consulted\r\n",
      "  -C PATH               run as if datalad was started in <path> instead of the\r\n",
      "                        current working directory. When multiple -C options\r\n",
      "                        are given, each subsequent non-absolute -C <path> is\r\n",
      "                        interpreted relative to the preceding -C <path>. This\r\n",
      "                        option affects the interpretations of the path names\r\n",
      "                        in that they are made relative to the working\r\n",
      "                        directory caused by the -C option\r\n",
      "  --version             show the program's version and license information\r\n",
      "  --dbg                 enter Python debugger when uncaught exception happens\r\n",
      "  --idbg                enter IPython debugger when uncaught exception happens\r\n",
      "  -c KEY=VALUE          configuration variable setting. Overrides any\r\n",
      "                        configuration read from a file, but is potentially\r\n",
      "                        overridden itself by configuration variables in the\r\n",
      "                        process environment.\r\n",
      "  --output-format {default,json,json_pp,tailored,'<template>'\r\n",
      "                        select format for returned command results. 'default'\r\n",
      "                        give one line per result reporting action, status,\r\n",
      "                        path and an optional message; 'json' renders a JSON\r\n",
      "                        object with all properties for each result (one per\r\n",
      "                        line); 'json_pp' pretty-prints JSON spanning multiple\r\n",
      "                        lines; 'tailored' enables a command-specific rendering\r\n",
      "                        style that is typically tailored to human consumption\r\n",
      "                        (no result output otherwise), '<template>' reports any\r\n",
      "                        value(s) of any result properties in any format\r\n",
      "                        indicated by the template (e.g. '{path}', compare with\r\n",
      "                        JSON output for all key-value choices).\r\n",
      "  --report-status {success,failure,ok,notneeded,impossible,error}\r\n",
      "                        constrain command result report to records matching\r\n",
      "                        the given status. 'success' is a synonym for 'ok' OR\r\n",
      "                        'notneeded', 'failure' stands for 'impossible' OR\r\n",
      "                        'error'.\r\n",
      "  --report-type {dataset,file}\r\n",
      "                        constrain command result report to records matching\r\n",
      "                        the given type. Can be given more than once to match\r\n",
      "                        multiple types.\r\n",
      "  --on-failure {ignore,continue,stop}\r\n",
      "                        when an operation fails: 'ignore' and continue with\r\n",
      "                        remaining operations, the error is logged but does not\r\n",
      "                        lead to a non-zero exit code of the command;\r\n",
      "                        'continue' works like 'ignore', but an error causes a\r\n",
      "                        non-zero exit code; 'stop' halts on first failure and\r\n",
      "                        yields non-zero exit code. A failure is any result\r\n",
      "                        with status 'impossible' or 'error'.\r\n",
      "  --run-before PLUGINSPEC [PLUGINSPEC ...]\r\n",
      "                        DataLad plugin to run after the command. PLUGINSPEC is\r\n",
      "                        a list comprised of a plugin name plus optional\r\n",
      "                        `key=value` pairs with arguments for the plugin call\r\n",
      "                        (see `plugin` command documentation for details). This\r\n",
      "                        option can be given more than once to run multiple\r\n",
      "                        plugins in the order in which they were given. For\r\n",
      "                        running plugins that require a --dataset argument it\r\n",
      "                        is important to provide the respective dataset as the\r\n",
      "                        --dataset argument of the main command, if it is not\r\n",
      "                        in the list of plugin arguments.\r\n",
      "  --run-after PLUGINSPEC [PLUGINSPEC ...]\r\n",
      "                        Like --run-before, but plugins are executed after the\r\n",
      "                        main command has finished.\r\n",
      "  --cmd                 syntactical helper that can be used to end the list of\r\n",
      "                        global command line options before the subcommand\r\n",
      "                        label. Options like --run-before can take an arbitray\r\n",
      "                        number of arguments and may require to be followed by\r\n",
      "                        a single --cmd in order to enable identification of\r\n",
      "                        the subcommand.\r\n",
      "\r\n",
      "\"Control Your Data\"\r\n",
      "\r\n"
     ]
    }
   ],
   "source": [
    "!datalad --help"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Usage: datalad install [--version] [-h] [-l LEVEL] [--pbs-runner {condor}]\r\n",
      "                       [-s SOURCE] [-d DATASET] [-g] [-D DESCRIPTION] [-r]\r\n",
      "                       [--recursion-limit LEVELS] [--nosave] [--reckless]\r\n",
      "                       [-J NJOBS]\r\n",
      "                       [PATH [PATH ...]]\r\n",
      "\r\n",
      "Install a dataset from a (remote) source.\r\n",
      "\r\n",
      "This command creates a local sibling of an existing dataset from a\r\n",
      "(remote) location identified via a URL or path. Optional recursion into\r\n",
      "potential subdatasets, and download of all referenced data is supported.\r\n",
      "The new dataset can be optionally registered in an existing\r\n",
      "superdataset by identifying it via the DATASET argument (the new\r\n",
      "dataset's path needs to be located within the superdataset for that).\r\n",
      "\r\n",
      "It is recommended to provide a brief description to label the dataset's\r\n",
      "nature *and* location, e.g. \"Michael's music on black laptop\". This helps\r\n",
      "humans to identify data locations in distributed scenarios.  By default an\r\n",
      "identifier comprised of user and machine name, plus path will be generated.\r\n",
      "\r\n",
      "When only partial dataset content shall be obtained, it is recommended to\r\n",
      "use this command without the GET-DATA flag, followed by a\r\n",
      "`get` operation to obtain the desired data.\r\n",
      "\r\n",
      "NOTE\r\n",
      "  Power-user info: This command uses git clone, and\r\n",
      "  git annex init to prepare the dataset. Registering to a\r\n",
      "  superdataset is performed via a git submodule add operation\r\n",
      "  in the discovered superdataset.\r\n",
      "\r\n",
      "*Arguments*\r\n",
      "  PATH                  path/name of the installation target. If no PATH is\r\n",
      "                        provided a destination path will be derived from a\r\n",
      "                        source URL similar to git clone. [Default: None]\r\n",
      "\r\n",
      "*Options*\r\n",
      "  --version             show the program's version and license information\r\n",
      "  -h, --help, --help-np\r\n",
      "                        show this help message. --help-np forcefully disables\r\n",
      "                        the use of a pager for displaying the help message\r\n",
      "  -l LEVEL, --log-level LEVEL\r\n",
      "                        set logging verbosity level. Choose among critical,\r\n",
      "                        error, warning, info, debug. Also you can specify an\r\n",
      "                        integer <10 to provide even more debugging information\r\n",
      "  --pbs-runner {condor}\r\n",
      "                        execute command by scheduling it via available PBS.\r\n",
      "                        For settings, config file will be consulted\r\n",
      "  -s SOURCE, --source SOURCE\r\n",
      "                        URL or local path of the installation source.\r\n",
      "                        Constraints: value must be a string [Default: None]\r\n",
      "  -d DATASET, --dataset DATASET\r\n",
      "                        specify the dataset to perform the install operation\r\n",
      "                        on. If no dataset is given, an attempt is made to\r\n",
      "                        identify the dataset in a parent directory of the\r\n",
      "                        current working directory and/or the PATH given.\r\n",
      "                        Constraints: Value must be a Dataset or a valid\r\n",
      "                        identifier of a Dataset (e.g. a path) [Default: None]\r\n",
      "  -g, --get-data        if given, obtain all data content too. [Default:\r\n",
      "                        False]\r\n",
      "  -D DESCRIPTION, --description DESCRIPTION\r\n",
      "                        short description to use for a dataset location. Its\r\n",
      "                        primary purpose is to help humans to identify a\r\n",
      "                        dataset copy (e.g., \"mike's dataset on lab server\").\r\n",
      "                        Note that when a dataset is published, this\r\n",
      "                        information becomes available on the remote side.\r\n",
      "                        Constraints: value must be a string [Default: None]\r\n",
      "  -r, --recursive       if set, recurse into potential subdataset. [Default:\r\n",
      "                        False]\r\n",
      "  --recursion-limit LEVELS\r\n",
      "                        limit recursion into subdataset to the given number of\r\n",
      "                        levels. Constraints: value must be convertible to type\r\n",
      "                        'int' [Default: None]\r\n",
      "  --nosave              by default all modifications to a dataset are\r\n",
      "                        immediately saved. Given this option will disable this\r\n",
      "                        behavior. [Default: True]\r\n",
      "  --reckless            Set up the dataset to be able to obtain content in the\r\n",
      "                        cheapest/fastest possible way, even if this poses a\r\n",
      "                        potential risk the data integrity (e.g. hardlink files\r\n",
      "                        from a local clone of the dataset). Use with care, and\r\n",
      "                        limit to \"read-only\" use cases. With this flag the\r\n",
      "                        installed dataset will be marked as untrusted.\r\n",
      "                        [Default: False]\r\n",
      "  -J NJOBS, --jobs NJOBS\r\n",
      "                        how many parallel jobs (where possible) to use.\r\n",
      "                        Constraints: value must be convertible to type 'int'\r\n",
      "                        [Default: None]\r\n",
      "\r\n"
     ]
    }
   ],
   "source": [
    "!datalad install --help"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/data\n"
     ]
    }
   ],
   "source": [
    "cd /data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0m\u001b[01;34mcifti-data\u001b[0m/  \u001b[01;34mds000114\u001b[0m/\r\n"
     ]
    }
   ],
   "source": [
    "ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\u001b[1;37mINFO   \u001b[0m] Cloning dataset from 'http://datasets.datalad.org/' (trying 2 location candidate(s)) to '/data/datasets.datalad.org' \r\n",
      "\u001b[1;1minstall\u001b[0m(\u001b[1;32mok\u001b[0m): /data/datasets.datalad.org (\u001b[1;35mdataset\u001b[0m)\r\n"
     ]
    }
   ],
   "source": [
    "!datalad install /// "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Usage: datalad search [--version] [-h] [-l LEVEL] [--pbs-runner {condor}]\r\n",
      "                      [-d DATASET] [-s PROPERTY] [-r PROPERTY] [-R]\r\n",
      "                      [-f FORMAT] [--regex]\r\n",
      "                      STRING [STRING ...]\r\n",
      "\r\n",
      "Search within available in datasets' meta data\r\n",
      "\r\n",
      "*Arguments*\r\n",
      "  STRING                a string (or a regular expression if --regex) to\r\n",
      "                        search for in all meta data values. If multiple\r\n",
      "                        provided, all must have a match among some fields of a\r\n",
      "                        dataset.\r\n",
      "\r\n",
      "*Options*\r\n",
      "  --version             show the program's version and license information\r\n",
      "  -h, --help, --help-np\r\n",
      "                        show this help message. --help-np forcefully disables\r\n",
      "                        the use of a pager for displaying the help message\r\n",
      "  -l LEVEL, --log-level LEVEL\r\n",
      "                        set logging verbosity level. Choose among critical,\r\n",
      "                        error, warning, info, debug. Also you can specify an\r\n",
      "                        integer <10 to provide even more debugging information\r\n",
      "  --pbs-runner {condor}\r\n",
      "                        execute command by scheduling it via available PBS.\r\n",
      "                        For settings, config file will be consulted\r\n",
      "  -d DATASET, --dataset DATASET\r\n",
      "                        specify the dataset to perform the query operation on.\r\n",
      "                        If no dataset is given, an attempt is made to identify\r\n",
      "                        the dataset based on the current working directory\r\n",
      "                        and/or the PATH given. Constraints: Value must be a\r\n",
      "                        Dataset or a valid identifier of a Dataset (e.g. a\r\n",
      "                        path) [Default: None]\r\n",
      "  -s PROPERTY, --search PROPERTY\r\n",
      "                        name of the property to search for any match. This\r\n",
      "                        option can be given multiple times. By default, all\r\n",
      "                        properties are searched. [Default: None]\r\n",
      "  -r PROPERTY, --report PROPERTY\r\n",
      "                        name of the property to report for any match. This\r\n",
      "                        option can be given multiple times. If '*' is given,\r\n",
      "                        all properties are reported. [Default: None]\r\n",
      "  -R, --report-matched  flag to report those fields which have matches. If\r\n",
      "                        REPORT option values are provided, union of matched\r\n",
      "                        and those in REPORT will be output. [Default: False]\r\n",
      "  -f FORMAT, --format FORMAT\r\n",
      "                        format for output. Constraints: value must be one of\r\n",
      "                        ('custom', 'json', 'yaml') [Default: 'custom']\r\n",
      "  --regex               flag for STRING to be used as a (Python) regular\r\n",
      "                        expression which should match the value. [Default:\r\n",
      "                        False]\r\n",
      "\r\n"
     ]
    }
   ],
   "source": [
    "!datalad search --help"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\u001b[1;37mINFO   \u001b[0m] Loading and caching local meta-data... might take a few seconds \r\n",
      "\u001b[1;4mdatasets.datalad.org/labs/haxby/attention\u001b[0m \r\n",
      "\u001b[1;4mdatasets.datalad.org/openfmri/ds000233\u001b[0m \r\n",
      "\u001b[1;4mdatasets.datalad.org/hbnssi\u001b[0m \r\n",
      "\u001b[1;4mdatasets.datalad.org/labs/haxby\u001b[0m \r\n",
      "\u001b[1;4mdatasets.datalad.org/labs/haxby/raiders\u001b[0m \r\n",
      "\u001b[1;4mdatasets.datalad.org/openfmri/ds000105\u001b[0m \r\n"
     ]
    }
   ],
   "source": [
    "!datalad search -d datasets.datalad.org haxby"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;4mdatasets.datalad.org/openfmri/ds000114\u001b[0m \r\n",
      "\u001b[1;4mdatasets.datalad.org/workshops/nih-2017/ds000114\u001b[0m \r\n",
      "\u001b[1;4mdatasets.datalad.org/openfmri/ds000158\u001b[0m \r\n",
      "\u001b[1;4mdatasets.datalad.org/openfmri/ds000221\u001b[0m \r\n",
      "\u001b[1;4mdatasets.datalad.org/workshops/nipype-2017/ds000114\u001b[0m \r\n"
     ]
    }
   ],
   "source": [
    "!datalad search -d datasets.datalad.org gorgolewski"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\u001b[1;37mINFO   \u001b[0m] Cloning dataset from 'http://datasets.datalad.org/workshops/.git' (trying 1 location candidate(s)) to '/data/datasets.datalad.org/workshops' \r\n",
      "\u001b[1;1minstall\u001b[0m(\u001b[1;32mok\u001b[0m): /data/datasets.datalad.org/workshops (\u001b[1;35mdataset\u001b[0m) [Installed subdataset in order to get /data/datasets.datalad.org/workshops/nih-2017/ds000114]\r\n",
      "[\u001b[1;37mINFO   \u001b[0m] Cloning dataset from 'http://datasets.datalad.org/workshops/nih-2017/.git' (trying 1 location candidate(s)) to '/data/datasets.datalad.org/workshops/nih-2017' \r\n",
      "\u001b[1;1minstall\u001b[0m(\u001b[1;32mok\u001b[0m): /data/datasets.datalad.org/workshops/nih-2017 (\u001b[1;35mdataset\u001b[0m) [Installed subdataset in order to get /data/datasets.datalad.org/workshops/nih-2017/ds000114]\r\n",
      "[\u001b[1;37mINFO   \u001b[0m] Cloning dataset from 'http://datasets.datalad.org/workshops/nih-2017/ds000114/.git' (trying 2 location candidate(s)) to '/data/datasets.datalad.org/workshops/nih-2017/ds000114' \r\n",
      "\u001b[1;1minstall\u001b[0m(\u001b[1;32mok\u001b[0m): /data/datasets.datalad.org/workshops/nih-2017/ds000114 (\u001b[1;35mdataset\u001b[0m) [Installed subdataset in order to get /data/datasets.datalad.org/workshops/nih-2017/ds000114]\r\n",
      "action summary:\r\n",
      "  install (ok: 3)\r\n"
     ]
    }
   ],
   "source": [
    "!datalad install datasets.datalad.org/workshops/nih-2017/ds000114"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[01;34mdatasets.datalad.org/workshops/nih-2017/ds000114\u001b[00m\r\n",
      "|-- CHANGES\r\n",
      "|-- dataset_description.json\r\n",
      "|-- \u001b[01;34mderivatives\u001b[00m\r\n",
      "|   |-- \u001b[01;34mfmriprep\u001b[00m\r\n",
      "|   `-- \u001b[01;34mfreesurfer\u001b[00m\r\n",
      "|-- \u001b[40;31;01mdwi.bval\u001b[00m -> \u001b[00m.git/annex/objects/JX/4K/MD5E-s335--5bd6fa32ccd0c79e79f9ac63a2c09c1a.bval/MD5E-s335--5bd6fa32ccd0c79e79f9ac63a2c09c1a.bval\u001b[00m\r\n",
      "|-- \u001b[40;31;01mdwi.bvec\u001b[00m -> \u001b[00m.git/annex/objects/Pg/wk/MD5E-s1248--0641c68ff6ee6164928c984541653430.bvec/MD5E-s1248--0641c68ff6ee6164928c984541653430.bvec\u001b[00m\r\n",
      "|-- \u001b[01;34msub-01\u001b[00m\r\n",
      "|   |-- \u001b[01;34mses-retest\u001b[00m\r\n",
      "|   |   |-- \u001b[01;34manat\u001b[00m\r\n",
      "|   |   |-- \u001b[01;34mdwi\u001b[00m\r\n",
      "|   |   `-- \u001b[01;34mfunc\u001b[00m\r\n",
      "|   `-- \u001b[01;34mses-test\u001b[00m\r\n",
      "|       |-- \u001b[01;34manat\u001b[00m\r\n",
      "|       |-- \u001b[01;34mdwi\u001b[00m\r\n",
      "|       `-- \u001b[01;34mfunc\u001b[00m\r\n",
      "|-- \u001b[01;34msub-02\u001b[00m\r\n",
      "|   |-- \u001b[01;34mses-retest\u001b[00m\r\n",
      "|   |   |-- \u001b[01;34manat\u001b[00m\r\n",
      "|   |   |-- \u001b[01;34mdwi\u001b[00m\r\n",
      "|   |   `-- \u001b[01;34mfunc\u001b[00m\r\n",
      "|   `-- \u001b[01;34mses-test\u001b[00m\r\n",
      "|       |-- \u001b[01;34manat\u001b[00m\r\n",
      "|       |-- \u001b[01;34mdwi\u001b[00m\r\n",
      "|       `-- \u001b[01;34mfunc\u001b[00m\r\n",
      "|-- \u001b[01;34msub-03\u001b[00m\r\n",
      "|   |-- \u001b[01;34mses-retest\u001b[00m\r\n",
      "|   |   |-- \u001b[01;34manat\u001b[00m\r\n",
      "|   |   |-- \u001b[01;34mdwi\u001b[00m\r\n",
      "|   |   `-- \u001b[01;34mfunc\u001b[00m\r\n",
      "|   `-- \u001b[01;34mses-test\u001b[00m\r\n",
      "|       |-- \u001b[01;34manat\u001b[00m\r\n",
      "|       |-- \u001b[01;34mdwi\u001b[00m\r\n",
      "|       `-- \u001b[01;34mfunc\u001b[00m\r\n",
      "|-- \u001b[01;34msub-04\u001b[00m\r\n",
      "|   |-- \u001b[01;34mses-retest\u001b[00m\r\n",
      "|   |   |-- \u001b[01;34manat\u001b[00m\r\n",
      "|   |   |-- \u001b[01;34mdwi\u001b[00m\r\n",
      "|   |   `-- \u001b[01;34mfunc\u001b[00m\r\n",
      "|   `-- \u001b[01;34mses-test\u001b[00m\r\n",
      "|       |-- \u001b[01;34manat\u001b[00m\r\n",
      "|       |-- \u001b[01;34mdwi\u001b[00m\r\n",
      "|       `-- \u001b[01;34mfunc\u001b[00m\r\n",
      "|-- \u001b[01;34msub-05\u001b[00m\r\n",
      "|   |-- \u001b[01;34mses-retest\u001b[00m\r\n",
      "|   |   |-- \u001b[01;34manat\u001b[00m\r\n",
      "|   |   |-- \u001b[01;34mdwi\u001b[00m\r\n",
      "|   |   `-- \u001b[01;34mfunc\u001b[00m\r\n",
      "|   `-- \u001b[01;34mses-test\u001b[00m\r\n",
      "|       |-- \u001b[01;34manat\u001b[00m\r\n",
      "|       |-- \u001b[01;34mdwi\u001b[00m\r\n",
      "|       `-- \u001b[01;34mfunc\u001b[00m\r\n",
      "|-- \u001b[01;34msub-06\u001b[00m\r\n",
      "|   |-- \u001b[01;34mses-retest\u001b[00m\r\n",
      "|   |   |-- \u001b[01;34manat\u001b[00m\r\n",
      "|   |   |-- \u001b[01;34mdwi\u001b[00m\r\n",
      "|   |   `-- \u001b[01;34mfunc\u001b[00m\r\n",
      "|   `-- \u001b[01;34mses-test\u001b[00m\r\n",
      "|       |-- \u001b[01;34manat\u001b[00m\r\n",
      "|       |-- \u001b[01;34mdwi\u001b[00m\r\n",
      "|       `-- \u001b[01;34mfunc\u001b[00m\r\n",
      "|-- \u001b[01;34msub-07\u001b[00m\r\n",
      "|   |-- \u001b[01;34mses-retest\u001b[00m\r\n",
      "|   |   |-- \u001b[01;34manat\u001b[00m\r\n",
      "|   |   |-- \u001b[01;34mdwi\u001b[00m\r\n",
      "|   |   `-- \u001b[01;34mfunc\u001b[00m\r\n",
      "|   `-- \u001b[01;34mses-test\u001b[00m\r\n",
      "|       |-- \u001b[01;34manat\u001b[00m\r\n",
      "|       |-- \u001b[01;34mdwi\u001b[00m\r\n",
      "|       `-- \u001b[01;34mfunc\u001b[00m\r\n",
      "|-- \u001b[01;34msub-08\u001b[00m\r\n",
      "|   |-- \u001b[01;34mses-retest\u001b[00m\r\n",
      "|   |   |-- \u001b[01;34manat\u001b[00m\r\n",
      "|   |   |-- \u001b[01;34mdwi\u001b[00m\r\n",
      "|   |   `-- \u001b[01;34mfunc\u001b[00m\r\n",
      "|   `-- \u001b[01;34mses-test\u001b[00m\r\n",
      "|       |-- \u001b[01;34manat\u001b[00m\r\n",
      "|       |-- \u001b[01;34mdwi\u001b[00m\r\n",
      "|       `-- \u001b[01;34mfunc\u001b[00m\r\n",
      "|-- \u001b[01;34msub-09\u001b[00m\r\n",
      "|   |-- \u001b[01;34mses-retest\u001b[00m\r\n",
      "|   |   |-- \u001b[01;34manat\u001b[00m\r\n",
      "|   |   |-- \u001b[01;34mdwi\u001b[00m\r\n",
      "|   |   `-- \u001b[01;34mfunc\u001b[00m\r\n",
      "|   `-- \u001b[01;34mses-test\u001b[00m\r\n",
      "|       |-- \u001b[01;34manat\u001b[00m\r\n",
      "|       |-- \u001b[01;34mdwi\u001b[00m\r\n",
      "|       `-- \u001b[01;34mfunc\u001b[00m\r\n",
      "|-- \u001b[01;34msub-10\u001b[00m\r\n",
      "|   |-- \u001b[01;34mses-retest\u001b[00m\r\n",
      "|   |   |-- \u001b[01;34manat\u001b[00m\r\n",
      "|   |   |-- \u001b[01;34mdwi\u001b[00m\r\n",
      "|   |   `-- \u001b[01;34mfunc\u001b[00m\r\n",
      "|   `-- \u001b[01;34mses-test\u001b[00m\r\n",
      "|       |-- \u001b[01;34manat\u001b[00m\r\n",
      "|       |-- \u001b[01;34mdwi\u001b[00m\r\n",
      "|       `-- \u001b[01;34mfunc\u001b[00m\r\n",
      "|-- task-covertverbgeneration_bold.json\r\n",
      "|-- task-covertverbgeneration_events.tsv\r\n",
      "|-- task-fingerfootlips_bold.json\r\n",
      "|-- task-fingerfootlips_events.tsv\r\n",
      "|-- task-linebisection_bold.json\r\n",
      "|-- task-overtverbgeneration_bold.json\r\n",
      "|-- task-overtverbgeneration_events.tsv\r\n",
      "|-- task-overtwordrepetition_bold.json\r\n",
      "`-- task-overtwordrepetition_events.tsv\r\n",
      "\r\n",
      "93 directories, 13 files\r\n"
     ]
    }
   ],
   "source": [
    "!tree -L 3 datasets.datalad.org/workshops/nih-2017/ds000114"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cat: datasets.datalad.org/workshops/nih-2017/ds000114/dwi.bval: No such file or directory\r\n"
     ]
    }
   ],
   "source": [
    "!cat datasets.datalad.org/workshops/nih-2017/ds000114/dwi.bval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "Total:   0%|                                          | 0.00/335 [00:00<?, ?B/s]\r",
      "Total: 100%|#####################################| 335/335 [00:00<00:00, 779B/s]\r",
      "                                                                                \r",
      "Total (1 ok out of 1)100%|#######################| 335/335 [00:00<00:00, 779B/s]\r",
      "                                                                                \r",
      "\u001b[1;1mget\u001b[0m(\u001b[1;32mok\u001b[0m): /data/datasets.datalad.org/workshops/nih-2017/ds000114/dwi.bval (\u001b[1;35mfile\u001b[0m)\r\n"
     ]
    }
   ],
   "source": [
    "!datalad get datasets.datalad.org/workshops/nih-2017/ds000114/dwi.bval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0 0 0 0 0 0 1000 1000 1000 1000 1000 1000 1000 1000 1000 1000 1000 1000 1000 1000 1000 1000 1000 1000 1000 1000 1000 1000 1000 1000 1000 1000 1000 1000 1000 1000 1000 1000 1000 1000 1000 1000 1000 1000 1000 1000 1000 1000 1000 1000 1000 1000 1000 1000 1000 1000 1000 1000 1000 1000 1000 1000 1000 1000 1000 1000 1000 1000 1000 1000 \r\n"
     ]
    }
   ],
   "source": [
    "!cat datasets.datalad.org/workshops/nih-2017/ds000114/dwi.bval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;4mdatasets.datalad.org/workshops/nih-2017/ds000114\u001b[0m   [annex]  master  2.0.1-9-g324ebf438 2017-07-30/02:51:13  \u001b[1;32m���\u001b[0m\r\n"
     ]
    }
   ],
   "source": [
    "!LC_ALL= datalad ls datasets.datalad.org/workshops/nih-2017/ds000114"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "git-annex: Not in a git repository.\r\n"
     ]
    }
   ],
   "source": [
    "!git-annex list datasets.datalad.org/workshops/nih-2017/ds000114/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "git-annex list - show which remotes contain files\r\n",
      "\r\n",
      "Usage: git-annex list [PATH ...] [--allrepos]\r\n",
      "\r\n",
      "Available options:\r\n",
      "  --allrepos               show all repositories, not only remotes\r\n",
      "  --force                  allow actions that may lose annexed data\r\n",
      "  -F,--fast                avoid slow operations\r\n",
      "  -q,--quiet               avoid verbose output\r\n",
      "  -v,--verbose             allow verbose output (default)\r\n",
      "  -d,--debug               show debug messages\r\n",
      "  --no-debug               don't show debug messages\r\n",
      "  -b,--backend NAME        specify key-value backend to use\r\n",
      "  -N,--numcopies NUMBER    override default number of copies\r\n",
      "  --trust REMOTE           override trust setting\r\n",
      "  --semitrust REMOTE       override trust setting back to default\r\n",
      "  --untrust REMOTE         override trust setting to untrusted\r\n",
      "  -c,--config NAME=VALUE   override git configuration setting\r\n",
      "  --user-agent NAME        override default User-Agent\r\n",
      "  --trust-glacier          Trust Amazon Glacier inventory\r\n",
      "  --notify-finish          show desktop notification after transfer finishes\r\n",
      "  --notify-start           show desktop notification after transfer starts\r\n",
      "  -i,--in REMOTE           match files present in a remote\r\n",
      "  -C,--copies REMOTE       skip files with fewer copies\r\n",
      "  --lackingcopies NUMBER   match files that need more copies\r\n",
      "  --approxlackingcopies NUMBER\r\n",
      "                           match files that need more copies (faster)\r\n",
      "  -B,--inbackend NAME      match files using a key-value backend\r\n",
      "  --securehash             match files using a cryptographically secure hash\r\n",
      "  --inallgroup GROUP       match files present in all remotes in a group\r\n",
      "  --metadata FIELD=VALUE   match files with attached metadata\r\n",
      "  --want-get               match files the repository wants to get\r\n",
      "  --want-drop              match files the repository wants to drop\r\n",
      "  -x,--exclude GLOB        skip files matching the glob pattern\r\n",
      "  -I,--include GLOB        limit to files matching the glob pattern\r\n",
      "  --largerthan SIZE        match files larger than a size\r\n",
      "  --smallerthan SIZE       match files smaller than a size\r\n",
      "  --not                    negate next option\r\n",
      "  --and                    both previous and next option must match\r\n",
      "  --or                     either previous or next option must match\r\n",
      "  -(                       open group of options\r\n",
      "  -)                       close group of options\r\n",
      "  -T,--time-limit TIME     stop after the specified amount of time\r\n",
      "  -h,--help                Show this help text\r\n",
      "\r\n",
      "For details, run: git-annex help list\r\n"
     ]
    }
   ],
   "source": [
    "!git-annex list --help"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "here\n",
      "|origin\n",
      "||web\n",
      "|||bittorrent\n",
      "||||datalad-archives\n",
      "|||||\n",
      "X_X_X dwi.bval\n",
      "__X_X dwi.bvec\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "\n",
    "cd datasets.datalad.org/workshops/nih-2017/ds000114/\n",
    "git-annex list dwi.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;1mdrop\u001b[0m(\u001b[1;32mok\u001b[0m): /data/datasets.datalad.org/workshops/nih-2017/ds000114/dwi.bval (\u001b[1;35mfile\u001b[0m) [checking http://openneuro.s3.amazonaws.com/ds000114/ds000114_R2.0.0/uncompressed/dwi.bval?versionId=null...]\r\n"
     ]
    }
   ],
   "source": [
    "!datalad drop datasets.datalad.org/workshops/nih-2017/ds000114/dwi.bval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "here\n",
      "|origin\n",
      "||web\n",
      "|||bittorrent\n",
      "||||datalad-archives\n",
      "|||||\n",
      "__X_X dwi.bval\n",
      "__X_X dwi.bvec\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "\n",
    "cd datasets.datalad.org/workshops/nih-2017/ds000114/\n",
    "git-annex list dwi.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0m\u001b[01;34mcifti-data\u001b[0m/  \u001b[01;34mdatasets.datalad.org\u001b[0m/  \u001b[01;34mds000114\u001b[0m/\r\n"
     ]
    }
   ],
   "source": [
    "ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\u001b[1;37mINFO   \u001b[0m] Creating a new annex repo at /data/mydataset \r\n",
      "\r",
      "Total:   0%|                                         | 0.00/21.0 [00:00<?, ?B/s]\r",
      "                                                                                \r",
      "\r",
      "Total:   0%|                                          | 0.00/233 [00:00<?, ?B/s]\r",
      "                                                                                \r",
      "\u001b[1;1mcreate\u001b[0m(\u001b[1;32mok\u001b[0m): /data/mydataset (\u001b[1;35mdataset\u001b[0m)\r\n"
     ]
    }
   ],
   "source": [
    "!datalad create mydataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "Total:   0%|          | 0.00/4.00 [00:00<?, ?B/s]\r",
      "Total: 100%|##########| 4.00/4.00 [00:00<00:00, 27.2B/s]\r",
      "          \r",
      "Total (1 ok out of 1)100%|##########| 4.00/4.00 [00:00<00:00, 27.2B/s]\r",
      "                                                        \r",
      "add(ok): /data/mydataset/123 (file)\n",
      "save(ok): /data/mydataset (dataset)\n",
      "action summary:\n",
      "  add (ok: 1)\n",
      "  save (ok: 1)\n"
     ]
    }
   ],
   "source": [
    "%%bash \n",
    "\n",
    "echo \"123\" > mydataset/123\n",
    "datalad add mydataset/123"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "here\n",
      "|web\n",
      "||bittorrent\n",
      "|||\n",
      "X__ 123\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "\n",
    "cd mydataset\n",
    "git-annex list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[01;34mmydataset\u001b[00m\r\n",
      "`-- \u001b[01;36m123\u001b[00m -> .git/annex/objects/pF/Zf/MD5E-s4--ba1f2511fc30423bdbb183fe33f3dd0f/MD5E-s4--ba1f2511fc30423bdbb183fe33f3dd0f\r\n",
      "\r\n",
      "0 directories, 1 file\r\n"
     ]
    }
   ],
   "source": [
    "!tree mydataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "123\r\n"
     ]
    }
   ],
   "source": [
    "!cat mydataset/.git/annex/objects/pF/Zf/MD5E-s4--ba1f2511fc30423bdbb183fe33f3dd0f/MD5E-s4--ba1f2511fc30423bdbb183fe33f3dd0f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\u001b[1;31mERROR  \u001b[0m] Failed to run ['git', '-c', 'receive.autogc=0', '-c', 'gc.auto=0', 'annex', 'drop', '--json', '123'] under '/data/mydataset'. Exit code=1. out={\"command\":\"drop\",\"wanted\":[],\"note\":\"(Use --force to override this check, or adjust numcopies.)\",\"skipped\":[],\"success\":false,\"key\":\"MD5E-s4--ba1f2511fc30423bdbb183fe33f3dd0f\",\"file\":\"123\"}\r\n",
      "|  err=git-annex: drop: 1 failed\r\n",
      "|  \r\n",
      "[\u001b[1;31mERROR  \u001b[0m] configured minimum number of copies not found [drop(/data/mydataset/123)] \r\n",
      "\u001b[1;1mdrop\u001b[0m(\u001b[1;31merror\u001b[0m): /data/mydataset/123 (\u001b[1;35mfile\u001b[0m) [configured minimum number of copies not found]\r\n"
     ]
    }
   ],
   "source": [
    "!datalad drop mydataset/123"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/usr/bin/sh: 1: cannot create mydataset/123: Permission denied\r\n"
     ]
    }
   ],
   "source": [
    "!echo \"321\" > mydataset/123"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "unlock(ok): 123 (file)\n",
      "\r",
      "Total:   0%|          | 0.00/4.00 [00:00<?, ?B/s]\r",
      "Total: 100%|##########| 4.00/4.00 [00:00<00:00, 14.8B/s]\r",
      "          \r",
      "Total (1 ok out of 1)100%|##########| 4.00/4.00 [00:00<00:00, 14.8B/s]\r",
      "                                                        \r",
      "add(ok): /data/mydataset/123 (file)\n",
      "save(ok): /data/mydataset (dataset)\n",
      "action summary:\n",
      "  add (ok: 1)\n",
      "  save (ok: 1)\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "\n",
    "datalad unlock mydataset/123\n",
    "echo \"321\" > mydataset/123\n",
    "datalad add mydataset/123"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/usr/bin/sh: 1: cannot create mydataset/123: Permission denied\r\n"
     ]
    }
   ],
   "source": [
    "!echo \"123\" > mydataset/123"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[01;34mmydataset\u001b[00m\r\n",
      "`-- \u001b[01;36m123\u001b[00m -> .git/annex/objects/6v/gZ/MD5E-s4--9492fe88f263d58e0b686885e8c98c0e/MD5E-s4--9492fe88f263d58e0b686885e8c98c0e\r\n",
      "\r\n",
      "0 directories, 1 file\r\n"
     ]
    }
   ],
   "source": [
    "!tree mydataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "321\r\n"
     ]
    }
   ],
   "source": [
    "!cat mydataset/.git/annex/objects/6v/gZ/MD5E-s4--9492fe88f263d58e0b686885e8c98c0e/MD5E-s4--9492fe88f263d58e0b686885e8c98c0e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "123\r\n"
     ]
    }
   ],
   "source": [
    "!cat mydataset/.git/annex/objects/pF/Zf/MD5E-s4--ba1f2511fc30423bdbb183fe33f3dd0f/MD5E-s4--ba1f2511fc30423bdbb183fe33f3dd0f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "commit c647aa8d373850dd0f7a357228feda806ff3cee9\n",
      "Author: neuro <neuro>\n",
      "Date:   Fri Aug 11 16:19:56 2017 +0000\n",
      "\n",
      "    [DATALAD] added content\n",
      "\n",
      "commit 33073ba0219c904f4eb336742fad56d590c6a697\n",
      "Author: neuro <neuro>\n",
      "Date:   Fri Aug 11 16:19:51 2017 +0000\n",
      "\n",
      "    [DATALAD] added content\n",
      "\n",
      "commit 8e38faf2753470aaa368576856b8905bab214f25\n",
      "Author: neuro <neuro>\n",
      "Date:   Fri Aug 11 16:19:49 2017 +0000\n",
      "\n",
      "    [DATALAD] new dataset\n",
      "\n",
      "commit f86888aeef1ab10d4e92bcb0132ddad276b7f8e4\n",
      "Author: neuro <neuro>\n",
      "Date:   Fri Aug 11 16:19:48 2017 +0000\n",
      "\n",
      "    [DATALAD] Set default backend for all files to be MD5E\n"
     ]
    }
   ],
   "source": [
    "%%bash \n",
    "\n",
    "cd mydataset/\n",
    "git log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#!/bin/bash\n",
      "cat $1 | wc -c\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "\n",
    "cd mydataset\n",
    "mkdir -p scripts\n",
    "cmd=$(cat << EOM\n",
    "#!/bin/bash\\ncat \\$1 | wc -c\n",
    "EOM\n",
    ")\n",
    "echo -e $cmd > scripts/run.sh\n",
    "chmod +x scripts/run.sh\n",
    "cat scripts/run.sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "Total:   0%|          | 0.00/2.00 [00:00<?, ?B/s]\r",
      "Total: 100%|##########| 2.00/2.00 [00:00<00:00, 13.0B/s]\r",
      "          \r",
      "Total (1 ok out of 1)100%|##########| 2.00/2.00 [00:00<00:00, 13.0B/s]\r",
      "                                                        \r",
      "add(ok): /data/mydataset/out (file)\n",
      "save(ok): /data/mydataset (dataset)\n",
      "action summary:\n",
      "  add (ok: 1)\n",
      "  save (ok: 1)\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "cd mydataset\n",
    "scripts/run.sh 123 > out\n",
    "datalad add out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Usage: datalad metadata [--version] [-h] [-l LEVEL] [--pbs-runner {condor}]\r\n",
      "                        [-d DATASET] [-a KEY [VAL ...]] [-i KEY [VAL ...]]\r\n",
      "                        [--remove KEY [VAL ...]] [--reset KEY [VAL ...]]\r\n",
      "                        [--define-key KEY DEFINITION] [-g] [-r]\r\n",
      "                        [--recursion-limit LEVELS]\r\n",
      "                        [PATH [PATH ...]]\r\n",
      "\r\n",
      "Metadata manipulation for files and whole datasets\r\n",
      "\r\n",
      "Two types of metadata are supported:\r\n",
      "\r\n",
      "1. metadata describing a dataset as a whole (dataset-global), and\r\n",
      "\r\n",
      "2. metadata for individual files in a dataset.\r\n",
      "\r\n",
      "Both types can be accessed and modified with this command.\r\n",
      "Note, however, that this only refers to Datalad's native metadata,\r\n",
      "and not to any other metadata that is possibly stored in files of a\r\n",
      "dataset.\r\n",
      "\r\n",
      "Datalad's native metadata capability is primarily targeting data\r\n",
      "description via arbitrary tags and other (brief) key-value attributes\r\n",
      "(with possibly multiple values for a single key).\r\n",
      "\r\n",
      "Metadata key names are limited to alphanumerics (and [_-.]). Moreover,\r\n",
      "all key names are converted to lower case.\r\n",
      "\r\n",
      "*Dataset (global) metadata*\r\n",
      "\r\n",
      "Metadata describing a dataset as a whole is stored in JSON format\r\n",
      "in the dataset at .datalad/metadata/dataset.json. The amount of\r\n",
      "metadata that can be stored is not limited by Datalad. However,\r\n",
      "it should be kept brief as this information is stored in the Git\r\n",
      "history of the dataset, and access or modification requires to\r\n",
      "read the entire file.\r\n",
      "\r\n",
      "Arbitrary metadata keys can be used. However, Datalad reserves the\r\n",
      "keys 'tag' and 'definition' for its own use. The can still be\r\n",
      "manipulated without any restrictions like any other metadata items,\r\n",
      "but doing so can impact Datalad's metadata-related functionality,\r\n",
      "handle with care.\r\n",
      "\r\n",
      "The 'tag' key is used to store a list of (unique) tags.\r\n",
      "\r\n",
      "The 'definition' key is used to store key-value mappings that define\r\n",
      "metadata keys used elsewhere in the metadata. Using the feature is\r\n",
      "optional (see --define-key). It can be useful in the context of\r\n",
      "data discovery needs, where metadata keys can be precisely defined\r\n",
      "by linking them to specific ontology terms.\r\n",
      "\r\n",
      "*File metadata*\r\n",
      "\r\n",
      "Metadata storage for individual files is provided by git-annex, and\r\n",
      "generally the same rules as for dataset-global metadata apply.\r\n",
      "However, there is just one reserved key name: 'tag'.\r\n",
      "\r\n",
      "Again, the amount of metadata is not limited, but metadata is stored\r\n",
      "in git-annex' internal data structures in the Git repository of a\r\n",
      "dataset. Large amounts of metadata can slow its performance.\r\n",
      "\r\n",
      "*Output rendering*\r\n",
      "\r\n",
      "By default, a short summary of the metadata for each dataset\r\n",
      "(component) is rendered::\r\n",
      "\r\n",
      "  <path> (<type>): -|<keys> [<tags>]\r\n",
      "\r\n",
      "where <path> is the path of the respective component, <type> a label\r\n",
      "for the type of dataset components metadata is presented for. Non-existant\r\n",
      "metadata is indicated by a dash, otherwise a comma-separated list of\r\n",
      "metadata keys (except for 'tag'), is followed by a list of tags, if there\r\n",
      "are any.\r\n",
      "\r\n",
      "*Arguments*\r\n",
      "  PATH                  path(s) to set/get metadata. Constraints: value must\r\n",
      "                        be a string [Default: None]\r\n",
      "\r\n",
      "*Options*\r\n",
      "  --version             show the program's version and license information\r\n",
      "  -h, --help, --help-np\r\n",
      "                        show this help message. --help-np forcefully disables\r\n",
      "                        the use of a pager for displaying the help message\r\n",
      "  -l LEVEL, --log-level LEVEL\r\n",
      "                        set logging verbosity level. Choose among critical,\r\n",
      "                        error, warning, info, debug. Also you can specify an\r\n",
      "                        integer <10 to provide even more debugging information\r\n",
      "  --pbs-runner {condor}\r\n",
      "                        execute command by scheduling it via available PBS.\r\n",
      "                        For settings, config file will be consulted\r\n",
      "  -d DATASET, --dataset DATASET\r\n",
      "                        Constraints: Value must be a Dataset or a valid\r\n",
      "                        identifier of a Dataset (e.g. a path) [Default: None]\r\n",
      "  -a KEY [VAL ...], --add KEY [VAL ...]\r\n",
      "                        metadata items to add. If only a key is given, a\r\n",
      "                        corresponding tag is added. If a key-value mapping\r\n",
      "                        (multiple values at once are supported) is given, the\r\n",
      "                        values are added to the metadata item of that key.\r\n",
      "                        Constraints: value must be a string [Default: None]\r\n",
      "  -i KEY [VAL ...], --init KEY [VAL ...]\r\n",
      "                        like --add, but tags are only added if no tag was\r\n",
      "                        present before. Likewise, values are only added to a\r\n",
      "                        metadata key, if that key did not exist before.\r\n",
      "                        Constraints: value must be a string [Default: None]\r\n",
      "  --remove KEY [VAL ...]\r\n",
      "                        metadata values to remove. If only a key is given, a\r\n",
      "                        corresponding tag is removed. If a key-value mapping\r\n",
      "                        (multiple values at once are supported) is given, only\r\n",
      "                        those values are removed from the metadata item of\r\n",
      "                        that key. If no values are left after the removal, the\r\n",
      "                        entire item of that key is removed. Constraints: value\r\n",
      "                        must be a string [Default: None]\r\n",
      "  --reset KEY [VAL ...]\r\n",
      "                        metadata items to remove. If only a key is given, a\r\n",
      "                        corresponding metadata key with all its values is\r\n",
      "                        removed. If a key-value mapping (multiple values at\r\n",
      "                        once are supported) is given, any existing values for\r\n",
      "                        this key are replaced by the given ones. Constraints:\r\n",
      "                        value must be a string [Default: None]\r\n",
      "  --define-key KEY DEFINITION\r\n",
      "                        convenience option to add an item in the dataset's\r\n",
      "                        global metadata ('definition' key). This can be used\r\n",
      "                        to define (custom) keys used in the datasets's\r\n",
      "                        metadata, for example by providing a URL to an\r\n",
      "                        ontology term for a given key label. This option does\r\n",
      "                        not need --dataset-global to be set to be in effect.\r\n",
      "                        Constraints: value must be a string [Default: None]\r\n",
      "  -g, --dataset-global  Whether to perform metadata query or modification on\r\n",
      "                        the global dataset metadata, or on individual dataset\r\n",
      "                        components. For example, without this switch setting\r\n",
      "                        metadata using the root path of a dataset, will set\r\n",
      "                        the given metadata for all files in a dataset, whereas\r\n",
      "                        with this flag only the metadata record of the dataset\r\n",
      "                        itself will be altered. [Default: False]\r\n",
      "  -r, --recursive       if set, recurse into potential subdataset. [Default:\r\n",
      "                        False]\r\n",
      "  --recursion-limit LEVELS\r\n",
      "                        limit recursion into subdataset to the given number of\r\n",
      "                        levels. Constraints: value must be convertible to type\r\n",
      "                        'int' [Default: None]\r\n",
      "\r\n"
     ]
    }
   ],
   "source": [
    "!datalad metadata --help"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "Total:   0%|                                         | 0.00/25.0 [00:00<?, ?B/s]\r",
      "                                                                                \r",
      ". (dataset): author\r\n"
     ]
    }
   ],
   "source": [
    "!datalad metadata -d mydataset -i author satra -g"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "123 (file): type\r\n",
      "out (file): type\r\n"
     ]
    }
   ],
   "source": [
    "!datalad metadata -d mydataset -i type output mydataset/123"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "123 (file): type\r\n",
      "out (file): type\r\n"
     ]
    }
   ],
   "source": [
    "!datalad metadata -d mydataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
